Exp 2 Jan23 - Regular Dataset  - Tokenization 2 - 6 OM combined


Notes
Optimizers used: 'Adam''Adadelta', 'Adagrad', 'Adam', ‘SGD’, ‘RMSProp’


statement specific tokenization, full completed statements from the dataset appeared in vocabulary.
Noticed separated sections (OM / Mapping / Schema) and also conjoint in few cases.


Ran multiple times to observe translations

- Run 1 - Till sample 20 (RMSProp)
- Run 2 - Adam (till sample 23)
- Run 3 - Adam (Sample 24 - 68)
- Run 4 - SGD (sample 68 - 77)

- Run 5 after 84 sample (Adadelta)


- validation_steps = 40  shows consistent performance


---------------------------



Total Instance = 102

Pareto = 71
Not Pareto = 31


correctly predicted P = 45 (TP)
Incorrectly predicted P = 26 (FN)


correctly predicted NP = 11 (TN)
Incorrectly predicted NP = 20 (FP)

Incorrectly predicted = 26 predicted as NP but they were P | 45 predicted as P and they are P
20 predicted as P but they were NP | 11 predicted as NP and they are NP





Precision - TP / (TP + FP) = 47 / (47 + 25) = 0.65
Recall - TP / (TP + FN) = 47 / (47 + 24) = 0.66
Accuracy = (TN + TP ) / (TN + FP + TP + FN ) = (6 + 47) / (6 + 25 + 47 + 24) = 0.519
F1 = 2 * (Precision * Recall / Precision + Recall) = 2 * (0.65 * 0.66 / 0.65 + 0.66) = 2 * (0.429 / 1.31) = 0.65




018 198 226 19
