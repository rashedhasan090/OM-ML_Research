# -*- coding: utf-8 -*-
"""1_OM_Train_1_OM_Test(CO)_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E6R5KtqRoiI66DLrEIJXNIvmB_Uw1_3A

# 1_OM_Train_1_OM_Test(CO)

# P Regular , NP Regular

New Orientation example 

(OM + Schema + Mapping)  ---> (Label + OM + Schema + Mapping)

###1 OM Dataset - Store Management

###1 OM - Testing - Customer-Order (Unseen)

## P - NP Distribution 

### Original


## Training 


### Total instances - 2688

### P samples - 50  P 
### NP samples -  2638 NP

## Testing 

### Total instances - 8

### P samples - 6
### NP samples - 2

## Setup (installing necessary libraries)
"""

!pip install "tensorflow-text>=2.10"
!pip install einops

"""#Importing Libraries """

import numpy as np
import typing
from typing import Any, Tuple
from sklearn.model_selection import train_test_split

import pathlib

import einops
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker

import tensorflow as tf
import tensorflow_text as tf_text

"""#Defining the Shapechecker"""

#@title
class ShapeChecker():
  def __init__(self):
    # Keep a cache of every axis-name seen
    self.shapes = {}

  def __call__(self, tensor, names, broadcast=False):
    if not tf.executing_eagerly():
      return

    parsed = einops.parse_shape(tensor, names)

    for name, new_dim in parsed.items():
      old_dim = self.shapes.get(name, None)
      
      if (broadcast and new_dim == 1):
        continue

      if old_dim is None:
        # If the axis name is new, add its length to the cache.
        self.shapes[name] = new_dim
        continue

      if new_dim != old_dim:
        raise ValueError(f"Shape mismatch for dimension: '{name}'\n"
                         f"    found: {new_dim}\n"
                         f"    expected: {old_dim}\n")

"""# Loading the Dataset"""

import pandas as pd
ORM_data = pd.read_excel('store_mgmt_full_ds_mod.xlsx')

"""#Reading Data from Dataset"""

ORM_data.head()

OM_Regular = ORM_data['OM_Regular'].values
OM_Prediction = ORM_data['OM_Prediction'].values

X = OM_Regular
Y = OM_Prediction

"""#### Dividing data as Target and Context"""

target_raw =  Y
context_raw = X
print(context_raw[-1])

print(target_raw[-1])

"""### Create a tf.data dataset

From these arrays of strings you can create a `tf.data.Dataset` of strings that shuffles and batches them efficiently:
"""

BUFFER_SIZE = len(context_raw)
BATCH_SIZE = 1

is_train = np.random.uniform(size=(len(target_raw),)) < 0.8

train_raw = (
    tf.data.Dataset
    .from_tensor_slices((context_raw[is_train], target_raw[is_train]))
    .shuffle(BUFFER_SIZE)
    .batch(BATCH_SIZE))
val_raw = (
    tf.data.Dataset
    .from_tensor_slices((context_raw[~is_train], target_raw[~is_train]))
    .shuffle(BUFFER_SIZE)
    .batch(BATCH_SIZE))

for example_context_strings, example_target_strings in train_raw.take(1):
  print(example_context_strings[:5])
  print()
  print(example_target_strings[:5])
  break

"""### Text preprocessing

One of the goals of this tutorial is to build a model that can be exported as a `tf.saved_model`. To make that exported model useful it should take `tf.string` inputs, and return `tf.string` outputs: All the text processing happens inside the model. Mainly using a `layers.TextVectorization` layer.

#### Standardization

The model is dealing with multilingual text with a limited vocabulary. So it will be important to standardize the input text.

The first step is Unicode normalization to split accented characters and replace compatibility characters with their ASCII equivalents.

The `tensorflow_text` package contains a unicode normalize operation, We may or may not decide to Use this for ORM data. I kept it in the experiment
"""

#example_text = tf.constant('moduleOM_nameopenDeclarationonesigclass1_nameextendsClassattrSet=c1_at1+c1_at2id=c1_at1noparentisAbstract=No}onesigc1_at1extendsc1_at1_typeonesigc1_at2extendsc1_at2_typeonesigclass2_nameextendsClassattrSet=c2_at1+c2_at2+c2_at3+c2_at4id=c2_at1noparentisAbstract=No}onesigc2_at1extendsc2_at1_typeonesigc2_at2extendsc2_at2_typeonesigc2_at3extendsc2_at3_typeonesigc2_at4extendsc2_at4_typeonesigclass3_nameextendsClassattrSet=c3_at1+c3_at2+c3_at3+c3_at4id=c3_at1noparentisAbstract=No}onesigc3_at1extendsc3_at1_typeonesigc3_at2extendsc3_at2_typeonesigc3_at3extendsc3_at3_typeonesigc3_at4extendsc3_at4_typeonesigclass4_nameextendsClassattrSet=c4_at1id=c4_at1noparentisAbstract=No}onesigc4_at1extendsc4_at1_typeonesigclass5_nameextendsClassattrSet=c5_at1+c5_at2+c5_at3+c5_at4id=c5_at1noparentisAbstract=No}onesigc5_at1extendsc5_at1_typeonesigc5_at2extendsc5_at2_typeonesigc5_at3extendsc5_at3_typeonesigc5_at4extendsc5_at4_typeonesigclass6_nameextendsClassattrSet=c6_at1+c6_at2+c6_at3+c6_at4id=c6_at1noparentisAbstract=No}onesigc6_at1extendsc6_at1_typeonesigc6_at2extendsc6_at2_typeonesigc6_at3extendsc6_at3_typeonesigc6_at4extendsc6_at4_typeonesigassoc1extendsAssociationsrc=class1_namedst=class5_namesrc_multiplicity=src_mlpcdst_multiplicity=dst_mlpc}onesigassoc2extendsAssociationsrc=class1_namedst=class5_namesrc_multiplicity=src_mlpcdst_multiplicity=dst_mlpc}onesigassoc3extendsAssociationsrc=class4_namedst=class5_namesrc_multiplicity=src_mlpcdst_multiplicity=dst_mlpc}onesigassoc4extendsAssociationsrc=class1_namedst=class6_namesrc_multiplicity=src_mlpcdst_multiplicity=dst_mlpc2}onesigassoc5extendsAssociationsrc=class1_namedst=class3_namesrc_multiplicity=src_mlpcdst_multiplicity=dst_mlpc2}predshowrunshowfor38,â€‹OM_name_Solution:0Table:class1_nameAttributec1_at1:c1_at1_typePrimaryKeyTable:class1_nameAttributec1_at2:c1_at2_typeTable:class2_nameAttributec2_at1:c1_at1_typePrimaryKeyTable:class2_nameAttributec2_at2:c2_at2_typeTable:class2_nameAttributec2_at3:c2_at3_typeTable:class3_nameAttributec3_at1:c3_at1_typePrimaryKeyTable:class3_nameAttributec3_at4:c3_at4_typeTable:class3_nameAttributec3_at2:c3_at2_typeTable:class4_nameAttributec4_at1:c4_at1_typePrimaryKeyTable:class5_nameAttributec5_at3:c5_at3_typeTable:class5_nameAttributec5_at4:c5_at4_typeTable:class6_nameAttributec6_at1:c6_at1_typePrimaryKeyTable:class6_nameAttributec6_at2:c6_at2_typeTable:class6_nameAttributec6_at3:c6_at3_typeTable:class6_nameAttributec6_at4:c6_at4_typeTable:class1_nameAttributec1_at1:c1_at1_typePrimaryKeyTable:class2_nameAttributec2_at1:c1_at1_typePrimaryKeyTable:class3_nameAttributec3_at1:c3_at1_typePrimaryKeyTable:class4_nameAttributec4_at1:c4_at1_typePrimaryKeyTable:class6_nameAttributec6_at1:c6_at1_typePrimaryKeyTableName:class1_nameTableName:class2_nameTableName:class3_nameTableName:class4_nameTableName:class5_nameTableName:class6_nameMappingStrategyofTableclass1_name:map_str2MappingStrategyofTableclass2_name:map_str2MappingStrategyofTableclass3_name:map_str2MappingStrategyofTableclass4_name:map_str2MappingStrategyofTableclass6_name:map_str2AssociationStrategyforassoc1:assoc_type1AssociationStrategyforassoc2:assoc_type1AssociationStrategyforassoc3:assoc_type2AssociationStrategyforassoc4:assoc_type2AssociationStrategyforassoc5:assoc_type2,USEOM_name0----CREATETABLE`assoc1`(`c5_at1`c5_at1_type`c1_at1`c1_at1_typeKEY`FK_assoc1_c5_at1_idx`(`c5_at1`)KEY`FK_assoc1_c1_at1_idx`(`c1_at1`)PRIMARYKEY(`c5_at1``c1_at1`));----CREATETABLE`assoc3``c5_at1`c5_at1_type`c4_at1`c4_at1_typeKEY`FK_assoc3_c5_at1_idx`(`c5_at1`)KEY`FK_assoc3_c4_at1_idx`(`c4_at1`)PRIMARYKEY(`c5_at1``c4_at1`));----CREATETABLE`class5_name`(`c5_at4`c5_at4_type(64)`c5_at3`c5_at3_type(64)`c1_at1`c1_at1_type`c5_at1`c5_at1_typePRIMARYKEY(`c5_at1`));----CREATETABLE`class3_name`(`c3_at3`c3_at3_type(64)`c3_at2`c3_at2_type(64)`c3_at4`c3_at4_type`c3_at1`c3_at1_typeNOTNULLPRIMARYKEY(`c3_at1`));----CREATETABLE`class2_name`(`c2_at3`c2_at3_type(64)`c2_at2`c2_at2_type(64)`c2_at4`c2_at4_type`c2_at1`c2_at1_typePRIMARYKEY(`c2_at1`));----CREATETABLE`class4_name`(`c4_at1`c4_at1_typePRIMARYKEY(`c4_at1`));----CREATETABLE`class1_name`(`c1_at2`c1_at2_type(64)`c1_at1`c1_at1_typePRIMARYKEY(`c1_at1`));----CREATETABLE`class6_name`(`c6_at4`c6_at4_type`c6_at3`c6_at3_type`c6_at2`c6_at2_type`c6_at1`c6_at1_typeNOTNULLPRIMARYKEY(`c6_at1`));----CREATETABLE`assoc2`(`c5_at1`c5_at1_type`c2_at1`c2_at1_typeKEY`FK_assoc2_c5_at1_idx`(`c5_at1`)KEY`FK_assoc2_c2_at1_idx`(`c2_at1`)PRIMARYKEY(`c5_at1``c2_at1`));----CREATETABLE`assoc5`(`c3_at1`c3_at1_typeNOTNULL`c2_at1`c2_at1_typeKEY`FK_assoc5_c3_at1_idx`(`c3_at1`)KEY`FK_assoc5_c2_at1_idx`(`c2_at1`)PRIMARYKEY(`c3_at1``c2_at1`));----CREATETABLE`assoc4`(`c6_at1`c6_at1_typeNOTNULL`c2_at1`c2_at1_typeKEY`FK_assoc4_c6_at1_idx`(`c6_at1`)KEY`FK_assoc4_c2_at1_idx`(`c2_at1`)PRIMARYKEY(`c6_at1``c2_at1`));ALTERTABLE`assoc1`ADDCONSTRAINT`FK_assoc1_c5_at1`FOREIGNKEY(`c5_at1`)REFERENCES`class5_name`(`c5_at1`)ONDELETECASCADEONUPDATECASCADEADDCONSTRAINT`FK_assoc1_c1_at1`FOREIGNKEY(`c1_at1`)REFERENCES`class1_name`(`c1_at1`)ONDELETECASCADEONUPDATECASCADE;ALTERTABLE`assoc3`ADDCONSTRAINT`FK_assoc3_c5_at1`FOREIGNKEY(`c5_at1`)REFERENCES`class5_name`(`c5_at1`)ONDELETECASCADEONUPDATECASCADEADDCONSTRAINT`FK_assoc3_c4_at1`FOREIGNKEY(`c4_at1`)REFERENCES`class4_name`(`c4_at1`)ONDELETECASCADEONUPDATECASCADE;ALTERTABLE`assoc2`ADDCONSTRAINT`FK_assoc2_c5_at1`FOREIGNKEY(`c5_at1`)REFERENCES`class5_name`(`c5_at1`)ONDELETECASCADEONUPDATECASCADEADDCONSTRAINT`FK_assoc2_c2_at1`FOREIGNKEY(`c2_at1`)REFERENCES`class2_name`(`c2_at1`)ONDELETECASCADEONUPDATECASCADEALTERTABLE`assoc5`ADDCONSTRAINT`FK_assoc5_c3_at1`FOREIGNKEY(`c3_at1`)REFERENCES`class3_name`(`c3_at1`)ONDELETECASCADEONUPDATECASCADEADDCONSTRAINT`FK_assoc5_c2_at1`FOREIGNKEY(`c2_at1`)REFERENCES`class2_name`(`c2_at1`)ONDELETECASCADEONUPDATECASCADE;ALTERTABLE`assoc4`ADDCONSTRAINT`FK_assoc4_c6_at1`FOREIGNKEY(`c6_at1`)REFERENCES`class6_name`(`c6_at1`)ONDELETECASCADEONUPDATECASCADEADDCONSTRAINT`FK_assoc4_c2_at1`FOREIGNKEY(`c2_at1`)REFERENCES`class2_name`(`c2_at1`)ONDELETECASCADEONUPDATECASCADE')
example_text = tf.constant('module OM_name 0;open Declarationone sig class1_name extends ClassattrSet = c1_at1+c1_at2id=c1_at1no parentisAbstract = No}one sig c1_at1 extends c1_at1_typeone sig c1_at2 extends c1_at2_typeone sig class2_name extends ClassattrSet = c2_at1+c2_at2id=c2_at1no parentisAbstract = No}one sig c2_at1 extends c2_at1_typeone sig c2_at1 extends c2_at2_typeone sig class3_name extends ClassattrSet = c3_at1+c3_at2+c3_at3+c3_at4c3_at5+c3_at6id=c3_at1no parentisAbstract = No}one sig c3_at1 extends c3_at1_typeone sig c3_at2 extends c3_at2_typeone sig c3_at3 extends c3_at3_typeone sig c3_at4 extends c3_at4_typeone sig c3_at5 extends c3_at5_typeone sig c3_at6 extends c3_at6_typeone sig class4_name extends ClassattrSet = c4_at1+c4_at2+c4_at3+c4_at4id=c4_at3no parentisAbstract = No}one sig c4_at1 extends c4_at1_typeone sig c4_at2 extends c4_at2_typeone sig c4_at3 extends c4_at3_typeone sig c4_at4 extends c4_at4_typeone sig class5_name extends ClassattrSet = c5_at1one parentparent in class2_nameid = c2_at1isAbstract = No}one sig c5_at1 extends c5_at1_typeone sig class6_name extends ClassattrSet = c6_at1_0+c6_at2+c6_at3+c6_at4id=c6_at2no parentisAbstract = No}one sig c6_at1_0 extends c6_at1_typeone sig c6_at2 extends c6_at2_typeone sig c6_at3 extends c6_at3_typeone sig c6_at4 extends c6_at4_typeone sig class7_name extends ClassattrSet = c7_at1one parentparent in class2_nameid = c2_at1isAbstract = No}one sig c7_at1 extends c7_at1_typeone sig class8_name extends ClassattrSet = c8_at1one parentparent in class2_nameid = c2_at1isAbstract = No}one sig c8_at1 extends c8_at1_typeone sig class9_name extends ClassattrSet = c9_at1one parentparent in class2_nameid = c2_at1isAbstract = No}one sig c9_at1 extends c9_at1_typeone sig class10_name extends ClassattrSet = c10_at1+c10_at2+c2_at2+c6_at3id=c10_at1no parentisAbstract = No}one sig c10_at1 extends c10_at1_typeone sig c10_at2 extends c10_at2_typeone sig assoc10 extends Associationsrc = class10_namedst= class2_namesrc_multiplicity = src_mlpcdst_multiplicity = dst_mlpc}one sig assoc1 extends Associationsrc = class2_namedst= class4_namesrc_multiplicity = src_mlpc2dst_multiplicity = dst_mlpc}one sig assoc2 extends Associationsrc = class10_namedst= class3_namesrc_multiplicity = src_mlpcdst_multiplicity = dst_mlpc}one sig assoc3 extends Associationsrc = class10_namedst= class4_namesrc_multiplicity = src_mlpcdst_multiplicity = dst_mlpc}one sig assoc4 extends Associationsrc = class10_namedst= class6_namesrc_multiplicity = src_mlpcdst_multiplicity = dst_mlpc2}one sig assoc5 extends Associationsrc = class6_namedst= class1_namesrc_multiplicity = src_mlpcdst_multiplicity = dst_mlpc2}one sig assoc6 extends Associationsrc = class6_namedst= class4_namesrc_multiplicity = src_mlpcdst_multiplicity = dst_mlpc}one sig assoc7 extends Associationsrc = class1_namedst= class3_namesrc_multiplicity = src_mlpcdst_multiplicity = dst_mlpc}one sig assoc8 extends Associationsrc = class1_namedst= class4_namesrc_multiplicity = src_mlpcdst_multiplicity = dst_mlpc}one sig assoc9 extends Associationsrc = class2_namedst= class3_namesrc_multiplicity = src_mlpcdst_multiplicity = dst_mlpc2}pred showrun show,Mapping Strategy for class1_name : map_str2Mapping Strategy for class5_name : map_str2Mapping Strategy for class7_name : map_str2Mapping Strategy for class8_name : map_str2Mapping Strategy for class10_name : map_str2Association Strategy for assoc2 : assoc_str1Association Strategy for assoc3 : assoc_str1Association Strategy for assoc7 : assoc_str1Association Strategy for assoc1 : assoc_str2Association Strategy for assoc4 : assoc_str2Association Strategy for assoc5 : assoc_str2Association Strategy for assoc6 : assoc_str2Association Strategy for assoc9 : assoc_str2,USE OM_name:0;CREATE TABLE `class7_name` (`c7_at1` c7_at1_type(64),`c2_at2` c2_at2_type(64),`c2_at1` c2_at1_type NOT NULL,PRIMARY KEY (`c2_at1`)CREATE TABLE `class5_name` (c5_at1` c5_at1_type(64),`c2_at1` c2_at1_type NOT NULL,KEY `FK_class5_name_c2_at1_idx` (`c2_at1`),PRIMARY KEY (`c2_at1`)CREATE TABLE `class2_name` (`c2_at2` c2_at2_type(64),c10_at1` c10_at1_type`c2_at1` c2_at1_type NOT NULL,KEY `FK_class2_name_c10_at1_idx` (`c10_at1`),PRIMARY KEY (`c2_at1`)CREATE TABLE `class3_name` (`c3_at6` c3_at6_type,`c3_at5` c3_at5_type,`c3_at4` c3_at4_type,`c3_at3` c3_at3_type,`c3_at2` c3_at2_type,`c3_at1` c3_at1_type NOT NULL,`c1_at1` int,KEY `FK_class3_name_c1_at1_idx` (`c1_at1`),PRIMARY KEY (`c3_at1`)CREATE TABLE `class1_name` (`c1_at2` c1_at2_type(64),`c1_at1` c1_at1_type NOT NULL,PRIMARY KEY (`c1_at1`)CREATE TABLE `class6_name` (`c6_at4` c6_at4_type(64),`c6_at1_0` c6_at1_type(64),`c6_at2` c6_at2_type NOT NULL,PRIMARY KEY (`c6_at2`)CREATE TABLE `assoc1` (`c4_at3` c4_at3_type NOT NULL,`c2_at1` c2_at1_type NOT NULL,KEY `FK_assoc1_c4_at3_idx` (`c4_at3`),KEY `FK_assoc1_c2_at1_idx` (`c2_at1`),PRIMARY KEY (`c4_at3`,`c2_at1`)CREATE TABLE `assoc4` (c10_at1` c10_at1_type`c6_at2` c6_at2_type NOT NULL,KEY `FK_assoc4_c10_at1_idx` (`c10_at1`),KEY `FK_assoc4_c6_at2_idx` (`c6_at2`),PRIMARY KEY (`c10_at1`,`c6_at2`)CREATE TABLE `assoc2` (c10_at1` c10_at1_type`c3_at1` c3_at1_type NOT NULL,Association Strategy for assoc5 : assoc_str2`c6_at2` c6_at2_type NOT NULL,`c1_at1` c1_at1_type NOT NULL,KEY `FK_assoc5_c6_at2_idx` (`c6_at2`),KEY `FK_assoc5_c1_at1_idx` (`c1_at1`),PRIMARY KEY (`c6_at2`,`c1_at1`)CREATE TABLE `class10_name` (`c10_at2` c10_at2_type(64),`c6_at3` c6_at3_type(64),c10_at1` c10_at1_typePRIMARY KEY (`c10_at1`)CREATE TABLE `class4_name` (`c4_at4` c4_at4_type(64),`c4_at2` c4_at2_type(64),`c4_at1` c4_at1_type(64),c10_at1` c10_at1_type`c6_at2` c6_at2_type,`c4_at3` c4_at3_type NOT NULL,`c1_at1` int,KEY `FK_class4_name_c10_at1_idx` (`c10_at1`),KEY `FK_class4_name_c6_at2_idx` (`c6_at2`),KEY `FK_class4_name_c1_at1_idx` (`c1_at1`),PRIMARY KEY (`c4_at3`)CREATE TABLE `assoc9` (`c3_at1` c3_at1_type NOT NULL,`c2_at1` c2_at1_type NOT NULL,KEY `FK_assoc9_c3_at1_idx` (`c3_at1`)KEY `FK_assoc9_c2_at1_idx` (`c2_at1`)PRIMARY KEY (`c3_at1`,`c2_at1`)CREATE TABLE `class9_name` (`c9_at1` c9_at1_type(64),`c2_at1` c2_at1_type NOT NULL,KEY `FK_class9_name_c2_at1_idx` (`c2_at1`),PRIMARY KEY (`c2_at1`)CREATE TABLE `class8_name` (`c8_at1` c8_at1_type(64),`c2_at1` c2_at1_type NOT NULL,PRIMARY KEY (`c2_at1`)ALTER TABLE `class5_name`ADD CONSTRAINT `FK_class5_name_c2_at1` FOREIGN KEY (`c2_at1`) REFERENCES `class2_name` (`c2_at1`) ON DELETE CASCADE ON UPDATE CASCADE,ALTER TABLE `class2_name`ADD CONSTRAINT `FK_class2_name_c10_at1` FOREIGN KEY (`c10_at1`) REFERENCES `class10_name` (`c10_at1`) ON DELETE CASCADE ON UPDATE CASCADE;ALTER TABLE `class3_name`ADD CONSTRAINT `FK_class3_name_c1_at1` FOREIGN KEY (`c1_at1`) REFERENCES `class1_name` (`c1_at1`) ON DELETE CASCADE ON UPDATE CASCADE,ALTER TABLE `assoc1`ADD CONSTRAINT `FK_assoc1_c4_at3` FOREIGN KEY (`c4_at3`) REFERENCES `class4_name` (`c4_at3`) ON DELETE CASCADE ON UPDATE CASCADE;ADD CONSTRAINT `FK_assoc1_c2_at1` FOREIGN KEY (`c2_at1`) REFERENCES `class2_name` (`c2_at1`) ON DELETE CASCADE ON UPDATE CASCADE;ALTER TABLE `assoc4`ADD CONSTRAINT `FK_assoc4_c10_at1` FOREIGN KEY (`c10_at1`) REFERENCES `class10_name` (`c10_at1`) ON DELETE CASCADE ON UPDATE CASCADE;ADD CONSTRAINT `FK_assoc4_c6_at2` FOREIGN KEY (`c6_at2`) REFERENCES `class6_name` (`c6_at2`) ON DELETE CASCADE ON UPDATE CASCADE;ALTER TABLE `assoc2`ALTER TABLE `assoc5`ADD CONSTRAINT `FK_assoc5_c6_at2` FOREIGN KEY (`c6_at2`) REFERENCES `class6_name` (`c6_at2`) ON DELETE CASCADE ON UPDATE CASCADE;ADD CONSTRAINT `FK_assoc5_c1_at1` FOREIGN KEY (`c1_at1`) REFERENCES `class1_name` (`c1_at1`) ON DELETE CASCADE ON UPDATE CASCADE;ALTER TABLE `class4_name`ADD CONSTRAINT `FK_class4_name_c10_at1` FOREIGN KEY (`c10_at1`) REFERENCES `class10_name` (`c10_at1`) ON DELETE CASCADE ON UPDATE CASCADE;ADD CONSTRAINT `FK_class4_name_c6_at2` FOREIGN KEY (`c6_at2`) REFERENCES `class6_name` (`c6_at2`) ON DELETE CASCADE ON UPDATE CASCADE;ADD CONSTRAINT `FK_class4_name_c1_at1` FOREIGN KEY (`c1_at1`) REFERENCES `class1_name` (`c1_at1`) ON DELETE CASCADE ON UPDATE CASCADE,ALTER TABLE `assoc9`ADD CONSTRAINT `FK_assoc9_c3_at1` FOREIGN KEY (`c3_at1`) REFERENCES `class3_name` (`c3_at1`) ON DELETE CASCADE ON UPDATE CASCADE;ADD CONSTRAINT `FK_assoc9_c2_at1` FOREIGN KEY (`c2_at1`) REFERENCES `class2_name` (`c2_at1`) ON DELETE CASCADE ON UPDATE CASCADE;ALTER TABLE `class9_name`ADD CONSTRAINT `FK_class9_name_c2_at1` FOREIGN KEY (`c2_at1`) REFERENCES `class2_name` (`c2_at1`) ON DELETE CASCADE ON UPDATE CASCADE;')
#example_text = tf.constant('class1,table2,obj1,atr1')
print(example_text.numpy())
print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())

def tf_lower_and_split_punct(text):
  # Split accented characters.
  text = tf_text.normalize_utf8(text, 'NFKD')
  text = tf.strings.lower(text)
  # Keep space, a to z, and select punctuation.
  text = tf.strings.regex_replace(text, '', '')
  # Add spaces around punctuation.
  text = tf.strings.regex_replace(text, '', r'')
  # Strip whitespace.
  text = tf.strings.strip(text)

  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')
  return text

print(example_text.numpy().decode())
print(tf_lower_and_split_punct(example_text).numpy().decode())

"""#### Text Vectorization

This standardization function will be wrapped up in a `tf.keras.layers.TextVectorization` layer which will handle the vocabulary extraction and conversion of input text to sequences of tokens.
"""

max_vocab_size = 500000

context_text_processor = tf.keras.layers.TextVectorization(
    standardize=tf_lower_and_split_punct,
    max_tokens=max_vocab_size,
    split="whitespace",
    output_mode="int",
    output_sequence_length=None,
    ragged=True)

"""The `TextVectorization` layer and many other [Keras preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) have an `adapt` method. This method reads one epoch of the training data, and works a lot like `Model.fit`. This `adapt` method initializes the layer based on the data. Here it determines the vocabulary:"""

context_text_processor.adapt(train_raw.map(lambda context, target: context))

# Here are the first 10 words from the vocabulary:
context_text_processor.get_vocabulary()[:600]

"""That's the context data  `TextVectorization` layer, now build and `.adapt()` for the Target Data one:"""

target_text_processor = tf.keras.layers.TextVectorization(
    standardize=tf_lower_and_split_punct,
    max_tokens=max_vocab_size,
    split="whitespace",
    output_mode="int",
    output_sequence_length=None,
    ragged=True)

target_text_processor.adapt(train_raw.map(lambda context, target: target))
target_text_processor.get_vocabulary()[:600]

"""Now these layers can convert a batch of strings into a batch of token IDs:"""

# example_tokens = context_text_processor([example_context_strings, example_context_strings])
example_tokens = context_text_processor(example_context_strings)
example_tokens[:3, :]

"""The `get_vocabulary` method can be used to convert token IDs back to text:"""

context_vocab = np.array(context_text_processor.get_vocabulary())
tokens = context_vocab[example_tokens[0].numpy()]
' '.join(tokens)

"""The returned token IDs are zero-padded. This can easily be turned into a mask:"""

plt.subplot(1, 2, 1)
plt.pcolormesh(example_tokens.to_tensor())
plt.title('Token IDs')

plt.subplot(1, 2, 2)
plt.pcolormesh(example_tokens.to_tensor() != 0)
plt.title('Mask')

"""### Process the dataset

The `process_text` function below converts the `Datasets` of strings, into  0-padded tensors of token IDs. It also converts from a `(context, target)` pair to an `((context, target_in), target_out)` pair for training with `keras.Model.fit`. Keras expects `(inputs, labels)` pairs, the inputs are the `(context, target_in)` and the labels are `target_out`. The difference between `target_in` and `target_out` is that they are shifted by one step relative to eachother, so that at each location the label is the next token.
"""

def process_text(context, target):
  context = context_text_processor(context).to_tensor()
  target = target_text_processor(target)
  targ_in = target[:,:-1].to_tensor()
  targ_out = target[:,1:].to_tensor()
  return (context, targ_in), targ_out


train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)
val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)

"""Here is the first sequence of each, from the first batch:"""

for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):
  print(ex_context_tok[0, :10].numpy()) 
  print()
  print(ex_tar_in[0, :10].numpy()) 
  print(ex_tar_out[0, :10].numpy())

"""## The encoder/decoder

  <th colspan=1>This tutorial's model</th>
<tr>
</table>

Before getting into it define constants for the model:
"""

UNITS = 256

"""### The encoder


The encoder:

1. Takes a list of token IDs (from `context_text_processor`).
3. Looks up an embedding vector for each token (Using a `layers.Embedding`).
4. Processes the embeddings into a new sequence (Using a bidirectional `layers.GRU`).
5. Returns the processed sequence. This will be passed to the attention head.
"""

class Encoder(tf.keras.layers.Layer):
  def __init__(self, text_processor, units):
    super(Encoder, self).__init__()
    self.text_processor = text_processor
    self.vocab_size = text_processor.vocabulary_size()
    self.units = units
    
    # The embedding layer converts tokens to vectors
    self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,
                                               mask_zero=True)

    # The RNN layer processes those vectors sequentially.
    self.rnn = tf.keras.layers.Bidirectional(
        merge_mode='sum',
        layer=tf.keras.layers.GRU(units,
                            # Return the sequence and state
                            return_sequences=True,
                            recurrent_initializer='glorot_uniform'))

  def call(self, x):
    shape_checker = ShapeChecker()
    shape_checker(x, 'batch s')

    # 2. The embedding layer looks up the embedding vector for each token.
    x = self.embedding(x)
    shape_checker(x, 'batch s units')

    # 3. The GRU processes the sequence of embeddings.
    x = self.rnn(x)
    shape_checker(x, 'batch s units')

    # 4. Returns the new sequence of embeddings.
    return x

  def convert_input(self, texts):
    texts = tf.convert_to_tensor(texts)
    if len(texts.shape) == 0:
      texts = tf.convert_to_tensor(texts)[tf.newaxis]
    context = self.text_processor(texts).to_tensor()
    context = self(context)
    return context

# Encode the input sequence.
encoder = Encoder(context_text_processor, UNITS)
ex_context = encoder(ex_context_tok)

print(f'Context tokens, shape (batch, s): {ex_context_tok.shape}')
print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')

"""### The attention layer

The attention layer lets the decoder access the information extracted by the encoder. It computes a vector from the entire context sequence, and adds that to the decoder's output. 

"""

class CrossAttention(tf.keras.layers.Layer):
  def __init__(self, units, **kwargs):
    super().__init__()
    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)
    self.layernorm = tf.keras.layers.LayerNormalization()
    self.add = tf.keras.layers.Add()

  def call(self, x, context):
    shape_checker = ShapeChecker()
 
    shape_checker(x, 'batch t units')
    shape_checker(context, 'batch s units')

    attn_output, attn_scores = self.mha(
       query=x,
       value=context,
      return_attention_scores=True)
    
    shape_checker(x, 'batch t units')
    shape_checker(attn_scores, 'batch heads t s')
    
  #Cache the attention scores for plotting later.
    attn_scores = tf.reduce_mean(attn_scores, axis=1)
    shape_checker(attn_scores, 'batch t s')
    self.last_attention_weights = attn_scores

    x = self.add([x, attn_output])
    x = self.layernorm(x)

    return x

attention_layer = CrossAttention(UNITS)

# Attend to the encoded tokens
embed = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(),
                                 output_dim=UNITS, mask_zero=True)
ex_tar_embed = embed(ex_tar_in)

result = attention_layer(ex_tar_embed, ex_context)

print(f'Context sequence, shape (batch, s, units): {ex_context.shape}')
print(f'Target sequence, shape (batch, t, units): {ex_tar_embed.shape}')
print(f'Attention result, shape (batch, t, units): {result.shape}')
print(f'Attention weights, shape (batch, t, s):    {attention_layer.last_attention_weights.shape}')

"""The attention weights will sum to `1` over the context sequence, at each location in the target sequence."""

attention_layer.last_attention_weights[0].numpy().sum(axis=-1)

"""

Here are the attention weights across the context sequences at `t=0`:"""

attention_weights = attention_layer.last_attention_weights
mask=(ex_context_tok != 0).numpy()

plt.subplot(1, 2, 1)
plt.pcolormesh(mask*attention_weights[:, 0, :])
plt.title('Attention weights')

plt.subplot(1, 2, 2)
plt.pcolormesh(mask)
plt.title('Mask');

"""Because of the small-random initialization the attention weights are initially all close to `1/(sequence_length)`. The model will learn to make these less uniform as training progresses.

### The decoder

The decoder's job is to generate predictions for the next token at each location in the target sequence.

1. It looks up embeddings for each token in the target sequence.
2. It uses an RNN to process the target sequence, and keep track of what it has generated so far.
3. It uses RNN output as the "query" to the attention layer, when attending to the encoder's output.
4. At each location in the output it predicts the next token.

Here is the `Decoder` class' initializer. The initializer creates all the necessary layers.
"""

class Decoder(tf.keras.layers.Layer):
  @classmethod
  def add_method(cls, fun):
    setattr(cls, fun.__name__, fun)
    return fun

  def __init__(self, text_processor, units):
    super(Decoder, self).__init__()
    self.text_processor = text_processor
    self.vocab_size = text_processor.vocabulary_size()
    self.word_to_id = tf.keras.layers.StringLookup(
        vocabulary=text_processor.get_vocabulary(),
        mask_token='', oov_token='[UNK]')
    self.id_to_word = tf.keras.layers.StringLookup(
        vocabulary=text_processor.get_vocabulary(),
        mask_token='', oov_token='[UNK]',
        invert=True)
    self.start_token = self.word_to_id('[START]')
    self.end_token = self.word_to_id('[END]')

    self.units = units


    # 1. The embedding layer converts token IDs to vectors
    self.embedding = tf.keras.layers.Embedding(self.vocab_size,
                                               units, mask_zero=True)

    # 2. The RNN keeps track of what's been generated so far.
    self.rnn = tf.keras.layers.GRU(units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')

    # 3. The RNN output will be the query for the attention layer.
    self.attention = CrossAttention(units)

    # 4. This fully connected layer produces the logits for each
    # output token.
    self.output_layer = tf.keras.layers.Dense(self.vocab_size)

"""#### Training

Next, the `call` method, takes 3 arguments:

* `inputs` -  a `context, x` pair where:
  * `context` - is the context from the encoder's output.
  * `x` - is the target sequence input.
* `state` - Optional, the previous `state` output from the decoder (the internal state of the decoder's RNN). Pass the state from a previous run to continue generating text where you left off.
* `return_state` - [Default: False] - Set this to `True` to return the RNN state.
"""

@Decoder.add_method
def call(self,
         context, x,
         state=None,
         return_state=False):  
  shape_checker = ShapeChecker()
  shape_checker(x, 'batch t')
  shape_checker(context, 'batch s units')

  # 1. Lookup the embeddings
  x = self.embedding(x)
  shape_checker(x, 'batch t units')

  # 2. Process the target sequence.
  x, state = self.rnn(x, initial_state=state)
  shape_checker(x, 'batch t units')

  # 3. Use the RNN output as the query for the attention over the context.
  x = self.attention(x, context)
  self.last_attention_weights = self.attention.last_attention_weights
  shape_checker(x, 'batch t units')
  shape_checker(self.last_attention_weights, 'batch t s')

  # Step 4. Generate logit predictions for the next token.
  logits = self.output_layer(x)
  shape_checker(logits, 'batch t target_vocab_size')

  if return_state:
    return logits, state
  else:
    return logits

"""That will be sufficient for training. Create an instance of the decoder to test out:"""

decoder = Decoder(target_text_processor, UNITS)

"""Decoder usage"""

logits = decoder(ex_context, ex_tar_in)

print(f'encoder output shape: (batch, s, units) {ex_context.shape}')
print(f'input target tokens shape: (batch, t) {ex_tar_in.shape}')
print(f'logits shape shape: (batch, target_vocabulary_size) {logits.shape}')

"""#### Inference

For inference usage couple more methods.
"""

@Decoder.add_method
def get_initial_state(self, context):
  batch_size = tf.shape(context)[0]
  start_tokens = tf.fill([batch_size, 1], self.start_token)
  done = tf.zeros([batch_size, 1], dtype=tf.bool)
  embedded = self.embedding(start_tokens)
  return start_tokens, done, self.rnn.get_initial_state(embedded)[0]

@Decoder.add_method
def tokens_to_text(self, tokens):
  words = self.id_to_word(tokens)
  result = tf.strings.reduce_join(words, axis=-1, separator=' ')
  result = tf.strings.regex_replace(result, '^ *\[START\] *', '')
  result = tf.strings.regex_replace(result, ' *\[END\] *$', '')
  return result

@Decoder.add_method
def get_next_token(self, context, next_token, done, state, temperature = 0.0):
  logits, state = self(
    context, next_token,
    state = state,
    return_state=True) 
  
  if temperature == 0.0:
    next_token = tf.argmax(logits, axis=-1)
  else:
    logits = logits[:, -1, :]/temperature
    next_token = tf.random.categorical(logits, num_samples=1)

  # If a sequence produces an `end_token`, set it `done`
  done = done | (next_token == self.end_token)
  # Once a sequence is done it only produces 0-padding.
  next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)
  
  return next_token, done, state

"""With those extra functions, you can write a generation loop:"""

# Setup the loop variables.
next_token, done, state = decoder.get_initial_state(ex_context)
tokens = []

for n in range(10):
  # Run one step.
  next_token, done, state = decoder.get_next_token(
      ex_context, next_token, done, state, temperature=1.0)
  # Add the token to the output.
  tokens.append(next_token)

# Stack all the tokens together.
tokens = tf.concat(tokens, axis=-1) # (batch, t)

# Convert the tokens back to a a string
result = decoder.tokens_to_text(tokens)
#result[:3].numpy()

"""## The model

"""

class Translator(tf.keras.Model):
  @classmethod
  def add_method(cls, fun):
    setattr(cls, fun.__name__, fun)
    return fun

  def __init__(self, units,
               context_text_processor,
               target_text_processor):
    super().__init__()
    # Build the encoder and decoder
    encoder = Encoder(context_text_processor, units)
    decoder = Decoder(target_text_processor, units)

    self.encoder = encoder
    self.decoder = decoder

  def call(self, inputs):
    context, x = inputs
    context = self.encoder(context)
    logits = self.decoder(context, x)

    #TODO(b/250038731): remove this
    try:
      # Delete the keras mask, so keras doesn't scale the loss+accuracy. 
      del logits._keras_mask
    except AttributeError:
      pass

    return logits

"""During training the model will be used like this:"""

model = Translator(UNITS, context_text_processor, target_text_processor)

logits = model((ex_context_tok, ex_tar_in))

print(f'Context tokens, shape: (batch, s, units) {ex_context_tok.shape}')
print(f'Target tokens, shape: (batch, t) {ex_tar_in.shape}')
print(f'logits, shape: (batch, t, target_vocabulary_size) {logits.shape}')

"""### Train"""

def masked_loss(y_true, y_pred):
    # Calculate the loss for each item in the batch.
    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(
        from_logits=True, reduction='none')
    loss = loss_fn(y_true, y_pred)

    # Mask off the losses on padding.
    mask = tf.cast(y_true != 0, loss.dtype)
    loss *= mask

    # Return the total.
    return tf.reduce_sum(loss)/tf.reduce_sum(mask)

def masked_acc(y_true, y_pred):
    # Calculate the loss for each item in the batch.
    y_pred = tf.argmax(y_pred, axis=-1)
    y_pred = tf.cast(y_pred, y_true.dtype)
    
    match = tf.cast(y_true == y_pred, tf.float32)
    mask = tf.cast(y_true != 0, tf.float32)
    
    return tf.reduce_sum(match)/tf.reduce_sum(mask)

"""Configure the model for training:"""

model.compile(optimizer='Adam',
              loss=masked_loss, 
              metrics=[masked_acc, masked_loss])

"""The model is randomly initialized, and should give roughly uniform output probabilities. So it's easy to predict what the initial values of the metrics should be:"""

vocab_size = 1.0 * target_text_processor.vocabulary_size()

{"expected_loss": tf.math.log(vocab_size).numpy(),
 "expected_acc": 1/vocab_size}

"""That should roughly match the values returned by running a few steps of evaluation:"""

model.evaluate(val_ds, steps=70, return_dict=True)

history = model.fit(
    train_ds.repeat(), 
    epochs=100,
    steps_per_epoch = 100,
    validation_data=val_ds,
    validation_steps = 70,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(patience=8)])

"""## Plotting the Loss from Training """

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['masked_loss'], label='masked_loss')
plt.plot(history.history['val_masked_loss'], label='val_masked_loss')
plt.ylim([0, max(plt.ylim())])
plt.xlabel('Epoch #')
plt.ylabel('CE/token')
plt.legend()

"""## Plotting the aacuracy from the training"""

plt.plot(history.history['masked_acc'], label='accuracy')
plt.plot(history.history['val_masked_acc'], label='val_accuracy')
plt.ylim([0, max(plt.ylim())])
plt.xlabel('Epoch #')
plt.ylabel('CE/token')
plt.legend()

"""### Translate Module Development

Now that the model is trained, implement a function to execute the full `text => text` translation. This code is basically identical to the [inference example](#inference) in the [decoder section](#the_decoder), but this also captures the attention weights.
"""

#@title
@Translator.add_method
def translate(self,
              texts, *,
              max_length=50,
              temperature=0.0):
  # Process the input texts
  context = self.encoder.convert_input(texts)
  batch_size = tf.shape(texts)[0]

  # Setup the loop inputs
  tokens = []
  attention_weights = []
  next_token, done, state = self.decoder.get_initial_state(context)

  for _ in range(max_length):
    # Generate the next token
    next_token, done, state = self.decoder.get_next_token(
        context, next_token, done,  state, temperature)
        
    # Collect the generated tokens
    tokens.append(next_token)
    attention_weights.append(self.decoder.last_attention_weights)
    
    if tf.executing_eagerly() and tf.reduce_all(done):
      break

  # Stack the lists of tokens and attention weights.
  tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)
  self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)

  result = self.decoder.tokens_to_text(tokens)
  return result

"""Here are the two helper methods, used above, to convert tokens to text, and to get the next token:"""

#Individual translator mechanism, can be used to translate each data separately


result1 = model.translate([''])

result2 = model.translate([''])

result23 = model.translate([''])

result222 = model.translate([''])
#result1[0].numpy().decode()
#result2[0].numpy().decode()

"""### Attention plot generation after model training has been completed"""

#@title
@Translator.add_method
def plot_attention(self, text, **kwargs):
  assert isinstance(text, str)
  output = self.translate([text], **kwargs)
  output = output[0].numpy().decode()

  attention = self.last_attention_weights[0]

  context = tf_lower_and_split_punct(text)
  context = context.numpy().decode().split()

  output = tf_lower_and_split_punct(output)
  output = output.numpy().decode().split()[1:]

  fig = plt.figure(figsize=(10, 10))
  ax = fig.add_subplot(1, 1, 1)

  ax.matshow(attention, cmap='viridis', vmin=0.0)

  fontdict = {'fontsize': 14}

  ax.set_xticklabels([''] + context, fontdict=fontdict, rotation=90)
  ax.set_yticklabels([''] + output, fontdict=fontdict)

  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

  ax.set_xlabel('Input text')
  ax.set_ylabel('Output text')

#model.plot_attention('')

"""Translate a few more sentences and plot them:

The short sentences often work well, but if the input is too long the model literally loses focus and stops providing reasonable predictions. There are two main reasons for this:

1. The model was trained with teacher-forcing feeding the correct token at each step, regardless of the model's predictions. The model could be made more robust if it were sometimes fed its own predictions.
2. The model only has access to its previous output through the RNN state. If the RNN state looses track of where it was in the context sequence there's no way for the model to recover. [Transformers](transformer.ipynb) improve on this by letting the decoder look at what it has output so far.

The raw data is sorted by length, so try translating the longest sequence:
"""

long_text = context_raw[-1]

import textwrap
#print('Expected output:\n', '\n'.join(textwrap.wrap(target_raw[-1])))

"""# Testing unseen samples"""

import pandas as pd

dc = pd.read_excel('Customer_Order_authentic_dataset_space.xlsx')

dc.head()

"""### Separating Columns in X_test and y_test"""

X_test2 = dc['OM_Regular'].values
y_test2 = dc['OM_Prediction'].values

print(X_test2.shape)
print(y_test2.shape)

print("X data type: ", X_test2.dtype)
print("y data type: ", y_test2.dtype)

print(y_test2)

inputs = X_test2

"""### Obtaining results from the model of the unseen dataset"""

# Commented out IPython magic to ensure Python compatibility.
#  %%time
 for t in inputs:
  mylist_res = model.translate([t])[0].numpy().decode()
  print(model.translate([t])[0].numpy().decode())

 print()

"""# Classification Report (Unseen samples)

"""

import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score
from sklearn.metrics import precision_recall_curve, classification_report

"""### The result is obtained and captured in a separate file, labels are converted to 1 and 0 . Where 1 denotes P and 0 denotes NP.

###READING the predicted dataset
"""

dd = pd.read_excel('Customer_Order_authentic_dataset_space_pred_2.xlsx')

dd.head()

X_test_pred2 = dd['OM_Regular'].values
y_test_pred2 = dd['OM_Prediction'].values

"""### Printing predicted labels"""

print (y_test_pred2 )

precision = precision_score(y_test2, y_test_pred2) 
print("Testing: Precision = %f" % precision)


recall = recall_score(y_test2, y_test_pred2)
print("Testing: Recall = %f" % recall)


f1 = f1_score(y_test2, y_test_pred2)
print("Testing: F1 Score = %f" % f1)

print("\nConfusion Matrix (Test Data):\n", confusion_matrix(y_test2, y_test_pred2))

print(classification_report(y_test2,y_test_pred2))