{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SIr2IisslaWO"
      },
      "outputs": [],
      "source": [
        "# ! pip install transformers datasets\n",
        "# ! pip install rouge-score\n",
        "# ! pip install huggingface_hub\n",
        "# ! pip install keras_nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization"
      ],
      "metadata": {
        "id": "y2uRkT6G6BvN"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable mixed precision for better performance on GPUs with Tensor Cores\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Check the number of available GPUs\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "num_gpus = len(gpus)\n",
        "if num_gpus > 1:\n",
        "    # Enable mirrored strategy for distributed training\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    print('Running distributed training on', num_gpus, 'GPUs.')\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "    print('Running training on a single GPU.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlHZNX0C6Do9",
        "outputId": "7f1c9ecf-0cfb-40ad-b129-2ca9f85180aa"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running training on a single GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jU8fQael6JX5"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data from the Excel file\n",
        "file_path = \"raw_8_om_training_set.xlsx\"\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Extract the source and target sequences\n",
        "source_sequences = data[\"OM_Regular\"].tolist()\n",
        "target_sequences = data[\"OM_Prediction\"].tolist()\n",
        "\n",
        "# Concatenate \"[start]\" and \"[end]\" to each target sequence\n",
        "target_sequences = [\"[start] \" + text + \" [end]\" for text in target_sequences]\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_src, val_test_src, train_tgt, val_test_tgt = train_test_split(source_sequences, target_sequences, test_size=0.3, random_state=42)\n",
        "val_src, test_src, val_tgt, test_tgt = train_test_split(val_test_src, val_test_tgt, test_size=0.5, random_state=42)\n",
        "\n",
        "# Combine the source and target sequences into pairs\n",
        "train_pairs = list(zip(train_src, train_tgt))\n",
        "val_pairs = list(zip(val_src, val_tgt))\n",
        "test_pairs = list(zip(test_src, test_tgt))\n",
        "\n",
        "print(f\"Total pairs: {len(train_pairs) + len(val_pairs) + len(test_pairs)}\")\n",
        "print(f\"Training pairs: {len(train_pairs)}\")\n",
        "print(f\"Validation pairs: {len(val_pairs)}\")\n",
        "print(f\"Test pairs: {len(test_pairs)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GI6OKKH6cSk",
        "outputId": "9c9cc018-25e0-4784-c3ae-e92d4a47aa2a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pairs: 13336\n",
            "Training pairs: 9335\n",
            "Validation pairs: 2000\n",
            "Test pairs: 2001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Define the maximum lengths for input and target sequences\n",
        "MAX_INPUT_LENGTH = 500\n",
        "MAX_TARGET_LENGTH = 500\n",
        "\n",
        "# Load the data from the Excel file\n",
        "file_path = \"raw_8_om_training_set.xlsx\"\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Extract the source and target sequences\n",
        "source_sequences = data[\"OM_Regular\"].tolist()\n",
        "target_sequences = data[\"OM_Prediction\"].tolist()\n",
        "\n",
        "# Initialize the T5 tokenizer with customized vocab size\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\", vocab_size=15000)\n",
        "\n",
        "# Tokenize the sequences\n",
        "tokenized_sequences = []\n",
        "for source, target in zip(source_sequences, target_sequences):\n",
        "    tokenized_input = tokenizer(source, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    tokenized_target = tokenizer(target, max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "    tokenized_sequence_pair = {\n",
        "        \"input_ids\": tokenized_input.input_ids,\n",
        "        \"attention_mask\": tokenized_input.attention_mask,\n",
        "        \"decoder_input_ids\": tokenized_target.input_ids,\n",
        "        \"decoder_attention_mask\": tokenized_target.attention_mask\n",
        "    }\n",
        "\n",
        "    tokenized_sequences.append(tokenized_sequence_pair)\n",
        "\n",
        "# Save the tokenizer model in the current directory\n",
        "tokenizer.save_pretrained(\".\")\n",
        "\n",
        "# The tokenizer model will be saved in the current directory.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxOUYmTEppk7",
        "outputId": "6c59e840-a680-416e-da3f-57251ce3ea48"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./tokenizer_config.json',\n",
              " './special_tokens_map.json',\n",
              " './spiece.model',\n",
              " './added_tokens.json',\n",
              " './tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Print tokenized data for the first sequence pair\n",
        "print(\"Tokenized Input:\")\n",
        "print(tokenized_sequences[0][\"input_ids\"])\n",
        "print(\"Tokenized Target:\")\n",
        "print(tokenized_sequences[0][\"decoder_input_ids\"])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfUbHnPZsh8M",
        "outputId": "eabc057d-0424-429e-e38e-4d5a649aa264"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Input:\n",
            "tensor([[ 6008,  2204,  9505,    35, 28449,   782,     3, 13658,  5570, 24722,\n",
            "          4285,     7,  4501,     2,   144,    17,    52, 17175,  3274,   564,\n",
            "          6633,  4309,  1220,  4350,  6633, 23954,    23,    26,  2423,  4350,\n",
            "          6633,  4309,   159,  8952,     7,  6471,    17,  3274,  5388,    32,\n",
            "          4208,     2,   782,     3, 13658,   564,  6633,  4309,  4285,     7,\n",
            "            86,    17,    15,  1304,     2,   782,     3, 13658,   564,  6633,\n",
            "         23954,  4285,     7,  6108,     2,   782,     3, 13658, 12928,   179,\n",
            "          4285,     7,  4501,     2,   144,    17,    52, 17175,  3274,  7660,\n",
            "          4309,  1220,  9504,   179, 23954,    23,    26,  2423,  9504,   179,\n",
            "          4309,   159,  8952,     7,  6471,    17,  3274,  5388,    32,  4208,\n",
            "             2,   782,     3, 13658,  7660,  4309,  4285,     7,    86,    17,\n",
            "            15,  1304,     2,   782,     3, 13658,  7660, 23954,  4285,     7,\n",
            "          6108,     2,   782,     3, 13658, 28898,  2009,  4285,     7,  4501,\n",
            "             2,   144,    17,    52, 17175,  3274,  1675,  4309,  1220,    60,\n",
            "          6105,  2009, 23954,    23,    26,  2423,    60,  6105,  2009,  4309,\n",
            "           159,  8952,     7,  6471,    17,  3274,  5388,    32,  4208,     2,\n",
            "           782,     3, 13658,  1675,  4309,  4285,     7,    86,    17,    15,\n",
            "          1304,     2,   782,     3, 13658,  1675, 23954,  4285,     7,  6108,\n",
            "             2,   782,     3, 13658, 10137, 25714,  4285,     7,  2125,     2,\n",
            "             7,    52,    75,  3274, 12928,   179,    26,     7,    17,  3274,\n",
            "         28898,  2009,     7,    52,    75,   834, 23829, 10435,   485,  3274,\n",
            "             3,  7894,    26,     7,    17,   834, 23829, 10435,   485,  3274,\n",
            "             3,  9312,   476,     2,   782,     3, 13658,  2158,   109,  4285,\n",
            "             7,  4501,     2,   144,    17,    52, 17175,  3274,  1075,  4309,\n",
            "          1220,  3491,    15, 23954,    23,    26,  2423,  3491,    15,  4309,\n",
            "           159,  8952,     7,  6471,    17,  3274,  5388,    32,  4208,     2,\n",
            "           782,     3, 13658,  1075,  4309,  4285,     7,    86,    17,    15,\n",
            "          1304,     2,   782,     3, 13658,  1075, 23954,  4285,     7,  6108,\n",
            "             2,   782,     3, 13658, 25006, 24722,  4285,     7,  4501,     2,\n",
            "           144,    17,    52, 17175,  3274,  1357, 24722,  4309,  1220,   221,\n",
            "         18901, 24722, 23954,    23,    26,  2423,   221, 18901, 24722,  4309,\n",
            "           159,  8952,     7,  6471,    17,  3274,  5388,    32,  4208,     2,\n",
            "           782,     3, 13658,  1357, 24722,  4309,  4285,     7,    86,    17,\n",
            "            15,  1304,     2,   782,     3, 13658,  1357, 24722, 23954,  4285,\n",
            "             7,  6108,     2,   782,     3, 13658,  2158,   109,   279,    77,\n",
            "            26,    53,     7, 25714,  4285,     7,  2125,     2,     7,    52,\n",
            "            75,  3274, 15077,   288,    26,     7,    17,  3274,  2158,   965,\n",
            "            52,    75,   834, 23829, 10435,   485,  3274,     3,  9312,   476,\n",
            "            26,     7,    17,   834, 23829, 10435,   485,  3274,     3,  9312,\n",
            "           476,     2,   782,     3, 13658, 15077,   288,  4285,     7,  4501,\n",
            "             2,   144,    17,    52, 17175,  2423,  2274,    23,  3389,   288,\n",
            "          4309,    23,    26,  2423,  2274,    23,  3389,   288,  4309,   159,\n",
            "          8952,     7,  6471,    17,  3274,  5388,    32,  4208,     2,   782,\n",
            "             3, 13658,  8344,  4309,  4285,     7,    86,    17,    15,  1304,\n",
            "             2,   782,     3, 13658,    93, 18901, 24722, 13725,    23,  3389,\n",
            "          2366, 25714,  4285,     7,  2125,     2,     7,    52,    75,  3274,\n",
            "         25006, 24722,    26,     7,    17,  3274, 19204,    52,    75,   834,\n",
            "         23829, 10435,   485,  3274,     3,  9312,   476,    26,     7,    17,\n",
            "           834, 23829, 10435,   485,  3274,     3,  9312,   476,     2,   782,\n",
            "             3, 13658,    93, 18901, 24722,   553,  1665,   179,     7,     1]])\n",
            "Tokenized Target:\n",
            "tensor([[  276,     6, 22763,    15,  2204,  9505,    35, 28449,   782,     3,\n",
            "         13658,  5570, 24722,  4285,     7,  4501,     2,   144,    17,    52,\n",
            "         17175,  3274,   564,  6633,  4309,  1220,  4350,  6633, 23954,    23,\n",
            "            26,  2423,  4350,  6633,  4309,   159,  8952,     7,  6471,    17,\n",
            "          3274,  5388,    32,  4208,     2,   782,     3, 13658,   564,  6633,\n",
            "          4309,  4285,     7,    86,    17,    15,  1304,     2,   782,     3,\n",
            "         13658,   564,  6633, 23954,  4285,     7,  6108,     2,   782,     3,\n",
            "         13658, 12928,   179,  4285,     7,  4501,     2,   144,    17,    52,\n",
            "         17175,  3274,  7660,  4309,  1220,  9504,   179, 23954,    23,    26,\n",
            "          2423,  9504,   179,  4309,   159,  8952,     7,  6471,    17,  3274,\n",
            "          5388,    32,  4208,     2,   782,     3, 13658,  7660,  4309,  4285,\n",
            "             7,    86,    17,    15,  1304,     2,   782,     3, 13658,  7660,\n",
            "         23954,  4285,     7,  6108,     2,   782,     3, 13658, 28898,  2009,\n",
            "          4285,     7,  4501,     2,   144,    17,    52, 17175,  3274,  1675,\n",
            "          4309,  1220,    60,  6105,  2009, 23954,    23,    26,  2423,    60,\n",
            "          6105,  2009,  4309,   159,  8952,     7,  6471,    17,  3274,  5388,\n",
            "            32,  4208,     2,   782,     3, 13658,  1675,  4309,  4285,     7,\n",
            "            86,    17,    15,  1304,     2,   782,     3, 13658,  1675, 23954,\n",
            "          4285,     7,  6108,     2,   782,     3, 13658, 10137, 25714,  4285,\n",
            "             7,  2125,     2,     7,    52,    75,  3274, 12928,   179,    26,\n",
            "             7,    17,  3274, 28898,  2009,     7,    52,    75,   834, 23829,\n",
            "         10435,   485,  3274,     3,  7894,    26,     7,    17,   834, 23829,\n",
            "         10435,   485,  3274,     3,  9312,   476,     2,   782,     3, 13658,\n",
            "          2158,   109,  4285,     7,  4501,     2,   144,    17,    52, 17175,\n",
            "          3274,  1075,  4309,  1220,  3491,    15, 23954,    23,    26,  2423,\n",
            "          3491,    15,  4309,   159,  8952,     7,  6471,    17,  3274,  5388,\n",
            "            32,  4208,     2,   782,     3, 13658,  1075,  4309,  4285,     7,\n",
            "            86,    17,    15,  1304,     2,   782,     3, 13658,  1075, 23954,\n",
            "          4285,     7,  6108,     2,   782,     3, 13658, 25006, 24722,  4285,\n",
            "             7,  4501,     2,   144,    17,    52, 17175,  3274,  1357, 24722,\n",
            "          4309,  1220,   221, 18901, 24722, 23954,    23,    26,  2423,   221,\n",
            "         18901, 24722,  4309,   159,  8952,     7,  6471,    17,  3274,  5388,\n",
            "            32,  4208,     2,   782,     3, 13658,  1357, 24722,  4309,  4285,\n",
            "             7,    86,    17,    15,  1304,     2,   782,     3, 13658,  1357,\n",
            "         24722, 23954,  4285,     7,  6108,     2,   782,     3, 13658,  2158,\n",
            "           109,   279,    77,    26,    53,     7, 25714,  4285,     7,  2125,\n",
            "             2,     7,    52,    75,  3274, 15077,   288,    26,     7,    17,\n",
            "          3274,  2158,   965,    52,    75,   834, 23829, 10435,   485,  3274,\n",
            "             3,  9312,   476,    26,     7,    17,   834, 23829, 10435,   485,\n",
            "          3274,     3,  9312,   476,     2,   782,     3, 13658, 15077,   288,\n",
            "          4285,     7,  4501,     2,   144,    17,    52, 17175,  2423,  2274,\n",
            "            23,  3389,   288,  4309,    23,    26,  2423,  2274,    23,  3389,\n",
            "           288,  4309,   159,  8952,     7,  6471,    17,  3274,  5388,    32,\n",
            "          4208,     2,   782,     3, 13658,  8344,  4309,  4285,     7,    86,\n",
            "            17,    15,  1304,     2,   782,     3, 13658,    93, 18901, 24722,\n",
            "         13725,    23,  3389,  2366, 25714,  4285,     7,  2125,     2,     7,\n",
            "            52,    75,  3274, 25006, 24722,    26,     7,    17,  3274, 19204,\n",
            "            52,    75,   834, 23829, 10435,   485,  3274,     3,  9312,   476,\n",
            "            26,     7,    17,   834, 23829, 10435,   485,  3274,     3,  9312,\n",
            "           476,     2,   782,     3, 13658,    93, 18901, 24722,   553,     1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert token IDs back to tokens using the tokenizer\n",
        "input_ids = tokenized_sequences[0][\"input_ids\"].tolist()\n",
        "decoder_input_ids = tokenized_sequences[0][\"decoder_input_ids\"].tolist()\n",
        "\n",
        "input_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "target_tokens = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n",
        "\n",
        "# Print tokenized data as tokens\n",
        "print(\"Tokenized Input Tokens:\")\n",
        "print(input_tokens)\n",
        "print(\"Tokenized Target Tokens:\")\n",
        "print(target_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkiSdNP1s_zV",
        "outputId": "56a54c1a-1ef3-474e-d4a1-f44d52326f69"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Input Tokens:\n",
            "['▁module', '▁decide', 'rop', 'en', '▁Declaration', 'one', '▁', 'sig', '▁Name', 'Space', '▁extend', 's', '▁Class', '<unk>', 'at', 't', 'r', 'Set', '▁=', '▁name', 'space', 'ID', '+', 'name', 'space', 'Name', 'i', 'd', '=', 'name', 'space', 'ID', 'is', 'Ab', 's', 'trac', 't', '▁=', '▁Non', 'o', '▁parent', '<unk>', 'one', '▁', 'sig', '▁name', 'space', 'ID', '▁extend', 's', '▁In', 't', 'e', 'ger', '<unk>', 'one', '▁', 'sig', '▁name', 'space', 'Name', '▁extend', 's', '▁string', '<unk>', 'one', '▁', 'sig', '▁Vari', 'able', '▁extend', 's', '▁Class', '<unk>', 'at', 't', 'r', 'Set', '▁=', '▁variable', 'ID', '+', 'vari', 'able', 'Name', 'i', 'd', '=', 'vari', 'able', 'ID', 'is', 'Ab', 's', 'trac', 't', '▁=', '▁Non', 'o', '▁parent', '<unk>', 'one', '▁', 'sig', '▁variable', 'ID', '▁extend', 's', '▁In', 't', 'e', 'ger', '<unk>', 'one', '▁', 'sig', '▁variable', 'Name', '▁extend', 's', '▁string', '<unk>', 'one', '▁', 'sig', '▁Relation', 'ship', '▁extend', 's', '▁Class', '<unk>', 'at', 't', 'r', 'Set', '▁=', '▁relationship', 'ID', '+', 're', 'lation', 'ship', 'Name', 'i', 'd', '=', 're', 'lation', 'ship', 'ID', 'is', 'Ab', 's', 'trac', 't', '▁=', '▁Non', 'o', '▁parent', '<unk>', 'one', '▁', 'sig', '▁relationship', 'ID', '▁extend', 's', '▁In', 't', 'e', 'ger', '<unk>', 'one', '▁', 'sig', '▁relationship', 'Name', '▁extend', 's', '▁string', '<unk>', 'one', '▁', 'sig', '▁Var', 'Association', '▁extend', 's', '▁Association', '<unk>', 's', 'r', 'c', '▁=', '▁Vari', 'able', 'd', 's', 't', '▁=', '▁Relation', 'ship', 's', 'r', 'c', '_', 'multi', 'plic', 'ity', '▁=', '▁', 'ONE', 'd', 's', 't', '_', 'multi', 'plic', 'ity', '▁=', '▁', 'MAN', 'Y', '<unk>', 'one', '▁', 'sig', '▁Ro', 'le', '▁extend', 's', '▁Class', '<unk>', 'at', 't', 'r', 'Set', '▁=', '▁role', 'ID', '+', 'rol', 'e', 'Name', 'i', 'd', '=', 'rol', 'e', 'ID', 'is', 'Ab', 's', 'trac', 't', '▁=', '▁Non', 'o', '▁parent', '<unk>', 'one', '▁', 'sig', '▁role', 'ID', '▁extend', 's', '▁In', 't', 'e', 'ger', '<unk>', 'one', '▁', 'sig', '▁role', 'Name', '▁extend', 's', '▁string', '<unk>', 'one', '▁', 'sig', '▁Decision', 'Space', '▁extend', 's', '▁Class', '<unk>', 'at', 't', 'r', 'Set', '▁=', '▁decision', 'Space', 'ID', '+', 'de', 'cision', 'Space', 'Name', 'i', 'd', '=', 'de', 'cision', 'Space', 'ID', 'is', 'Ab', 's', 'trac', 't', '▁=', '▁Non', 'o', '▁parent', '<unk>', 'one', '▁', 'sig', '▁decision', 'Space', 'ID', '▁extend', 's', '▁In', 't', 'e', 'ger', '<unk>', 'one', '▁', 'sig', '▁decision', 'Space', 'Name', '▁extend', 's', '▁string', '<unk>', 'one', '▁', 'sig', '▁Ro', 'le', 'B', 'in', 'd', 'ing', 's', 'Association', '▁extend', 's', '▁Association', '<unk>', 's', 'r', 'c', '▁=', '▁Particip', 'ant', 'd', 's', 't', '▁=', '▁Ro', 'les', 'r', 'c', '_', 'multi', 'plic', 'ity', '▁=', '▁', 'MAN', 'Y', 'd', 's', 't', '_', 'multi', 'plic', 'ity', '▁=', '▁', 'MAN', 'Y', '<unk>', 'one', '▁', 'sig', '▁Particip', 'ant', '▁extend', 's', '▁Class', '<unk>', 'at', 't', 'r', 'Set', '=', 'part', 'i', 'cip', 'ant', 'ID', 'i', 'd', '=', 'part', 'i', 'cip', 'ant', 'ID', 'is', 'Ab', 's', 'trac', 't', '▁=', '▁Non', 'o', '▁parent', '<unk>', 'one', '▁', 'sig', '▁participant', 'ID', '▁extend', 's', '▁In', 't', 'e', 'ger', '<unk>', 'one', '▁', 'sig', '▁des', 'cision', 'Space', 'Part', 'i', 'cip', 'ants', 'Association', '▁extend', 's', '▁Association', '<unk>', 's', 'r', 'c', '▁=', '▁Decision', 'Space', 'd', 's', 't', '▁=', '▁Participants', 'r', 'c', '_', 'multi', 'plic', 'ity', '▁=', '▁', 'MAN', 'Y', 'd', 's', 't', '_', 'multi', 'plic', 'ity', '▁=', '▁', 'MAN', 'Y', '<unk>', 'one', '▁', 'sig', '▁des', 'cision', 'Space', 'V', 'ari', 'able', 's', '</s>']\n",
            "Tokenized Target Tokens:\n",
            "['▁P', ',', 'modul', 'e', '▁decide', 'rop', 'en', '▁Declaration', 'one', '▁', 'sig', '▁Name', 'Space', '▁extend', 's', '▁Class', '<unk>', 'at', 't', 'r', 'Set', '▁=', '▁name', 'space', 'ID', '+', 'name', 'space', 'Name', 'i', 'd', '=', 'name', 'space', 'ID', 'is', 'Ab', 's', 'trac', 't', '▁=', '▁Non', 'o', '▁parent', '<unk>', 'one', '▁', 'sig', '▁name', 'space', 'ID', '▁extend', 's', '▁In', 't', 'e', 'ger', '<unk>', 'one', '▁', 'sig', '▁name', 'space', 'Name', '▁extend', 's', '▁string', '<unk>', 'one', '▁', 'sig', '▁Vari', 'able', '▁extend', 's', '▁Class', '<unk>', 'at', 't', 'r', 'Set', '▁=', '▁variable', 'ID', '+', 'vari', 'able', 'Name', 'i', 'd', '=', 'vari', 'able', 'ID', 'is', 'Ab', 's', 'trac', 't', '▁=', '▁Non', 'o', '▁parent', '<unk>', 'one', '▁', 'sig', '▁variable', 'ID', '▁extend', 's', '▁In', 't', 'e', 'ger', '<unk>', 'one', '▁', 'sig', '▁variable', 'Name', '▁extend', 's', '▁string', '<unk>', 'one', '▁', 'sig', '▁Relation', 'ship', '▁extend', 's', '▁Class', '<unk>', 'at', 't', 'r', 'Set', '▁=', '▁relationship', 'ID', '+', 're', 'lation', 'ship', 'Name', 'i', 'd', '=', 're', 'lation', 'ship', 'ID', 'is', 'Ab', 's', 'trac', 't', '▁=', '▁Non', 'o', '▁parent', '<unk>', 'one', '▁', 'sig', '▁relationship', 'ID', '▁extend', 's', '▁In', 't', 'e', 'ger', '<unk>', 'one', '▁', 'sig', '▁relationship', 'Name', '▁extend', 's', '▁string', '<unk>', 'one', '▁', 'sig', '▁Var', 'Association', '▁extend', 's', '▁Association', '<unk>', 's', 'r', 'c', '▁=', '▁Vari', 'able', 'd', 's', 't', '▁=', '▁Relation', 'ship', 's', 'r', 'c', '_', 'multi', 'plic', 'ity', '▁=', '▁', 'ONE', 'd', 's', 't', '_', 'multi', 'plic', 'ity', '▁=', '▁', 'MAN', 'Y', '<unk>', 'one', '▁', 'sig', '▁Ro', 'le', '▁extend', 's', '▁Class', '<unk>', 'at', 't', 'r', 'Set', '▁=', '▁role', 'ID', '+', 'rol', 'e', 'Name', 'i', 'd', '=', 'rol', 'e', 'ID', 'is', 'Ab', 's', 'trac', 't', '▁=', '▁Non', 'o', '▁parent', '<unk>', 'one', '▁', 'sig', '▁role', 'ID', '▁extend', 's', '▁In', 't', 'e', 'ger', '<unk>', 'one', '▁', 'sig', '▁role', 'Name', '▁extend', 's', '▁string', '<unk>', 'one', '▁', 'sig', '▁Decision', 'Space', '▁extend', 's', '▁Class', '<unk>', 'at', 't', 'r', 'Set', '▁=', '▁decision', 'Space', 'ID', '+', 'de', 'cision', 'Space', 'Name', 'i', 'd', '=', 'de', 'cision', 'Space', 'ID', 'is', 'Ab', 's', 'trac', 't', '▁=', '▁Non', 'o', '▁parent', '<unk>', 'one', '▁', 'sig', '▁decision', 'Space', 'ID', '▁extend', 's', '▁In', 't', 'e', 'ger', '<unk>', 'one', '▁', 'sig', '▁decision', 'Space', 'Name', '▁extend', 's', '▁string', '<unk>', 'one', '▁', 'sig', '▁Ro', 'le', 'B', 'in', 'd', 'ing', 's', 'Association', '▁extend', 's', '▁Association', '<unk>', 's', 'r', 'c', '▁=', '▁Particip', 'ant', 'd', 's', 't', '▁=', '▁Ro', 'les', 'r', 'c', '_', 'multi', 'plic', 'ity', '▁=', '▁', 'MAN', 'Y', 'd', 's', 't', '_', 'multi', 'plic', 'ity', '▁=', '▁', 'MAN', 'Y', '<unk>', 'one', '▁', 'sig', '▁Particip', 'ant', '▁extend', 's', '▁Class', '<unk>', 'at', 't', 'r', 'Set', '=', 'part', 'i', 'cip', 'ant', 'ID', 'i', 'd', '=', 'part', 'i', 'cip', 'ant', 'ID', 'is', 'Ab', 's', 'trac', 't', '▁=', '▁Non', 'o', '▁parent', '<unk>', 'one', '▁', 'sig', '▁participant', 'ID', '▁extend', 's', '▁In', 't', 'e', 'ger', '<unk>', 'one', '▁', 'sig', '▁des', 'cision', 'Space', 'Part', 'i', 'cip', 'ants', 'Association', '▁extend', 's', '▁Association', '<unk>', 's', 'r', 'c', '▁=', '▁Decision', 'Space', 'd', 's', 't', '▁=', '▁Participants', 'r', 'c', '_', 'multi', 'plic', 'ity', '▁=', '▁', 'MAN', 'Y', 'd', 's', 't', '_', 'multi', 'plic', 'ity', '▁=', '▁', 'MAN', 'Y', '<unk>', 'one', '▁', 'sig', '▁des', 'cision', 'Space', 'V', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vIWgRDlyvQEd"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tJDkl0iYvfUh"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}