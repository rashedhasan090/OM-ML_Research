{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YJnMTPsgYlc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "\n",
        "#Experiment with P Oversample , NP Undersample \n",
        "###6 OM - Dataset , Camping, Bank,  Library Management, Decider, Customer_order, E-Commerce\n",
        "###1 OM - Testing - Online_Store\n",
        "\n",
        "## Training \n",
        "\n",
        "### Total instances - 330\n",
        "\n",
        "### P samples - 216\n",
        "### NP samples - 114\n",
        "\n",
        "## Testing \n",
        "\n",
        "### Total instances - 8\n",
        "\n",
        "### P samples - 4\n",
        "### NP samples - 4 \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAmSR1FaqKrl"
      },
      "source": [
        "## Setup (installing necessary libraries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "DGFTkuRvzWqc"
      },
      "outputs": [],
      "source": [
        "# !pip install \"tensorflow-text>=2.10\"\n",
        "# !pip install einops"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Libraries "
      ],
      "metadata": {
        "id": "A07RWC45HcG0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pathlib\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Defining the Shapechecker"
      ],
      "metadata": {
        "id": "h87kqCNBHly5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KqFqKi4fqN9X"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    parsed = einops.parse_shape(tensor, names)\n",
        "\n",
        "    for name, new_dim in parsed.items():\n",
        "      old_dim = self.shapes.get(name, None)\n",
        "      \n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB7rgJDbeBDF"
      },
      "source": [
        "# Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "daNcrh1lVej7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "ORM_data = pd.read_csv('6-OM-OS-test.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reading Dat from Dataset"
      ],
      "metadata": {
        "id": "KbiGtupGHyJd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ve7kyoOxWY1u",
        "outputId": "2494d4d0-0e0d-4b77-c700-9a6d47f7cbd6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          OM_Regular  \\\n",
              "0  moduleOM_name:0openDeclarationonesigclass1_nam...   \n",
              "1  moduleOM_name:0openDeclarationonesigclass1_nam...   \n",
              "2  moduleOM_name:0openDeclarationonesigclass1_nam...   \n",
              "3  moduleOM_name:0openDeclarationonesigclass1_nam...   \n",
              "4  moduleOM_name:0openDeclarationonesigclass1_nam...   \n",
              "\n",
              "                                       OM_Prediction  \n",
              "0  moduleOM_name:0openDeclarationonesigclass1_nam...  \n",
              "1  moduleOM_name:0openDeclarationonesigclass1_nam...  \n",
              "2  moduleOM_name:0openDeclarationonesigclass1_nam...  \n",
              "3  moduleOM_name:0openDeclarationonesigclass1_nam...  \n",
              "4  moduleOM_name:0openDeclarationonesigclass1_nam...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-975c1833-ee45-44ff-b8b0-a41606082715\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OM_Regular</th>\n",
              "      <th>OM_Prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>moduleOM_name:0openDeclarationonesigclass1_nam...</td>\n",
              "      <td>moduleOM_name:0openDeclarationonesigclass1_nam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>moduleOM_name:0openDeclarationonesigclass1_nam...</td>\n",
              "      <td>moduleOM_name:0openDeclarationonesigclass1_nam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>moduleOM_name:0openDeclarationonesigclass1_nam...</td>\n",
              "      <td>moduleOM_name:0openDeclarationonesigclass1_nam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>moduleOM_name:0openDeclarationonesigclass1_nam...</td>\n",
              "      <td>moduleOM_name:0openDeclarationonesigclass1_nam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>moduleOM_name:0openDeclarationonesigclass1_nam...</td>\n",
              "      <td>moduleOM_name:0openDeclarationonesigclass1_nam...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-975c1833-ee45-44ff-b8b0-a41606082715')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-975c1833-ee45-44ff-b8b0-a41606082715 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-975c1833-ee45-44ff-b8b0-a41606082715');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "ORM_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V7OaHrVYV-Xd"
      },
      "outputs": [],
      "source": [
        "OM_Regular = ORM_data['OM_Regular'].values\n",
        "OM_Prediction = ORM_data['OM_Prediction'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jTBVOEjFWAI5"
      },
      "outputs": [],
      "source": [
        "X = OM_Regular\n",
        "Y = OM_Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOujEo2geGod"
      },
      "source": [
        "#### Dividing data as Target and Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "cTbSbBz55QtF"
      },
      "outputs": [],
      "source": [
        "# target_raw =  Y\n",
        "# context_raw = X\n",
        "# print(context_raw[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "lH_dPY8TRp3c"
      },
      "outputs": [],
      "source": [
        "# print(target_raw[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfVWx3WaI5Df"
      },
      "source": [
        "From these arrays of strings you can create a `tf.data.Dataset` of strings that shuffles and batches them efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3rZFgz69nMPa"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(context_raw)\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
        "\n",
        "train_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw[is_train], target_raw[is_train]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE))\n",
        "val_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw[~is_train], target_raw[~is_train]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "qc6-NK1GtWQt"
      },
      "outputs": [],
      "source": [
        "# for example_context_strings, example_target_strings in train_raw.take(1):\n",
        "#   print(example_context_strings[:5])\n",
        "#   print()\n",
        "#   print(example_target_strings[:5])\n",
        "#   break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCoxLcuN3bwv"
      },
      "source": [
        "### Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kwdPcHvzz_a"
      },
      "source": [
        "One of the goals of this tutorial is to build a model that can be exported as a `tf.saved_model`. To make that exported model useful it should take `tf.string` inputs, and return `tf.string` outputs: All the text processing happens inside the model. Mainly using a `layers.TextVectorization` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOQ5n55X4uDB"
      },
      "source": [
        "#### Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upKhKAMK4zzI"
      },
      "source": [
        "The model is dealing with multilingual text with a limited vocabulary. So it will be important to standardize the input text.\n",
        "\n",
        "The first step is Unicode normalization to split accented characters and replace compatibility characters with their ASCII equivalents.\n",
        "\n",
        "The `tensorflow_text` package contains a unicode normalize operation, We may or may not decide to Use this for ORM data. I kept it in the experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mD0e-DWGQ2Vo"
      },
      "outputs": [],
      "source": [
        "example_text = tf.constant('moduleOM_nameopenDeclarationonesigclass1_nameextendsClassattrSet=c1_at1+c1_at2id=c1_at1noparentisAbstract=No}onesigc1_at1extendsc1_at1_typeonesigc1_at2extendsc1_at2_typeonesigclass2_nameextendsClassattrSet=c2_at1+c2_at2+c2_at3+c2_at4id=c2_at1noparentisAbstract=No}onesigc2_at1extendsc2_at1_typeonesigc2_at2extendsc2_at2_typeonesigc2_at3extendsc2_at3_typeonesigc2_at4extendsc2_at4_typeonesigclass3_nameextendsClassattrSet=c3_at1+c3_at2+c3_at3+c3_at4id=c3_at1noparentisAbstract=No}onesigc3_at1extendsc3_at1_typeonesigc3_at2extendsc3_at2_typeonesigc3_at3extendsc3_at3_typeonesigc3_at4extendsc3_at4_typeonesigclass4_nameextendsClassattrSet=c4_at1id=c4_at1noparentisAbstract=No}onesigc4_at1extendsc4_at1_typeonesigclass5_nameextendsClassattrSet=c5_at1+c5_at2+c5_at3+c5_at4id=c5_at1noparentisAbstract=No}onesigc5_at1extendsc5_at1_typeonesigc5_at2extendsc5_at2_typeonesigc5_at3extendsc5_at3_typeonesigc5_at4extendsc5_at4_typeonesigclass6_nameextendsClassattrSet=c6_at1+c6_at2+c6_at3+c6_at4id=c6_at1noparentisAbstract=No}onesigc6_at1extendsc6_at1_typeonesigc6_at2extendsc6_at2_typeonesigc6_at3extendsc6_at3_typeonesigc6_at4extendsc6_at4_typeonesigassoc1extendsAssociationsrc=class1_namedst=class5_namesrc_multiplicity=src_mlpcdst_multiplicity=dst_mlpc}onesigassoc2extendsAssociationsrc=class1_namedst=class5_namesrc_multiplicity=src_mlpcdst_multiplicity=dst_mlpc}onesigassoc3extendsAssociationsrc=class4_namedst=class5_namesrc_multiplicity=src_mlpcdst_multiplicity=dst_mlpc}onesigassoc4extendsAssociationsrc=class1_namedst=class6_namesrc_multiplicity=src_mlpcdst_multiplicity=dst_mlpc2}onesigassoc5extendsAssociationsrc=class1_namedst=class3_namesrc_multiplicity=src_mlpcdst_multiplicity=dst_mlpc2}predshowrunshowfor38,â€‹OM_name_Solution:0Table:class1_nameAttributec1_at1:c1_at1_typePrimaryKeyTable:class1_nameAttributec1_at2:c1_at2_typeTable:class2_nameAttributec2_at1:c1_at1_typePrimaryKeyTable:class2_nameAttributec2_at2:c2_at2_typeTable:class2_nameAttributec2_at3:c2_at3_typeTable:class3_nameAttributec3_at1:c3_at1_typePrimaryKeyTable:class3_nameAttributec3_at4:c3_at4_typeTable:class3_nameAttributec3_at2:c3_at2_typeTable:class4_nameAttributec4_at1:c4_at1_typePrimaryKeyTable:class5_nameAttributec5_at3:c5_at3_typeTable:class5_nameAttributec5_at4:c5_at4_typeTable:class6_nameAttributec6_at1:c6_at1_typePrimaryKeyTable:class6_nameAttributec6_at2:c6_at2_typeTable:class6_nameAttributec6_at3:c6_at3_typeTable:class6_nameAttributec6_at4:c6_at4_typeTable:class1_nameAttributec1_at1:c1_at1_typePrimaryKeyTable:class2_nameAttributec2_at1:c1_at1_typePrimaryKeyTable:class3_nameAttributec3_at1:c3_at1_typePrimaryKeyTable:class4_nameAttributec4_at1:c4_at1_typePrimaryKeyTable:class6_nameAttributec6_at1:c6_at1_typePrimaryKeyTableName:class1_nameTableName:class2_nameTableName:class3_nameTableName:class4_nameTableName:class5_nameTableName:class6_nameMappingStrategyofTableclass1_name:map_str2MappingStrategyofTableclass2_name:map_str2MappingStrategyofTableclass3_name:map_str2MappingStrategyofTableclass4_name:map_str2MappingStrategyofTableclass6_name:map_str2AssociationStrategyforassoc1:assoc_type1AssociationStrategyforassoc2:assoc_type1AssociationStrategyforassoc3:assoc_type2AssociationStrategyforassoc4:assoc_type2AssociationStrategyforassoc5:assoc_type2,USEOM_name0----CREATETABLE`assoc1`(`c5_at1`c5_at1_type`c1_at1`c1_at1_typeKEY`FK_assoc1_c5_at1_idx`(`c5_at1`)KEY`FK_assoc1_c1_at1_idx`(`c1_at1`)PRIMARYKEY(`c5_at1``c1_at1`));----CREATETABLE`assoc3``c5_at1`c5_at1_type`c4_at1`c4_at1_typeKEY`FK_assoc3_c5_at1_idx`(`c5_at1`)KEY`FK_assoc3_c4_at1_idx`(`c4_at1`)PRIMARYKEY(`c5_at1``c4_at1`));----CREATETABLE`class5_name`(`c5_at4`c5_at4_type(64)`c5_at3`c5_at3_type(64)`c1_at1`c1_at1_type`c5_at1`c5_at1_typePRIMARYKEY(`c5_at1`));----CREATETABLE`class3_name`(`c3_at3`c3_at3_type(64)`c3_at2`c3_at2_type(64)`c3_at4`c3_at4_type`c3_at1`c3_at1_typeNOTNULLPRIMARYKEY(`c3_at1`));----CREATETABLE`class2_name`(`c2_at3`c2_at3_type(64)`c2_at2`c2_at2_type(64)`c2_at4`c2_at4_type`c2_at1`c2_at1_typePRIMARYKEY(`c2_at1`));----CREATETABLE`class4_name`(`c4_at1`c4_at1_typePRIMARYKEY(`c4_at1`));----CREATETABLE`class1_name`(`c1_at2`c1_at2_type(64)`c1_at1`c1_at1_typePRIMARYKEY(`c1_at1`));----CREATETABLE`class6_name`(`c6_at4`c6_at4_type`c6_at3`c6_at3_type`c6_at2`c6_at2_type`c6_at1`c6_at1_typeNOTNULLPRIMARYKEY(`c6_at1`));----CREATETABLE`assoc2`(`c5_at1`c5_at1_type`c2_at1`c2_at1_typeKEY`FK_assoc2_c5_at1_idx`(`c5_at1`)KEY`FK_assoc2_c2_at1_idx`(`c2_at1`)PRIMARYKEY(`c5_at1``c2_at1`));----CREATETABLE`assoc5`(`c3_at1`c3_at1_typeNOTNULL`c2_at1`c2_at1_typeKEY`FK_assoc5_c3_at1_idx`(`c3_at1`)KEY`FK_assoc5_c2_at1_idx`(`c2_at1`)PRIMARYKEY(`c3_at1``c2_at1`));----CREATETABLE`assoc4`(`c6_at1`c6_at1_typeNOTNULL`c2_at1`c2_at1_typeKEY`FK_assoc4_c6_at1_idx`(`c6_at1`)KEY`FK_assoc4_c2_at1_idx`(`c2_at1`)PRIMARYKEY(`c6_at1``c2_at1`));ALTERTABLE`assoc1`ADDCONSTRAINT`FK_assoc1_c5_at1`FOREIGNKEY(`c5_at1`)REFERENCES`class5_name`(`c5_at1`)ONDELETECASCADEONUPDATECASCADEADDCONSTRAINT`FK_assoc1_c1_at1`FOREIGNKEY(`c1_at1`)REFERENCES`class1_name`(`c1_at1`)ONDELETECASCADEONUPDATECASCADE;ALTERTABLE`assoc3`ADDCONSTRAINT`FK_assoc3_c5_at1`FOREIGNKEY(`c5_at1`)REFERENCES`class5_name`(`c5_at1`)ONDELETECASCADEONUPDATECASCADEADDCONSTRAINT`FK_assoc3_c4_at1`FOREIGNKEY(`c4_at1`)REFERENCES`class4_name`(`c4_at1`)ONDELETECASCADEONUPDATECASCADE;ALTERTABLE`assoc2`ADDCONSTRAINT`FK_assoc2_c5_at1`FOREIGNKEY(`c5_at1`)REFERENCES`class5_name`(`c5_at1`)ONDELETECASCADEONUPDATECASCADEADDCONSTRAINT`FK_assoc2_c2_at1`FOREIGNKEY(`c2_at1`)REFERENCES`class2_name`(`c2_at1`)ONDELETECASCADEONUPDATECASCADEALTERTABLE`assoc5`ADDCONSTRAINT`FK_assoc5_c3_at1`FOREIGNKEY(`c3_at1`)REFERENCES`class3_name`(`c3_at1`)ONDELETECASCADEONUPDATECASCADEADDCONSTRAINT`FK_assoc5_c2_at1`FOREIGNKEY(`c2_at1`)REFERENCES`class2_name`(`c2_at1`)ONDELETECASCADEONUPDATECASCADE;ALTERTABLE`assoc4`ADDCONSTRAINT`FK_assoc4_c6_at1`FOREIGNKEY(`c6_at1`)REFERENCES`class6_name`(`c6_at1`)ONDELETECASCADEONUPDATECASCADEADDCONSTRAINT`FK_assoc4_c2_at1`FOREIGNKEY(`c2_at1`)REFERENCES`class2_name`(`c2_at1`)ONDELETECASCADEONUPDATECASCADE')\n",
        "\n",
        "#example_text = tf.constant('class1,table2,obj1,atr1')\n",
        "#print(example_text.numpy())\n",
        "#print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "chTF5N885F0P"
      },
      "outputs": [],
      "source": [
        "#import re\n",
        "\n",
        "#def tf_lower_and_split_punct(text):\n",
        "\n",
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '', r'')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UREvDg3sEKYa"
      },
      "outputs": [],
      "source": [
        "#print(example_text.numpy().decode())\n",
        "#print(tf_lower_and_split_punct(example_text).numpy().decode())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q-sKsSI7xRZ"
      },
      "source": [
        "#### Text Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aKn8qd37abi"
      },
      "source": [
        "This standardization function will be wrapped up in a `tf.keras.layers.TextVectorization` layer which will handle the vocabulary extraction and conversion of input text to sequences of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eAY9k49G3jE_"
      },
      "outputs": [],
      "source": [
        "max_vocab_size = 5000\n",
        "\n",
        "context_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size,\n",
        "    ragged=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kbC6ODP8IK_"
      },
      "source": [
        "The `TextVectorization` layer and many other [Keras preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) have an `adapt` method. This method reads one epoch of the training data, and works a lot like `Model.fit`. This `adapt` method initializes the layer based on the data. Here it determines the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bmsI1Yql8FYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a815ef5-ba6d-42ac-c87e-2b68da71b9ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "context_text_processor.adapt(train_raw.map(lambda context, target: context))\n",
        "\n",
        "# Here are the first 10 words from the vocabulary:\n",
        "#context_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kGjIFjX8_Wp"
      },
      "source": [
        "That's the context data  `TextVectorization` layer, now build and `.adapt()` for the Target Data one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jlC4xuZnKLBS"
      },
      "outputs": [],
      "source": [
        "target_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size,\n",
        "    ragged=True)\n",
        "\n",
        "target_text_processor.adapt(train_raw.map(lambda context, target: target))\n",
        "#target_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWQqlP_s9eIv"
      },
      "source": [
        "Now these layers can convert a batch of strings into a batch of token IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KZxj8IrNZ9S",
        "outputId": "fbc5b2c2-ab42-4e85-b1c9-3d78a4d5726c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[2, 121, 3]]>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "example_tokens = context_text_processor(example_context_strings)\n",
        "example_tokens[:3, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA9rUn9G9n78"
      },
      "source": [
        "The `get_vocabulary` method can be used to convert token IDs back to text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "98g9rcxGQY0I"
      },
      "outputs": [],
      "source": [
        "# context_vocab = np.array(context_text_processor.get_vocabulary())\n",
        "# tokens = context_vocab[example_tokens[0].numpy()]\n",
        "# ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot0aCL9t-Ghi"
      },
      "source": [
        "The returned token IDs are zero-padded. This can easily be turned into a mask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "_jx4Or_eFRSz",
        "outputId": "ffb74abc-1e4b-41bc-bf64-69e7cfc84f89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mask')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(example_tokens.to_tensor())\n",
        "plt.title('Token IDs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens.to_tensor() != 0)\n",
        "plt.title('Mask')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O0B4XdFlRgc"
      },
      "source": [
        "### Process the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVCuyuSp_whd"
      },
      "source": [
        "The `process_text` function below converts the `Datasets` of strings, into  0-padded tensors of token IDs. It also converts from a `(context, target)` pair to an `((context, target_in), target_out)` pair for training with `keras.Model.fit`. Keras expects `(inputs, labels)` pairs, the inputs are the `(context, target_in)` and the labels are `target_out`. The difference between `target_in` and `target_out` is that they are shifted by one step relative to eachother, so that at each location the label is the next token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wk5tbZWQl5u1"
      },
      "outputs": [],
      "source": [
        "def process_text(context, target):\n",
        "  context = context_text_processor(context).to_tensor()\n",
        "  target = target_text_processor(target)\n",
        "  targ_in = target[:,:-1].to_tensor()\n",
        "  targ_out = target[:,1:].to_tensor()\n",
        "  return (context, targ_in), targ_out\n",
        "\n",
        "\n",
        "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iGi7X2m_tbM"
      },
      "source": [
        "Here is the first sequence of each, from the first batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woQBWAjLsJkr",
        "outputId": "21a33f24-2848-4244-e390-bce4b9fbe538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  2 121   3]\n",
            "\n",
            "[  2 122]\n",
            "[122   3]\n"
          ]
        }
      ],
      "source": [
        "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
        "  print(ex_context_tok[0, :10].numpy()) \n",
        "  print()\n",
        "  print(ex_tar_in[0, :10].numpy()) \n",
        "  print(ex_tar_out[0, :10].numpy()) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNfHIF71ulLu"
      },
      "source": [
        "## The encoder/decoder\n",
        "\n",
        "  <th colspan=1>This tutorial's model</th>\n",
        "<tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzQWx2saImMV"
      },
      "source": [
        "Before getting into it define constants for the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_a9uNz3-IrF-"
      },
      "outputs": [],
      "source": [
        "UNITS = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blNgVbLSzpsr"
      },
      "source": [
        "### The encoder\n",
        "\n",
        "\n",
        "The encoder:\n",
        "\n",
        "1. Takes a list of token IDs (from `context_text_processor`).\n",
        "3. Looks up an embedding vector for each token (Using a `layers.Embedding`).\n",
        "4. Processes the embeddings into a new sequence (Using a bidirectional `layers.GRU`).\n",
        "5. Returns the processed sequence. This will be passed to the attention head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.units = units\n",
        "    \n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,\n",
        "                                               mask_zero=True)\n",
        "\n",
        "    # The RNN layer processes those vectors sequentially.\n",
        "    self.rnn = tf.keras.layers.Bidirectional(\n",
        "        merge_mode='sum',\n",
        "        layer=tf.keras.layers.GRU(units,\n",
        "                            # Return the sequence and state\n",
        "                            return_sequences=True,\n",
        "                            recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "  def call(self, x):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(x, 'batch s')\n",
        "\n",
        "    # 2. The embedding layer looks up the embedding vector for each token.\n",
        "    x = self.embedding(x)\n",
        "    shape_checker(x, 'batch s units')\n",
        "\n",
        "    # 3. The GRU processes the sequence of embeddings.\n",
        "    x = self.rnn(x)\n",
        "    shape_checker(x, 'batch s units')\n",
        "\n",
        "    # 4. Returns the new sequence of embeddings.\n",
        "    return x\n",
        "\n",
        "  def convert_input(self, texts):\n",
        "    texts = tf.convert_to_tensor(texts)\n",
        "    if len(texts.shape) == 0:\n",
        "      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "    context = self.text_processor(texts).to_tensor()\n",
        "    context = self(context)\n",
        "    return context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60gSVh05Jl6l",
        "outputId": "744bbbb7-b69b-4033-b3e3-e7e20ce4b0e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context tokens, shape (batch, s): (1, 3)\n",
            "Encoder output, shape (batch, s, units): (1, 3, 256)\n"
          ]
        }
      ],
      "source": [
        "# Encode the input sequence.\n",
        "encoder = Encoder(context_text_processor, UNITS)\n",
        "ex_context = encoder(ex_context_tok)\n",
        "\n",
        "print(f'Context tokens, shape (batch, s): {ex_context_tok.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45xM_Gl1MgXY"
      },
      "source": [
        "### The attention layer\n",
        "\n",
        "The attention layer lets the decoder access the information extracted by the encoder. It computes a vector from the entire context sequence, and adds that to the decoder's output. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-Ql3ymqwD8LS"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, x, context):\n",
        "    shape_checker = ShapeChecker()\n",
        " \n",
        "    shape_checker(x, 'batch t units')\n",
        "    shape_checker(context, 'batch s units')\n",
        "\n",
        "    attn_output, attn_scores = self.mha(\n",
        "       query=x,\n",
        "       value=context,\n",
        "      return_attention_scores=True)\n",
        "    \n",
        "    shape_checker(x, 'batch t units')\n",
        "    shape_checker(attn_scores, 'batch heads t s')\n",
        "    \n",
        "  #Cache the attention scores for plotting later.\n",
        "    attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
        "    shape_checker(attn_scores, 'batch t s')\n",
        "    self.last_attention_weights = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bRzduCU4tGN6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "7y7hjPkNMmHh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = CrossAttention(UNITS)\n",
        "\n",
        "# Attend to the encoded tokens\n",
        "embed = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(),\n",
        "                                 output_dim=UNITS, mask_zero=True)\n",
        "ex_tar_embed = embed(ex_tar_in)\n",
        "\n",
        "result = attention_layer(ex_tar_embed, ex_context)\n",
        "\n",
        "print(f'Context sequence, shape (batch, s, units): {ex_context.shape}')\n",
        "print(f'Target sequence, shape (batch, t, units): {ex_tar_embed.shape}')\n",
        "print(f'Attention result, shape (batch, t, units): {result.shape}')\n",
        "print(f'Attention weights, shape (batch, t, s):    {attention_layer.last_attention_weights.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVLdvss3zN4v",
        "outputId": "2f5089dd-8522-4c7e-a7a7-5047533c47c2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context sequence, shape (batch, s, units): (1, 3, 256)\n",
            "Target sequence, shape (batch, t, units): (1, 2, 256)\n",
            "Attention result, shape (batch, t, units): (1, 2, 256)\n",
            "Attention weights, shape (batch, t, s):    (1, 2, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d14A2DcPtQhS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx9fUhi3Pmwp"
      },
      "source": [
        "The attention weights will sum to `1` over the context sequence, at each location in the target sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxyR7cmQPn9P",
        "outputId": "97ca924c-26c0-4344-cf97-d27dffc9657a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "attention_layer.last_attention_weights[0].numpy().sum(axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AagyXMH-Jhqt"
      },
      "source": [
        "\n",
        "\n",
        "Here are the attention weights across the context sequences at `t=0`:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_weights = attention_layer.last_attention_weights\n",
        "mask=(ex_context_tok != 0).numpy()\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(mask*attention_weights[:, 0, :])\n",
        "plt.title('Attention weights')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(mask)\n",
        "plt.title('Mask');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "LDc9M_CUtYWD",
        "outputId": "04984d7c-8b4e-428e-e65b-529c8af46f03"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Cpq_sCKHtZzS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Eil-C_NN1rp"
      },
      "source": [
        "Because of the small-random initialization the attention weights are initially all close to `1/(sequence_length)`. The model will learn to make these less uniform as training progresses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ638eHN4iCK"
      },
      "source": [
        "### The decoder\n",
        "\n",
        "The decoder's job is to generate predictions for the next token at each location in the target sequence.\n",
        "\n",
        "1. It looks up embeddings for each token in the target sequence.\n",
        "2. It uses an RNN to process the target sequence, and keep track of what it has generated so far.\n",
        "3. It uses RNN output as the \"query\" to the attention layer, when attending to the encoder's output.\n",
        "4. At each location in the output it predicts the next token.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZsQJMqNmg_L"
      },
      "source": [
        "Here is the `Decoder` class' initializer. The initializer creates all the necessary layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "erYvHIgAl8kh"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.word_to_id = tf.keras.layers.StringLookup(\n",
        "        vocabulary=text_processor.get_vocabulary(),\n",
        "        mask_token='', oov_token='[UNK]')\n",
        "    self.id_to_word = tf.keras.layers.StringLookup(\n",
        "        vocabulary=text_processor.get_vocabulary(),\n",
        "        mask_token='', oov_token='[UNK]',\n",
        "        invert=True)\n",
        "    self.start_token = self.word_to_id('[START]')\n",
        "    self.end_token = self.word_to_id('[END]')\n",
        "\n",
        "    self.units = units\n",
        "\n",
        "\n",
        "    # 1. The embedding layer converts token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
        "                                               units, mask_zero=True)\n",
        "\n",
        "    # 2. The RNN keeps track of what's been generated so far.\n",
        "    self.rnn = tf.keras.layers.GRU(units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # 3. The RNN output will be the query for the attention layer.\n",
        "    self.attention = CrossAttention(units)\n",
        "\n",
        "    # 4. This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.output_layer = tf.keras.layers.Dense(self.vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd8-nRNzFR8x"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPnaw583CpnY"
      },
      "source": [
        "Next, the `call` method, takes 3 arguments:\n",
        "\n",
        "* `inputs` -  a `context, x` pair where:\n",
        "  * `context` - is the context from the encoder's output.\n",
        "  * `x` - is the target sequence input.\n",
        "* `state` - Optional, the previous `state` output from the decoder (the internal state of the decoder's RNN). Pass the state from a previous run to continue generating text where you left off.\n",
        "* `return_state` - [Default: False] - Set this to `True` to return the RNN state. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "PJOi5btHAPNK"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def call(self,\n",
        "         context, x,\n",
        "         state=None,\n",
        "         return_state=False):  \n",
        "  shape_checker = ShapeChecker()\n",
        "  shape_checker(x, 'batch t')\n",
        "  shape_checker(context, 'batch s units')\n",
        "\n",
        "  # 1. Lookup the embeddings\n",
        "  x = self.embedding(x)\n",
        "  shape_checker(x, 'batch t units')\n",
        "\n",
        "  # 2. Process the target sequence.\n",
        "  x, state = self.rnn(x, initial_state=state)\n",
        "  shape_checker(x, 'batch t units')\n",
        "\n",
        "  # 3. Use the RNN output as the query for the attention over the context.\n",
        "  x = self.attention(x, context)\n",
        "  self.last_attention_weights = self.attention.last_attention_weights\n",
        "  shape_checker(x, 'batch t units')\n",
        "  shape_checker(self.last_attention_weights, 'batch t s')\n",
        "\n",
        "  # Step 4. Generate logit predictions for the next token.\n",
        "  logits = self.output_layer(x)\n",
        "  shape_checker(logits, 'batch t target_vocab_size')\n",
        "\n",
        "  if return_state:\n",
        "    return logits, state\n",
        "  else:\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1-mLAcUEXpK"
      },
      "source": [
        "That will be sufficient for training. Create an instance of the decoder to test out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4ZUMbYXIEVeA"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(target_text_processor, UNITS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFWaI4wqzt4t"
      },
      "source": [
        "Decoder usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YM-lD7bzx18",
        "outputId": "1dc878b0-2506-430a-dbd3-9bfcc65a639b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder output shape: (batch, s, units) (1, 3, 256)\n",
            "input target tokens shape: (batch, t) (1, 2)\n",
            "logits shape shape: (batch, target_vocabulary_size) (1, 2, 194)\n"
          ]
        }
      ],
      "source": [
        "logits = decoder(ex_context, ex_tar_in)\n",
        "\n",
        "print(f'encoder output shape: (batch, s, units) {ex_context.shape}')\n",
        "print(f'input target tokens shape: (batch, t) {ex_tar_in.shape}')\n",
        "print(f'logits shape shape: (batch, target_vocabulary_size) {logits.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhS_tbk7VQkX"
      },
      "source": [
        "#### Inference\n",
        "\n",
        "For inference usage couple more methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "SPm12cnIVRQr"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def get_initial_state(self, context):\n",
        "  batch_size = tf.shape(context)[0]\n",
        "  start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "  embedded = self.embedding(start_tokens)\n",
        "  return start_tokens, done, self.rnn.get_initial_state(embedded)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "TzeOhpBvVS5L"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def tokens_to_text(self, tokens):\n",
        "  words = self.id_to_word(tokens)\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
        "  result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "v6ildnz_V1MA"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
        "  logits, state = self(\n",
        "    context, next_token,\n",
        "    state = state,\n",
        "    return_state=True) \n",
        "  \n",
        "  if temperature == 0.0:\n",
        "    next_token = tf.argmax(logits, axis=-1)\n",
        "  else:\n",
        "    logits = logits[:, -1, :]/temperature\n",
        "    next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "  # If a sequence produces an `end_token`, set it `done`\n",
        "  done = done | (next_token == self.end_token)\n",
        "  # Once a sequence is done it only produces 0-padding.\n",
        "  next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
        "  \n",
        "  return next_token, done, state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WiXLrVs-FTE"
      },
      "source": [
        "With those extra functions, you can write a generation loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "SuehagxL-JBZ"
      },
      "outputs": [],
      "source": [
        "# Setup the loop variables.\n",
        "next_token, done, state = decoder.get_initial_state(ex_context)\n",
        "tokens = []\n",
        "\n",
        "for n in range(10):\n",
        "  # Run one step.\n",
        "  next_token, done, state = decoder.get_next_token(\n",
        "      ex_context, next_token, done, state, temperature=1.0)\n",
        "  # Add the token to the output.\n",
        "  tokens.append(next_token)\n",
        "\n",
        "# Stack all the tokens together.\n",
        "tokens = tf.concat(tokens, axis=-1) # (batch, t)\n",
        "\n",
        "# Convert the tokens back to a a string\n",
        "result = decoder.tokens_to_text(tokens)\n",
        "#result[:3].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6xyru86m914"
      },
      "source": [
        "## The model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "WWIyuy71TkJT"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, units,\n",
        "               context_text_processor,\n",
        "               target_text_processor):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(context_text_processor, units)\n",
        "    decoder = Decoder(target_text_processor, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def call(self, inputs):\n",
        "    context, x = inputs\n",
        "    context = self.encoder(context)\n",
        "    logits = self.decoder(context, x)\n",
        "\n",
        "    #TODO(b/250038731): remove this\n",
        "    try:\n",
        "      # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rPi0FkS2iA5"
      },
      "source": [
        "During training the model will be used like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vhjTh84K6Mg",
        "outputId": "2517539e-c873-4344-bab2-dc3d36413415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context tokens, shape: (batch, s, units) (1, 3)\n",
            "Target tokens, shape: (batch, t) (1, 2)\n",
            "logits, shape: (batch, t, target_vocabulary_size) (1, 2, 194)\n"
          ]
        }
      ],
      "source": [
        "model = Translator(UNITS, context_text_processor, target_text_processor)\n",
        "\n",
        "logits = model((ex_context_tok, ex_tar_in))\n",
        "\n",
        "print(f'Context tokens, shape: (batch, s, units) {ex_context_tok.shape}')\n",
        "print(f'Target tokens, shape: (batch, t) {ex_tar_in.shape}')\n",
        "print(f'logits, shape: (batch, t, target_vocabulary_size) {logits.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "outputs": [],
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "nRB1CTmQWOIL"
      },
      "outputs": [],
      "source": [
        "def masked_acc(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "    \n",
        "    match = tf.cast(y_true == y_pred, tf.float32)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    \n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f32GuAhw2nXm"
      },
      "source": [
        "Configure the model for training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "9g0DRRvm3l9X"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='Adam',\n",
        "              loss=masked_loss, \n",
        "              metrics=[masked_acc, masked_loss])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DWLI3pssjnx"
      },
      "source": [
        "The model is randomly initialized, and should give roughly uniform output probabilities. So it's easy to predict what the initial values of the metrics should be:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuP3_LFENMJG",
        "outputId": "aae885de-cec9-4fb8-f487-81a6da8a468d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'expected_loss': 5.267858, 'expected_acc': 0.005154639175257732}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "vocab_size = 1.0 * target_text_processor.vocabulary_size()\n",
        "\n",
        "{\"expected_loss\": tf.math.log(vocab_size).numpy(),\n",
        " \"expected_acc\": 1/vocab_size}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frVba49Usd0Z"
      },
      "source": [
        "That should roughly match the values returned by running a few steps of evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rJITfxEsHKR",
        "outputId": "3000245d-f533-494e-aac9-81923d07a2ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65/70 [==========================>...] - ETA: 0s - loss: 3.6522 - masked_acc: 0.5000 - masked_loss: 3.6522"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 70 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r70/70 [==============================] - 3s 41ms/step - loss: 3.6257 - masked_acc: 0.5000 - masked_loss: 3.6257\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': 3.625699043273926,\n",
              " 'masked_acc': 0.5,\n",
              " 'masked_loss': 3.625699043273926}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "model.evaluate(val_ds, steps=70, return_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQd_esVVoSf3",
        "outputId": "4a29629e-42e2-4843-d47d-916009668f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.2507 - masked_acc: 0.5101 - masked_loss: 2.2507"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 70 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 9s 85ms/step - loss: 2.2769 - masked_acc: 0.5100 - masked_loss: 2.2769 - val_loss: 3.6218 - val_masked_acc: 0.5000 - val_masked_loss: 3.6218\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.3028 - masked_acc: 0.5550 - masked_loss: 2.3028"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 36ms/step - loss: 2.3028 - masked_acc: 0.5550 - masked_loss: 2.3028\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.2986 - masked_acc: 0.5300 - masked_loss: 2.2986"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 28ms/step - loss: 2.2986 - masked_acc: 0.5300 - masked_loss: 2.2986\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.1400 - masked_acc: 0.5300 - masked_loss: 2.1400"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 28ms/step - loss: 2.1400 - masked_acc: 0.5300 - masked_loss: 2.1400\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.0663 - masked_acc: 0.5500 - masked_loss: 2.0663"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 28ms/step - loss: 2.0663 - masked_acc: 0.5500 - masked_loss: 2.0663\n",
            "Epoch 6/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.8086 - masked_acc: 0.5808 - masked_loss: 1.8086"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 40ms/step - loss: 1.7918 - masked_acc: 0.5850 - masked_loss: 1.7918\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.8094 - masked_acc: 0.5850 - masked_loss: 1.8094"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 27ms/step - loss: 1.8094 - masked_acc: 0.5850 - masked_loss: 1.8094\n",
            "Epoch 8/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.7058 - masked_acc: 0.6111 - masked_loss: 1.7058"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 28ms/step - loss: 1.6926 - masked_acc: 0.6150 - masked_loss: 1.6926\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.2989 - masked_acc: 0.6200 - masked_loss: 1.2989"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 1.2989 - masked_acc: 0.6200 - masked_loss: 1.2989\n",
            "Epoch 10/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.3285 - masked_acc: 0.6818 - masked_loss: 1.3285"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 40ms/step - loss: 1.3261 - masked_acc: 0.6850 - masked_loss: 1.3261\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.2076 - masked_acc: 0.6800 - masked_loss: 1.2076"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 1.2076 - masked_acc: 0.6800 - masked_loss: 1.2076\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.9493 - masked_acc: 0.7600 - masked_loss: 0.9493"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.9493 - masked_acc: 0.7600 - masked_loss: 0.9493\n",
            "Epoch 13/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.1654 - masked_acc: 0.7020 - masked_loss: 1.1654"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 1.1727 - masked_acc: 0.7000 - masked_loss: 1.1727\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.5934 - masked_acc: 0.8600 - masked_loss: 0.5934"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 41ms/step - loss: 0.5934 - masked_acc: 0.8600 - masked_loss: 0.5934\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.7252 - masked_acc: 0.7800 - masked_loss: 0.7252"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.7252 - masked_acc: 0.7800 - masked_loss: 0.7252\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.7151 - masked_acc: 0.7850 - masked_loss: 0.7151"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.7151 - masked_acc: 0.7850 - masked_loss: 0.7151\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.3677 - masked_acc: 0.9200 - masked_loss: 0.3677"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 44ms/step - loss: 0.3677 - masked_acc: 0.9200 - masked_loss: 0.3677\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.4710 - masked_acc: 0.8900 - masked_loss: 0.4710"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 38ms/step - loss: 0.4710 - masked_acc: 0.8900 - masked_loss: 0.4710\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.4313 - masked_acc: 0.9000 - masked_loss: 0.4313"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.4313 - masked_acc: 0.9000 - masked_loss: 0.4313\n",
            "Epoch 20/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1886 - masked_acc: 0.9798 - masked_loss: 0.1886"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.1872 - masked_acc: 0.9800 - masked_loss: 0.1872\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.2500 - masked_acc: 0.9450 - masked_loss: 0.2500"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 31ms/step - loss: 0.2500 - masked_acc: 0.9450 - masked_loss: 0.2500\n",
            "Epoch 22/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0757 - masked_acc: 0.9949 - masked_loss: 0.0757"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 39ms/step - loss: 0.0750 - masked_acc: 0.9950 - masked_loss: 0.0750\n",
            "Epoch 23/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0593 - masked_acc: 0.9899 - masked_loss: 0.0593"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0806 - masked_acc: 0.9850 - masked_loss: 0.0806\n",
            "Epoch 24/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.1005 - masked_acc: 0.9796 - masked_loss: 0.1005"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0992 - masked_acc: 0.9800 - masked_loss: 0.0992\n",
            "Epoch 25/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0706 - masked_acc: 0.9848 - masked_loss: 0.0706"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 31ms/step - loss: 0.0701 - masked_acc: 0.9850 - masked_loss: 0.0701\n",
            "Epoch 26/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0262 - masked_acc: 1.0000 - masked_loss: 0.0262"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 38ms/step - loss: 0.0260 - masked_acc: 1.0000 - masked_loss: 0.0260\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0494 - masked_acc: 0.9900 - masked_loss: 0.0494"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0494 - masked_acc: 0.9900 - masked_loss: 0.0494\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0731 - masked_acc: 0.9900 - masked_loss: 0.0731"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 30ms/step - loss: 0.0731 - masked_acc: 0.9900 - masked_loss: 0.0731\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0684 - masked_acc: 0.9850 - masked_loss: 0.0684"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 32ms/step - loss: 0.0684 - masked_acc: 0.9850 - masked_loss: 0.0684\n",
            "Epoch 30/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0325 - masked_acc: 0.9949 - masked_loss: 0.0325"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 37ms/step - loss: 0.0323 - masked_acc: 0.9950 - masked_loss: 0.0323\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0538 - masked_acc: 0.9850 - masked_loss: 0.0538"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0538 - masked_acc: 0.9850 - masked_loss: 0.0538\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0338 - masked_acc: 0.9950 - masked_loss: 0.0338"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0338 - masked_acc: 0.9950 - masked_loss: 0.0338\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0270 - masked_acc: 0.9950 - masked_loss: 0.0270"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 31ms/step - loss: 0.0270 - masked_acc: 0.9950 - masked_loss: 0.0270\n",
            "Epoch 34/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0086 - masked_acc: 1.0000 - masked_loss: 0.0086"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 37ms/step - loss: 0.0086 - masked_acc: 1.0000 - masked_loss: 0.0086\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0105 - masked_acc: 1.0000 - masked_loss: 0.0105"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 30ms/step - loss: 0.0105 - masked_acc: 1.0000 - masked_loss: 0.0105\n",
            "Epoch 36/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - masked_acc: 0.9899 - masked_loss: 0.0285"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 30ms/step - loss: 0.0283 - masked_acc: 0.9900 - masked_loss: 0.0283\n",
            "Epoch 37/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0341 - masked_acc: 0.9950 - masked_loss: 0.0341"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 32ms/step - loss: 0.0341 - masked_acc: 0.9950 - masked_loss: 0.0341\n",
            "Epoch 38/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0172 - masked_acc: 0.9899 - masked_loss: 0.0172"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 37ms/step - loss: 0.0172 - masked_acc: 0.9900 - masked_loss: 0.0172\n",
            "Epoch 39/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0061 - masked_acc: 1.0000 - masked_loss: 0.0061"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0061 - masked_acc: 1.0000 - masked_loss: 0.0061\n",
            "Epoch 40/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0704 - masked_acc: 0.9850 - masked_loss: 0.0704"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0704 - masked_acc: 0.9850 - masked_loss: 0.0704\n",
            "Epoch 41/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0669 - masked_acc: 0.9848 - masked_loss: 0.0669"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 32ms/step - loss: 0.0665 - masked_acc: 0.9850 - masked_loss: 0.0665\n",
            "Epoch 42/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0410 - masked_acc: 0.9949 - masked_loss: 0.0410"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 38ms/step - loss: 0.0408 - masked_acc: 0.9950 - masked_loss: 0.0408\n",
            "Epoch 43/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0561 - masked_acc: 0.9848 - masked_loss: 0.0561"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 30ms/step - loss: 0.0556 - masked_acc: 0.9850 - masked_loss: 0.0556\n",
            "Epoch 44/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0531 - masked_acc: 0.9900 - masked_loss: 0.0531"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0531 - masked_acc: 0.9900 - masked_loss: 0.0531\n",
            "Epoch 45/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0574 - masked_acc: 0.9850 - masked_loss: 0.0574"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 32ms/step - loss: 0.0574 - masked_acc: 0.9850 - masked_loss: 0.0574\n",
            "Epoch 46/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.2342 - masked_acc: 0.9495 - masked_loss: 0.2342"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 38ms/step - loss: 0.2366 - masked_acc: 0.9500 - masked_loss: 0.2366\n",
            "Epoch 47/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.3031 - masked_acc: 0.9082 - masked_loss: 0.3031"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 30ms/step - loss: 0.3041 - masked_acc: 0.9050 - masked_loss: 0.3041\n",
            "Epoch 48/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.2929 - masked_acc: 0.9091 - masked_loss: 0.2929"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 30ms/step - loss: 0.2901 - masked_acc: 0.9100 - masked_loss: 0.2901\n",
            "Epoch 49/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1804 - masked_acc: 0.9550 - masked_loss: 0.1804"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 37ms/step - loss: 0.1804 - masked_acc: 0.9550 - masked_loss: 0.1804\n",
            "Epoch 50/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1544 - masked_acc: 0.9495 - masked_loss: 0.1544"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 35ms/step - loss: 0.1530 - masked_acc: 0.9500 - masked_loss: 0.1530\n",
            "Epoch 51/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0901 - masked_acc: 0.9697 - masked_loss: 0.0901"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 30ms/step - loss: 0.0893 - masked_acc: 0.9700 - masked_loss: 0.0893\n",
            "Epoch 52/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0634 - masked_acc: 0.9850 - masked_loss: 0.0634"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 30ms/step - loss: 0.0634 - masked_acc: 0.9850 - masked_loss: 0.0634\n",
            "Epoch 53/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0879 - masked_acc: 0.9800 - masked_loss: 0.0879"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 37ms/step - loss: 0.0879 - masked_acc: 0.9800 - masked_loss: 0.0879\n",
            "Epoch 54/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0694 - masked_acc: 0.9798 - masked_loss: 0.0694"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 35ms/step - loss: 0.0687 - masked_acc: 0.9800 - masked_loss: 0.0687\n",
            "Epoch 55/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0458 - masked_acc: 0.9899 - masked_loss: 0.0458"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 30ms/step - loss: 0.0453 - masked_acc: 0.9900 - masked_loss: 0.0453\n",
            "Epoch 56/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0188 - masked_acc: 0.9950 - masked_loss: 0.0188"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 30ms/step - loss: 0.0188 - masked_acc: 0.9950 - masked_loss: 0.0188\n",
            "Epoch 57/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0155 - masked_acc: 0.9949 - masked_loss: 0.0155"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 38ms/step - loss: 0.0154 - masked_acc: 0.9950 - masked_loss: 0.0154\n",
            "Epoch 58/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0365 - masked_acc: 0.9850 - masked_loss: 0.0365"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 32ms/step - loss: 0.0365 - masked_acc: 0.9850 - masked_loss: 0.0365\n",
            "Epoch 59/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0245 - masked_acc: 0.9949 - masked_loss: 0.0245"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0243 - masked_acc: 0.9950 - masked_loss: 0.0243\n",
            "Epoch 60/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0021 - masked_acc: 1.0000 - masked_loss: 0.0021"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 30ms/step - loss: 0.0021 - masked_acc: 1.0000 - masked_loss: 0.0021\n",
            "Epoch 61/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0561 - masked_acc: 0.9850 - masked_loss: 0.0561"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 39ms/step - loss: 0.0561 - masked_acc: 0.9850 - masked_loss: 0.0561\n",
            "Epoch 62/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0053 - masked_acc: 0.9950 - masked_loss: 0.0053"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 32ms/step - loss: 0.0053 - masked_acc: 0.9950 - masked_loss: 0.0053\n",
            "Epoch 63/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0167 - masked_acc: 0.9949 - masked_loss: 0.0167"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 30ms/step - loss: 0.0165 - masked_acc: 0.9950 - masked_loss: 0.0165\n",
            "Epoch 64/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0141 - masked_acc: 0.9950 - masked_loss: 0.0141"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 35ms/step - loss: 0.0141 - masked_acc: 0.9950 - masked_loss: 0.0141\n",
            "Epoch 65/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0348 - masked_acc: 0.9900 - masked_loss: 0.0348"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 5s 47ms/step - loss: 0.0348 - masked_acc: 0.9900 - masked_loss: 0.0348\n",
            "Epoch 66/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0124 - masked_acc: 0.9949 - masked_loss: 0.0124"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0123 - masked_acc: 0.9950 - masked_loss: 0.0123\n",
            "Epoch 67/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0216 - masked_acc: 0.9900 - masked_loss: 0.0216"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 28ms/step - loss: 0.0216 - masked_acc: 0.9900 - masked_loss: 0.0216\n",
            "Epoch 68/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0109 - masked_acc: 0.9950 - masked_loss: 0.0109"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 30ms/step - loss: 0.0109 - masked_acc: 0.9950 - masked_loss: 0.0109\n",
            "Epoch 69/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0272 - masked_acc: 0.9900 - masked_loss: 0.0272"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 42ms/step - loss: 0.0272 - masked_acc: 0.9900 - masked_loss: 0.0272\n",
            "Epoch 70/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0137 - masked_acc: 0.9949 - masked_loss: 0.0137"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0136 - masked_acc: 0.9950 - masked_loss: 0.0136\n",
            "Epoch 71/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0308 - masked_acc: 0.9900 - masked_loss: 0.0308"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0308 - masked_acc: 0.9900 - masked_loss: 0.0308\n",
            "Epoch 72/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0061 - masked_acc: 0.9949 - masked_loss: 0.0061"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 30ms/step - loss: 0.0060 - masked_acc: 0.9950 - masked_loss: 0.0060\n",
            "Epoch 73/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0145 - masked_acc: 0.9949 - masked_loss: 0.0145"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 41ms/step - loss: 0.0143 - masked_acc: 0.9950 - masked_loss: 0.0143\n",
            "Epoch 74/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - masked_acc: 0.9899 - masked_loss: 0.0303"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 30ms/step - loss: 0.0300 - masked_acc: 0.9900 - masked_loss: 0.0300\n",
            "Epoch 75/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0145 - masked_acc: 0.9899 - masked_loss: 0.0145"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0143 - masked_acc: 0.9900 - masked_loss: 0.0143\n",
            "Epoch 76/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0187 - masked_acc: 0.9950 - masked_loss: 0.0187"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 30ms/step - loss: 0.0187 - masked_acc: 0.9950 - masked_loss: 0.0187\n",
            "Epoch 77/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0066 - masked_acc: 0.9950 - masked_loss: 0.0066"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 40ms/step - loss: 0.0066 - masked_acc: 0.9950 - masked_loss: 0.0066\n",
            "Epoch 78/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0253 - masked_acc: 0.9900 - masked_loss: 0.0253"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 28ms/step - loss: 0.0253 - masked_acc: 0.9900 - masked_loss: 0.0253\n",
            "Epoch 79/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0166 - masked_acc: 0.9949 - masked_loss: 0.0166"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0165 - masked_acc: 0.9950 - masked_loss: 0.0165\n",
            "Epoch 80/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0236 - masked_acc: 0.9850 - masked_loss: 0.0236"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 5s 53ms/step - loss: 0.0236 - masked_acc: 0.9850 - masked_loss: 0.0236\n",
            "Epoch 81/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0102 - masked_acc: 0.9949 - masked_loss: 0.0102"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 34ms/step - loss: 0.0101 - masked_acc: 0.9950 - masked_loss: 0.0101\n",
            "Epoch 82/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0082 - masked_acc: 0.9949 - masked_loss: 0.0082"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0082 - masked_acc: 0.9950 - masked_loss: 0.0082\n",
            "Epoch 83/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0016 - masked_acc: 1.0000 - masked_loss: 0.0016"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 28ms/step - loss: 0.0016 - masked_acc: 1.0000 - masked_loss: 0.0016\n",
            "Epoch 84/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0489 - masked_acc: 0.9850 - masked_loss: 0.0489"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 36ms/step - loss: 0.0489 - masked_acc: 0.9850 - masked_loss: 0.0489\n",
            "Epoch 85/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0350 - masked_acc: 0.9899 - masked_loss: 0.0350"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 34ms/step - loss: 0.0346 - masked_acc: 0.9900 - masked_loss: 0.0346\n",
            "Epoch 86/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.3879 - masked_acc: 0.9200 - masked_loss: 0.3879"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 30ms/step - loss: 0.3879 - masked_acc: 0.9200 - masked_loss: 0.3879\n",
            "Epoch 87/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.3123 - masked_acc: 0.9100 - masked_loss: 0.3123"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 28ms/step - loss: 0.3123 - masked_acc: 0.9100 - masked_loss: 0.3123\n",
            "Epoch 88/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.2945 - masked_acc: 0.9050 - masked_loss: 0.2945"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 36ms/step - loss: 0.2945 - masked_acc: 0.9050 - masked_loss: 0.2945\n",
            "Epoch 89/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1941 - masked_acc: 0.9343 - masked_loss: 0.1941"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 35ms/step - loss: 0.1943 - masked_acc: 0.9350 - masked_loss: 0.1943\n",
            "Epoch 90/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0991 - masked_acc: 0.9750 - masked_loss: 0.0991"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 30ms/step - loss: 0.0991 - masked_acc: 0.9750 - masked_loss: 0.0991\n",
            "Epoch 91/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0275 - masked_acc: 1.0000 - masked_loss: 0.0275"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0275 - masked_acc: 1.0000 - masked_loss: 0.0275\n",
            "Epoch 92/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0556 - masked_acc: 0.9850 - masked_loss: 0.0556"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 36ms/step - loss: 0.0556 - masked_acc: 0.9850 - masked_loss: 0.0556\n",
            "Epoch 93/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0772 - masked_acc: 0.9900 - masked_loss: 0.0772"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 34ms/step - loss: 0.0772 - masked_acc: 0.9900 - masked_loss: 0.0772\n",
            "Epoch 94/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0413 - masked_acc: 0.9848 - masked_loss: 0.0413"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0409 - masked_acc: 0.9850 - masked_loss: 0.0409\n",
            "Epoch 95/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0248 - masked_acc: 0.9899 - masked_loss: 0.0248"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 28ms/step - loss: 0.0246 - masked_acc: 0.9900 - masked_loss: 0.0246\n",
            "Epoch 96/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0128 - masked_acc: 0.9950 - masked_loss: 0.0128"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 35ms/step - loss: 0.0128 - masked_acc: 0.9950 - masked_loss: 0.0128\n",
            "Epoch 97/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - masked_acc: 0.9899 - masked_loss: 0.0293"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 32ms/step - loss: 0.0290 - masked_acc: 0.9900 - masked_loss: 0.0290\n",
            "Epoch 98/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0028 - masked_acc: 1.0000 - masked_loss: 0.0028"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0028 - masked_acc: 1.0000 - masked_loss: 0.0028\n",
            "Epoch 99/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0025 - masked_acc: 1.0000 - masked_loss: 0.0025"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 28ms/step - loss: 0.0025 - masked_acc: 1.0000 - masked_loss: 0.0025\n",
            "Epoch 100/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0359 - masked_acc: 0.9848 - masked_loss: 0.0359"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 34ms/step - loss: 0.0355 - masked_acc: 0.9850 - masked_loss: 0.0355\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_ds.repeat(), \n",
        "    epochs=100,\n",
        "    steps_per_epoch = 100,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps = 70,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=8)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the Loss from Training "
      ],
      "metadata": {
        "id": "Uq9lHbPgenz9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "38rLdlmtQHCm",
        "outputId": "c07cda0a-6b77-4431-b03a-3cde96a53a51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f75b4625eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAA1MElEQVR4nO3deXhU9dn/8fc92UPIQhJCCAkJ+xYIGBarYqtVQW1tVcSlIrhQl1q3WrXautTHrk+tVZ9S3PHnAqJU3Kq2gIAgECCQQNghEAIkAbIBWef+/TFjGjFAgExOkrlf1zUXc5Y553NyQu452/crqooxxhj/5XI6gDHGGGdZITDGGD9nhcAYY/ycFQJjjPFzVgiMMcbPBTod4GTFxcVpamqq0zGMMaZdWblyZYmqxjc1rd0VgtTUVLKyspyOYYwx7YqI5B9rmp0aMsYYP2eFwBhj/JwVAmOM8XPt7hqBMaZl1NbWUlBQQFVVldNRTAsKDQ2lR48eBAUFNfszVgiM8VMFBQV07tyZ1NRURMTpOKYFqCr79++noKCAtLS0Zn/OTg0Z46eqqqqIjY21ItCBiAixsbEnfZRnhcAYP2ZFoOM5lX1qhcAYY/ycFQJjjGMiIiKcjmCwQmCMMX7PCoExxnGqyv3338+QIUNIT09n5syZAOzZs4exY8eSkZHBkCFDWLRoEfX19UyePLlh3qefftrh9O2f3T5qjOHxD9axvrC8RZc5qHskj/5gcLPmfe+998jOzmbNmjWUlJQwcuRIxo4dy5tvvslFF13Eww8/TH19PYcPHyY7O5vdu3eTm5sLQGlpaYvm9kd2RGCMcdzixYu55pprCAgIICEhgXPPPZcVK1YwcuRIXnnlFR577DFycnLo3LkzvXr1Ytu2bdx5553861//IjIy0un47Z7PjghEJBRYCIR41zNbVR89ap7JwJ+A3d5Rz6nqi77KZIxpWnO/ube2sWPHsnDhQj766CMmT57Mvffey6RJk1izZg2ffvop06ZNY9asWbz88stOR23XfHlEUA2cp6rDgAxgnIiMaWK+maqa4X1ZETDGD51zzjnMnDmT+vp6iouLWbhwIaNGjSI/P5+EhARuueUWbr75ZlatWkVJSQlut5srrriCJ598klWrVjkdv93z2RGBqipQ6R0M8r7UV+szxrRfP/7xj1m6dCnDhg1DRPjjH/9It27deO211/jTn/5EUFAQERERzJgxg927dzNlyhTcbjcAv/vd7xxO3/6J5++1jxYuEgCsBPoAz6vqA0dNnwz8DigGNgH3qOquJpYzFZgKkJKSckZ+/jH7VzDGNFNeXh4DBw50Oobxgab2rYisVNXMpub36cViVa1X1QygBzBKRIYcNcsHQKqqDgU+B147xnKmq2qmqmbGxzfZ05oxxphT1Cp3DalqKTAfGHfU+P2qWu0dfBE4ozXyGGOM+S+fFQIRiReRaO/7MOACYMNR8yQ2GvwhkOerPMYYY5rmywfKEoHXvNcJXMAsVf1QRJ4AslR1LvBzEfkhUAccACb7MI8xxpgm+PKuobXA8CbG/6bR+4eAh3yVwRhjzInZk8XGGOPnrBAYY4yfs0JgjOkwduzYwZAhR9+l3nzH6x/hdJfdllkhMMYYP2fNUBtj4JMHYW9Oyy6zWzqM//1xZ9mxYwfjxo1jzJgxLFmyhJEjRzJlyhQeffRRioqKeOONNwC46667qKqqIiwsjFdeeYX+/fuzbt06pkyZQk1NDW63m3fffZegoKCGZW/bto0rrriC6dOn06VLF+644w6Ki4sJDw/nhRdeYMCAAWzfvp1rr72WyspKLrvssmZvWlVVFbfddhtZWVkEBgbyl7/8he9973tNZurevTtXXXUVBQUF1NfX8+tf/5qJEyee2s/UR6wQGGMctWXLFt555x1efvllRo4cyZtvvsnixYuZO3cuTz31FDNmzGDRokUEBgby73//m1/96le8++67TJs2jbvuuovrrruOmpoa6uvr2bdvHwAbN27k6quv5tVXX2XYsGGcf/75TJs2jb59+7Js2TJuv/125s2bx1133cVtt93GpEmTeP7555ud+fnnn0dEyMnJYcOGDVx44YVs2rSpyUwff/wx3bt356OPPgKgrKzMJz/H02GFwBhzwm/uvpSWlkZ6ejoAgwcP5vzzz0dESE9PZ8eOHZSVlXHDDTewefNmRITa2loAzjzzTP7nf/6HgoICLr/8cvr27QtAcXExl112Ge+99x6DBg2isrKSJUuWMGHChIZ1Vld7GjT48ssveffddwG4/vrreeCBbzSHdkyLFy/mzjvvBGDAgAH07NmTTZs2NZkpPT2d++67jwceeIBLL72Uc845p2V+cC3IrhEYYxwVEhLS8N7lcjUMu1wu6urq+PWvf833vvc9cnNz+eCDD6iqqgLg2muvZe7cuYSFhXHxxRczb948AKKiokhJSWHx4sUAuN1uoqOjyc7Obnjl5f23EQMRabFtaSpTv379WLVqFenp6TzyyCM88cQTLba+lmKFwBjTppWVlZGUlATAq6++2jB+27Zt9OrVi5///OdcdtllrF27FoDg4GDmzJnDjBkzePPNN4mMjCQtLY133nkH8PSPvGbNGgDOOuss3n77bYCG6xHNcc455zTMv2nTJnbu3En//v2bzFRYWEh4eDg/+clPuP/++9tk/wlWCIwxbdovf/lLHnroIYYPH05dXV3D+FmzZjFkyBAyMjLIzc1l0qRJDdM6derEhx9+yNNPP83cuXN54403eOmllxg2bBiDBw/m/fffB+CZZ57h+eefJz09nd27d39r3cdy++2343a7SU9PZ+LEibz66quEhIQ0mSknJ4dRo0aRkZHB448/ziOPPNJyP5wW4tP+CHwhMzNTs7KynI5hTLtn/RF0XG2qPwJjjDFtn901ZIwxjeTk5HD99dd/Y1xISAjLli1zKJHvWSEwxphG0tPTyc7OdjpGq7JTQ8YY4+esEBhjjJ+zQmCMMX7OCoExxvg5KwTGmHbheH0F+MKCBQu49NJLT+mzJ+q74HSW7Qs+KwQiEioiy0VkjYisE5HHm5gnRERmisgWEVkmIqm+ymOMMaZpvrx9tBo4T1UrRSQIWCwin6jqV43muQk4qKp9RORq4A9A22qo2xg/8Iflf2DDgQ0tuswBXQbwwKhjt+b54IMPkpyczB133AHAY489RmBgIPPnz+fgwYPU1tby5JNPNqufgAULFvDoo48SHR1NTk4OV111Fenp6TzzzDMcOXKEf/7zn/Tu3ZsPPviAJ598kpqaGmJjY3njjTdISEjgiy++4K677gI8jdAtXLjwG8tfsWIFU6dOZfbs2ZSWlnLvvfdSWVlJXFwcr776KomJiaxcuZIbb7wRgAsvvLDZP6cDBw5w4403sm3bNsLDw5k+fTpDhw5tMlNlZSUTJ06kvLycuro6/v73v7dIa6Y+OyJQj0rvYJD3dXR7FpcBr3nfzwbOl5ZsCtAY02ZNnDiRWbNmNQzPmjWLG264gTlz5rBq1Srmz5/PfffdR3ObwVmzZg3Tpk0jLy+P119/nU2bNrF8+XJuvvlmnn32WQDOPvtsvvrqK1avXs3VV1/NH//4RwD+/Oc/8/zzz5Odnc2iRYsICwtrWO6SJUu49dZbef/990lJSeHOO+9k9uzZDX/4H374YQCmTJnCs88+29CgXXM9+uijDB8+nLVr1/LUU081tJnUVKY333yTiy66iOzsbNasWUNGRsZJretYfPpAmYgEACuBPsDzqnr0o3lJwC4AVa0TkTIgFig5ajlTgakAKSkpvoxsjF863jd3Xxk+fDhFRUUUFhZSXFxMTEwM3bp145577mHhwoW4XC52797Nvn376Nat2wmXN3LkSBITEwHo3bt3w7fy9PR05s+fD0BBQQETJ05kz5491NTUkJaWBnhaIb333nu57rrruPzyy+nRowfgabNn6tSpfPbZZ3Tv3p3c3Fxyc3O54IILAKivrycxMZHS0lJKS0sZO3Ys4Onb4JNPPmnWz2Hx4sUNfSKcd9557N+/n/Ly8iYzjRw5khtvvJHa2lp+9KMftVgh8OnFYlWtV9UMoAcwSkROqednVZ2uqpmqmhkfH9+iGY0xzpkwYQKzZ89m5syZTJw4kTfeeIPi4mJWrlxJdnY2CQkJDf0PnMiJ+jUAuPPOO/nZz35GTk4O//jHPxqW/eCDD/Liiy9y5MgRzjrrLDZs8JwmS0xMJDQ0lNWrVwOeJqwHDx7c0K9BTk4On332WYv9PBprKtPYsWNZuHAhSUlJTJ48mRkzZrTIulrlriFVLQXmA+OOmrQbSAYQkUAgCtjfGpmMMc6bOHEib7/9NrNnz2bChAmUlZXRtWtXgoKCmD9/Pvn5+S26vsZ9G7z22msN47du3Up6ejoPPPAAI0eObCgE0dHRfPTRRzz00EMsWLCA/v37U1xczNKlSwGora1l3bp1REdHEx0d3dAZzqn2bbBgwQLi4uKIjIxsMlN+fj4JCQnccsst3HzzzS3Wt4Ev7xqKF5Fo7/sw4ALg6KtRc4EbvO+vBOZpe2sX2xhzygYPHkxFRQVJSUkkJiZy3XXXkZWVRXp6OjNmzGDAgAEtur7HHnuMCRMmcMYZZxAXF9cw/q9//StDhgxh6NChBAUFMX78+IZpCQkJfPjhh9xxxx2sXr2a2bNn88ADDzBs2DAyMjJYsmQJAK+88gp33HEHGRkZzb6u8XWmlStXMnToUB588MGGAtVUpgULFjBs2DCGDx/OzJkzGy4mny6f9UcgIkPxXAgOwFNwZqnqEyLyBJClqnNFJBR4HRgOHACuVtVtx1uu9UdgTMuw/gg6rpPtj8BnF4tVdS2eP/BHj/9No/dVwISj5zHGGNN6rBlqY0y70d76Cvj000954IFv3pGVlpbGnDlzHErUNCsExvgxVaU9PbrT3voKuOiii7joootadZ2ncrrf2hoyxk+Fhoayf//+U/rDYdomVWX//v2Ehoae1OfsiMAYP9WjRw8KCgooLi52OoppQaGhoQ0PxDWXFQJj/FRQUFDDk7XGv9mpIWOM8XNWCIwxxs9ZITDGGD9nhcAYY/ycFQJjjPFzVgiMMcbPWSEwxhg/Z4XAGGP8nBUCY4zxc1YIjDHGz1khMMYYP2eFwBhj/JwVAmOM8XNWCIwxxs9ZITDGGD/ns0IgIskiMl9E1ovIOhG5q4l5visiZSKS7X39pqllGWOM8R1fdkxTB9ynqqtEpDOwUkQ+V9X1R823SFUv9WEOY4wxx+GzIwJV3aOqq7zvK4A8IMlX6zPGGHNqWuUagYikAsOBZU1MPlNE1ojIJyIy+BifnyoiWSKSZf2rGmNMy/J5IRCRCOBd4G5VLT9q8iqgp6oOA54F/tnUMlR1uqpmqmpmfHy8T/MaY4y/8WkhEJEgPEXgDVV97+jpqlquqpXe9x8DQSIS58tMxhhjvsmXdw0J8BKQp6p/OcY83bzzISKjvHn2+yqTMcaYb/PlXUNnAdcDOSKS7R33KyAFQFWnAVcCt4lIHXAEuFpV1YeZjDHGHMVnhUBVFwNygnmeA57zVQZjjDEnZk8WG2OMn7NCYIwxfs4KgTHG+DkrBMYY4+d8eddQm7Vh2WeUrXgLDQhBw7sQEJnIsPE3ERIa7nQ0Y4xpdX5TCNTtJnfxB7gW/5nBNWs5rCEAhEs1AMvqqhk94RdORjTGGEf4zamhrH8+S/q8ScTXFPBVv1/A/ZsJf7yIql/uZqcriYjNc52OaIwxjvCbI4IB51/Pcnc9wy69lTGNTgGFhkewO2k8o3a+RMnencR1S3EwpTHGtD6/OSLoHNWFUVfe2+R1gMTvXEOAKFsXvOFAMmOMcVazjwhE5DtAauPPqOoMH2RqdakDM9nhSiFy6wfAQ07HMcaYVtWsIwIReR34M3A2MNL7yvRhrla3J/liBtauY1/BVqejGGNMq2ruEUEmMKgjNwiXdNY1kD+N7QvfJOHaXzsdxxhjWk1zrxHkAt18GcRpKf0y2BqQRsy2D52OYowxraq5hSAOWC8in4rI3K9fvgzmhKKUi+lft4E9+RudjmKMMa2muaeGHvNliLYi5ZzrcW/7PwrmPEri3W87HccYY1pFs44IVPULYAcQ5H2/Ak9/wx1KUq+BLOsxmZGln5D14XSn4xhjTKto7l1DtwCzgX94RyVxjI7m27uRk//IhqBBDFjxG3Zvy3M6jjHG+FxzrxHcgafryXIAVd0MdPVVKCcFBgUT9ZNXcYuLQ29Ooram2ulIxhjjU80tBNWqWvP1gIgEAh32VtLEnv3ZMuYp+tVtYtWcvzodxxhjfKq5heALEfkVECYiFwDvAB8c7wMikiwi80VkvYisE5G7mphHRORvIrJFRNaKyIiT3wTfGDFuMnuJI2D3cqejGGOMTzW3EDwIFAM5wE+Bj1X14RN8pg64T1UHAWOAO0Rk0FHzjAf6el9Tgb83N3hr2BPej/jKDU7HMMYYn2puIXhMVV9Q1QmqeiXwsogct4U2Vd2jqqu87yuAPDwXmRu7DJihHl8B0SKSeJLb4DNVcekk1++msvyg01GMMcZnmlsIkkXkIQARCQbeBTY3dyUikgoMB5YdNSkJ2NVouIBvFwtEZKqIZIlIVnFxcXNXe9rCeo7AJcquPDs9ZIzpuJpbCG4E0r3F4EPgC1V9rDkfFJEIPIXjblUtP5WQqjpdVTNVNTM+Pv5UFnFKkgaOAaBsW1arrdMYY1rbcZ8sPuri7TN4niP4Es/F4xFfn/o5zueD8BSBN1T1vSZm2Q0kNxru4R3XJsR1S6GEaAL2rnU6ijHG+MyJmpj436OGDwKDvOMVOO9YHxQRAV4C8lT1L8eYbS7wMxF5GxgNlKnqnuYEbw3icrE7rB9xFXbB2BjTcR23EKjq905j2WcB1wM5IpLtHfcrIMW77GnAx8DFwBbgMDDlNNbnE4djhzB416tUHa4kNDzC6TjGGNPimtXonIhEAY8CY72jvgCeUNWyY31GVRcDcrzlevs3uKN5UZ0RkjyCwIKX2bYhi34jvut0HGOMaXHNvVj8MlABXOV9lQOv+CpUW5I4YBQAB7fYnUPGmI6puc1Q91bVKxoNP97odE+H1i25L6VEIHvWOB3FGGN8orlHBEdE5OyvB0TkLOCIbyK1LeJysSukL13KrSVSY0zH1NwjgluBGd5rBeC5e+gG30Rqeyq7pNO/8A1qqqsIDgl1Oo4xxrSo5h4RlKvqMGAoMFRVh+O5ZuAXgpMzCJZ6dm5YyaGKUrI/f9O6szTGdBjNPSJ4Fxhx1JPBs4EzWj5S25PQbzQsh4D3byegfjcZUkve8kEkPrzU6WjGGHPaTvRk8QBgMBAlIpc3mhQJ+M05ku5pA9lDPBHuMtbE/xBEGF08mw3LP2fAqAucjmeMMaflREcE/YFLgWjgB43GVwC3+ChTm+MKCCD+4fW4XAHEBwRwqKKUsv/9hCMLngYrBMaYdu5EhSAc+AUwXVX9+jxIYFBww/tOnaNZ2mMio3e9Qv7GbHr2z3AumDHGnKYTXSxOwdMb2R9F5DERGe1tQ8jv9fvBfdQSyL5//cnpKMYYc1qOWwhU9Q+qeh6e9oDW4GmOepWIvCkik0QkoTVCtkWxCT3IjruEjAP/omTvTqfjGGPMKWvW7aOqWqGqc1T1p95bR58E4oEZPk3XxiWNv59A6tn8wdGNtBpjTPtx3EIgIj9p9P6sr9+r6nqgWlUv8mG2Nq9HnyFsDB5IdNEKp6MYY8wpO9ERwb2N3j971LQbWzhLu1TZqSdxtYVOxzDGmFN2okIgx3jf1LBfqovqSTwHOXLIbx60NsZ0MCcqBHqM900N+6Wg+N4A7M23XsyMMe3TiZ4jGCAia/F8++/tfY93uJdPk7UTkd37AVBasBEGjXQ4jTHGnLwTFYJhQAKw66jxycBenyRqZ7qmDACgunirw0mMMebUnOjU0NN4OpTPb/wCyrzT/F5Ul66UE44c3OF0FGOMOSUnKgQJqppz9EjvuFSfJGpnxOWiKCCRsEp7qMwY0z6dqBBEH2da2PE+KCIvi0iRiOQeY/p3RaRMRLK9r9+cIEubVR7Wgy7Vu52OYYwxp+REhSBLRL7VyqiI3AysPMFnXwXGnWCeRaqa4X09cYJ526zqyJ4kuIuoq61xOooxxpy0E10svhuYIyLX8d8//JlAMPDj431QVReKSOrpBmwPAmJ7EVRYT2HBNrqnDXA6jjHGnJQTNTq3T1W/AzwO7PC+HlfVM1W1Je4aOlNE1ojIJyIy+FgzichUEckSkazi4uIWWG3LCu/WB4D9u6z7SmNM+9OsripVdT4wv4XXvQroqaqVInIx8E+g7zHWPx2YDpCZmdnmHmSLS/YcBRzet9nhJMYYc/Ka23l9i1PVclWt9L7/GAgSkTin8pyO+O5p1Ggg7gPbnY5ijDEnzbFCICLdvu7kRkRGebPsdyrP6QgIDGRvQAIh5XYLqTGm/WnWqaFTISJvAd8F4kSkAHgUCAJQ1WnAlcBtIlIHHAGuVtU2d9qnuQ6GJBFVVeB0DGOMOWk+KwSqes0Jpj8HPOer9be2qogUehfloG434nLsQMsYY06a/cVqIRqTRoQc4WDJHqejGGPMSbFC0EJCEzy3kBbttOaojTHtixWCFhKT5GmOurLQbiE1xrQvPrtG4G8SUjyFoLZkG8WFO9j6z6fQkM6ceZN1bG+MadusELSQ0PAIiuhC2s736PyPlxkjtQDkb7yenv0znA1njDHHYaeGWtDe0N7E6X7WxlzAxkvnUKVB7Pv0z07HMsaY47IjghbU48bX2F9Txagenn6Mly29mIySjynZu4u4bskOpzPGmKbZEUEL6tI1iQRvEQDoPu4+gqhjy4fWmZsxpu2yQuBDyX2HsabTdxhQMJMjhyqcjmOMMU2yQuBjYefeTTSVrP3weaejGGNMk6wQ+Fj/kd9nY+AAEjfOcDqKMcY0yQqBj4nLxcHU8aS4d3OgyPo1Nsa0PVYIWkFk79EA7Mr98hvjV3/2/9j5xCCqqw47EcsYYwArBK2i55AzqVfh8Pbl3xhfnzuHFPduSgrzHUpmjDFWCFpFp87R7AxIIbxkTcM4dbtJLl8NQEWJ9WNgjHGOFYJWUhw5mOQjG1C3G4A9OzeT4O2Q7cjBQiejGWP8nBWCVqLdh9OFcvbs9LROujv73w3Tasr2ORXLGGOsELSWLv3OBGDP+iUAaP6XlNMJtwruCisExhjnWCFoJT0HjqRGA6nduQKAxNLVbAsfykGJxHWoyOF0xhh/5rNCICIvi0iRiOQeY7qIyN9EZIuIrBWREb7K0hYEh4SyI6gXnQ+spWTvTpK1kKruoyl3xRBcVeJ0PGOMH/PlEcGrwLjjTB8P9PW+pgJ/92GWNuFgdDqp1ZvJX/kZADEDzqUyqAvhNfsdTmaM8Wc+KwSquhA4cJxZLgNmqMdXQLSIJPoqT1vg6nEGnaSKkLWvc1hD6DX0LKpC44isO96PyRhjfMvJawRJwK5GwwXecR1W1wGeC8ZDqrPZGjqIoOAQ6sPi6aKlDbeVGmNMa2sXF4tFZKqIZIlIVnFxsdNxTlly32FUahgAlQmjPCMjuhIitZSX2VGBMcYZThaC3UDjbrt6eMd9i6pOV9VMVc2Mj49vlXC+4AoIID/E08l9ZP9zAQiM7AZAWbE9XWyMcYaThWAuMMl799AYoExV9ziYp1WUJ4zkkIbSe7inEITGeC6LVJTY08XGGGf4rM9iEXkL+C4QJyIFwKNAEICqTgM+Bi4GtgCHgSm+ytKWDL/mcQ4UTaV7eAQAEXGeyyJV1syEMcYhPisEqnrNCaYrcIev1t9WhYZH0D21f8NwlLcQ1JbtdSqSMcbPtYuLxR1ZVJeu1GoAWmlPFxtjnGGFwGGugAAOShQBh9vv3VDGmPbNCkEbUBbQhZAqKwTGGGdYIWgDDgd3oVOtPUdgjHGGFYI2oDokjsj6g07HMMb4KSsEbUB9p6500VLc9fVORzHG+CErBG2ARCQQKG5K99stpMaY1meFoA0IikoArJkJY4wzrBC0AWEx3QGotCMCY4wDrBC0AZ3jPIWgutTTzERNdRVZc6eRt+xTKqxVUmOMj/msiQnTfNFdPY2w1pV7jghWz3ma0Rt+75n4CeS7ehD4k9kk9RroVERjTAdmRwRtQETnaKo0CCr2oW43CZveZEtAb9aMfYGlSZPp6S6gMPszp2MaYzooKwRtgLhcHHDFEHikhLzln5Hq3snBwTcw7LyrGDXlf6nRQOqKNzkd0xjTQVkhaCMqAroQUl3C4SUvUE44gy+cDEBAYCC7A5IILdvmbEBjTIdlhaCNOBwcS0J1PkPLFrA+/hLCI6Iaph0M60lsVb6D6YwxHZkVgjaiJiyOrhwgWOpIPO+2b0yrju5NYv1eamuqHUpnjOnIrBC0Ee7wrgCsC06n58AzvjEtsGs/gqSewu15TkQzxnRwVgjaCFdnz9PFR4bd8K1pUcmDATiQn9uqmYwx/sGeI2gjeo+9mqWHSsi8YNK3piX2Tgegau/G1o5ljPEDVgjaiLhuycRN+UOT0zpHdaGYGAIPbG7lVMYYf+DTU0MiMk5ENorIFhF5sInpk0WkWESyva+bfZmnPSsKTqFz5Q6nYxhjOiCfFQIRCQCeB8YDg4BrRGRQE7POVNUM7+tFX+Vp7yo7p5FYtxN1u52OYozpYHx5RDAK2KKq21S1BngbuMyH6+vQNLYvURziQHGh01GMMR2MLwtBErCr0XCBd9zRrhCRtSIyW0SSm1qQiEwVkSwRySou9s9O3sO7exqc27fd7hwyxrQsp28f/QBIVdWhwOfAa03NpKrTVTVTVTPj4+NbNWBbEZc6BIDKgvUAVJQdIPd35/LV67+xLi6NMafFl4VgN9D4G34P77gGqrpfVb9+XPZF4JtPUpkG3ZL7UKVBuL2Nz+W++zuGVGczZusz5P7pQvbvs97NjDGnxpeFYAXQV0TSRCQYuBqY23gGEUlsNPhDwB6dPQZXQACFAUmElW+jtGQv6fmvs6rTOSwb9Aj9j6zB/fez2Za7zOmYxph2yGeFQFXrgJ8Bn+L5Az9LVdeJyBMi8kPvbD8XkXUisgb4OTDZV3k6goPhqcRW5ZP37m8Jp4oulz7G6KvuZ/eEjwjTKkrmPet0RGNMO+TTB8pU9WPg46PG/abR+4eAh3yZoSOpielDYsUXxBbOYlXU98kcmAlAryGjWfvhEOJLcxxOaEz7criyjLDwzojL6culzvLvrW9ngrr2I0CUQOpJ/NHj35h2qOsIetbnWx/HxjRTbU011X8ezLJptzodxXFWCNqR6BRP43OrYi8hqdfgb0yL6D0Glyj5axc5Ec2Ydqdwex4xVDCmaCZr5s1yOo6jrBC0I73Tv8OyQQ8z4Lo/f2taytCxAFRsWdLasYxpl75uzfcgnemx8BeU7N11gk90XFYI2hFxuRh91S+Jik341rSomDh2uJIJL1rtQDJj2p9qb2u++8a/TCc9zO5Xp/htEy5WCDqQosh0Uo6s99tfZl/YunYJW9Ystof2OiDXgS3sJ4oBoy9kzcD7GFa1gtV/+RFr5r1NTXWV0/FalRWCjqTHSGKooGDbOqeTdAgbVvyb5Hcvpc+cSyj/bQqr/vQDNq1a4HQs00I6H9rBviDPM6+jrnqArxKuoU/lCoYt/ClVv0tjxXvPOJyw9Vgh6EDiB50DwN51dsH4dJXs3UmXj26h2BXHioyn2BQ9lrRD2UR8cIv1Hd1BdKstoDIiFfCcdh1z2zRCH9rOmrEvUBjYk0FrnvKbJ/atEHQgKf2GU6lhuHf+9wnjsgPFHK4sczBV+1NTXUXxS1cToYeoufJ1Rv7oDkbd/RY7x/6J7lrE6o/+4XREc5rK9u8jhnLcsX2+MT44JJRh511F+FXTCKGGzbMfcyZgK7NC0IEEBAayPXQAsd4Hy3Zuyoa/DSPvH9/uB9kc2+oXbmdg7TryRj1F2uDRDeOHfvcqtgT0pvva/6OutsbBhOZ07fG24hvabUCT01P6ZbAq9hJGFL1H4fYNrRnNEVYIOpjK+OGk1m1nT/5GXG9NJIpDDCr/kqrDlU5HaxdKCvMZXfIuy+Iu54xLvtlhnrhcVIy+lx66h+xPXnIooWkJFQWeZs1iezbVV5ZH6hW/xY2LwjmPtFYsx1gh6GDCe40mUNy4XhlPvHs/S3veSpjUsGHJB05Haxe2LX0PgPjvNv206bDzr2G7K5X47Oeor6trzWimBdUVbaJWA+jWs+kjAoCuSWmsTrqGEWX/ZmvOV62YrvVZIehgeg79LgAJ7Cd39B8449rHqdAwatZZIWiOoK2fsZd40gaNbHK6KyCAA5l309NdwOpPX2nldKalhJRtY09AN4KCQ44736AJj1Ih4Rz66OFWSuYMKwQdTHRcN1ZEjeOr/g9wxsU3ERwSyqbIMfQ+uNi+wZ5A1ZFD9D+0kvy4c47bCNnwiyax3dWTocsfYNmzN1C0e3srpjQtocuRfPaH9jzhfFExceT1vZWhVVmsnT+7FZI5wwpBBzTynpmMueZXDcPa/2JiKWPz6gXOhWoHNi79iHCpJmzIpcedzxUQQMTNc1kd9wOGl3xA1PSRLH3lgVZKaU5XfV0difV7qI5Ma9b8I678JQWSSOSixzrsTQJWCPxA37Mup1YDOLjqfaejtGlV6z7isIbQb/S4E84b3z2V0Xe+RsmUpeR2Posz86ex7O3ftUJKc7r27txMiNTiiu/XrPmDQ0IpPvMRUt27WPne0z5O5wwrBH4gKiaOjaHpdN83z+kobZa63aTuX8TGiJGEhnVq9ue6p/Yn4+53WR3+HTLz/tChTx90FCXexuY69xjY7M9kfP9a1gUPpV/ec5QdLPFVNMdYIfATlakX0dNdwK7Na74xfu+uLXz1/x5j6Wu/6pC/4M21LfcrEthPbZ+LTvqzAYGB9LvtLXYEppK24GfsyMvyQULTUo7s8TwXkJA2pNmfEZeLkEt+T5RWkPd2x7ud1Kc9lJm2o+d3roCNf6Dwkz9TuHoo7vJCovctY2DtOrp55yl95jW+6vtThl/xC0JCw4+5LHW72V9UQKfOMYR16tw6G+BjRSvfJ02FXmf++JQ+36lzNJ0mz6b6pfOJnnkZS3v+hEE/up/IqC7kLf+M6oXPkHR4A9tjxxI56lr6Z34fV0BAC2+FaQ7Zv4UyOhETl3jimRvpM+wsln3xA0btfZvcReMZcs5lTc5XuGMj5UU76Z95frvp+UxU1ekMJyUzM1Ozsuwb16nY/Nsz6Fu/BYB6FXYGpLA3eTzJ51xP1aEyDn/ya4ZWraRAEqmd8Pq3bqGsqa5i7aevErX2RfrWbQbgsIZQHBBP8Yi7OOPimx37xV82608MXP80VYRwyBVJeXA87hGTGXb+NQ1/cNXtZv/eXeByER4RSWhYBBXlB6nYv5eqtyZRL4H0f2T5aeXIz1tJ6dyHGHZkGRUaxr7A7vSp38pBItkRPoT+h1YSLtUUSld29r6WgRf/jKgu8S3xI6Cy/CChYZ0IDApukeV1VOueGkugu5r+jyw78cxHOVxZRvFfzqKTuwJuXURctxQA6mprWDtvJoGrX2XIkZW4RFnd6WySr59GXLfklt6EUyIiK1U1s8lpVgj8R8neXRwo3Ep0Qk+6dE1q8g9GzhfvkTj/HsK0is1nP03GBddysHgPGz96ll473qIrB9jpSmJ36hUAyKFiYktW0Ld+C6s6nUPPSdMozs+j4ssXGFT6BTuDe1MxYAIDzp9EZHSsT7Zr7fzZDF5wM5uCB1ERkUpQdSndDm8ikWK2u3qyr+9EpHgDyQe/orsWHXM5S9N+xpk3/E+LZNq6dglln/2eyCMF7O9/NUMvuY2wTp05VFFK3vy3CMt9g8E1ORzWEHLiLyH50gfpntr/mMtTt/u4RXb7umVEvXMl+wO6Enfrh8TEn9y33Y6scPsGCuc8TG2nbkQM/D7d5t3DjqjRjLxn5iktb0deFglvj2dbyED6/eJz1nw+g4Ssv5CshRTRha3JV0BgCCO2/YPDEsrWEY+QfuGk4x5lN0dl+UECg4JP6hpWY44VAhEZBzwDBAAvqurvj5oeAswAzgD2AxNVdcfxlmmFwPeKdm+n7JUJ9K7dQk74SPofXk2o1JITMgL3mNtJH3v5N05r1NXWkPXWbxmx9f8AJVjqqdQwNkSdTXxlHj3dBVRpEDuD0qgMTaQmogehfccy6OwfERwS+q317921hR2L3iZgXw5hVXvpXFtCjYRSknQeCaOuJG3QyIY/ivl5K+ny9iUUBSbS7e75dOoc3ZAp+5OXict+jlT3Lio1jM2dRlDd4ztIQBBaXYnWHkZCIwnoFEdIVAL9z7z4tP+znoyta5dwYN7fGHbwcwRldcxFdL/0YXr08Zy7Vreb9Us/4ciylxlS9gX7XV3Y03ko9T1G0evsq4jvngrA9vUriJ51OfW4iNBD7A3oRvhNH9A1qenbI6sOV5I7701Cct4kovYA+3r9mIHjb2/o8Ki8dD9lJYV0Tx1IQKDn7HFdbQ0bl39OxbZldErJoPcZ5xMeEcWBot1sWTQLV/5iNHUsQy++5Vs/w0MVpWxa+iE1m/6DOzSamIxL6ZtxbsOyv+4/o3GhU7ebjavmU7lvGzHJg0nqM5SQ0HDKDhRRvGsTNYcrSB16VsP+PpbNqxfS5f2fEKZVBFJHsHj6lViadgdn3vDUSeytb1o+51lGrXmEEqKJo5RtrlRKR/+CoedNbPiClb9hFdWzf0q/uk0c0lA2dh5FfZ+L6D70e3RPHehpsqTsAJuXvE9N/goiM37IoDHfvmNN3W5Wf/oaPZY9wdaUKznzxj+dUmZHCoGIBACbgAuAAmAFcI2qrm80z+3AUFW9VUSuBn6sqhOPt1wrBK2j6nAludMmM7jsC9bGjiPhgrtIHdjk71CD/LyV7Pv8aUg6g8EXTqFT52jU7WbTqgUcXP424eVbiKneQ4K7iGCpo4xObIw+l/qYXlBXhdQeIaZkJf3rPBfziujCwcCuHArtSlh1Cf1r8nCJso9YdncaRHXXofTcMZtgrabupv/QLbnPtzK56+sp2JpDYurAEz5F6pR9BVvZ8f5TZBS9T4jUUq1BVIjnW18cpZQTzoaY7xFYU0HKobXEUUqdulgbcRbu/pfQa9VT1BFI9U/mUl60i56f3ki5K5JdA2+GvTnElq0jor6cKlcoNa4wutXtJpJDFEpXygNiGVCXR5UGsTVkILE1hXTDc9PAYQ1hZ3BvDgfH0fvQSqI41JC5RgMoDEgiuX4XAaKUE04khykmhq2p10JIJ6RkExEVW+lbnUew1HFYQwihhgBRDhJJqasLnd2lRGkFlRLOzrBBHEnIRGsq6bnnX3TXfQ3rc6twhBA6yX87jKnVALYG9aM0dhgaEoUEhyHBEYTEJhOV2JvSgo30//JeSl1R1F79DnHdU9mS9TmHt6+gz7jbGwrpqVr23BQSDqygZMTPGTH+piav+dTV1pC78D2q131ErwOLiOcgAAeJpCiwO2m1mwmWetwquERZF5yO++z76NprKC5XAJUH91HxwcMMrVrB1oBe1I3/X/pnnndKeZ0qBGcCj6nqRd7hhwBU9XeN5vnUO89SEQkE9gLxepxQVghaV11tTYufc66priLvy7nUrHmHgaWLiJAjAFRpEAWBKZQkjyPprIkk9x32jc+V7N3FtsXvEJC/iITK9fTQvVRpEPk/mHXK/znakpLCfLbMexkqi3HVlOOqO4L2Oo8hF0xquCivbje7tqylcN50+u+dSwwVFBND1XXvN/y8Nq36gq5zryWaSsrpRH5IP6pCuxJQd4TA+iNUh8QQlvkTBn3nUlwBAWzLXUbx/OfpUrqOsk6p1MYOILBzV+r35hJ1cB0xtUXsihpBwMCLSck4nz0bV1C5cT6dDqzjcFwG8aOupNeQMeQufh++fIb0ak93qeWEszewBwdizyAi/WL6jbyQI5VlbP5qLrrpcwJrK6kNiaE+tAsBR0pIKF9LT3cBdepifdgIqgdcTmyfTA7uXE/N3vW4jhxAo3sSEt8LV1AIhzcvIqZoOWm1WwiR2iZ/plsCehN9yz8bzuU7yV1fT/6GLIrzvoSCFXQ+tIOy2OFEZvyQlEGjyf3weXpteomuHPjG5w5pKDn97yRzwi9P6/+iU4XgSmCcqt7sHb4eGK2qP2s0T653ngLv8FbvPCVHLWsqMNU72B/YeIqx4gB/vEfSH7fbH7cZ/HO7/XGb4eS3u6eqNnlnQru4fVRVpwPTT3c5IpJ1rIrYkfnjdvvjNoN/brc/bjO07Hb78l6/3UDj+6Z6eMc1OY/31FAUnovGxhhjWokvC8EKoK+IpIlIMHA1MPeoeeYCX3efdSUw73jXB4wxxrQ8n50aUtU6EfkZ8Cme20dfVtV1IvIEkKWqc4GXgNdFZAtwAE+x8KXTPr3UTvnjdvvjNoN/brc/bjO04Ha3uwfKjDHGtKz20RCGMcYYn7FCYIwxfs5vCoGIjBORjSKyRUQedDqPL4hIsojMF5H1IrJORO7yju8iIp+LyGbvvzFOZ/UFEQkQkdUi8qF3OE1Elnn3+UzvTQsdhohEi8hsEdkgInkicqY/7GsRucf7+50rIm+JSGhH3Nci8rKIFHmft/p6XJP7Vzz+5t3+tSIy4mTW5ReFwNvcxfPAeGAQcI2IDHI2lU/UAfep6iBgDHCHdzsfBP6jqn2B/3iHO6K7gLxGw38AnlbVPsBB4CZHUvnOM8C/VHUAMAzPtnfofS0iScDPgUxVHYLnRpSr6Zj7+lXg6MaHjrV/xwN9va+pwN9PZkV+UQiAUcAWVd2mqjXA20DTjYm3Y6q6R1VXed9X4PnDkIRnW1/zzvYa8CNHAvqQiPQALgFe9A4LcB7wdZdhHWq7RSQKGIvnzjtUtUZVS/GDfY3nbscw77NH4cAeOuC+VtWFcFR7E8fev5cBM9TjKyBaRJrdBK2/FIIkYFej4QLvuA5LRFKB4cAyIEFV93gn7QUSnMrlQ38Ffgm4vcOxQKmq1nmHO9o+TwOKgVe8p8NeFJFOdPB9raq7gT8DO/EUgDJgJR17Xzd2rP17Wn/j/KUQ+BURiQDeBe5W1fLG07wP7HWoe4ZF5FKgSFVXOp2lFQUCI4C/q+pw4BBHnQbqoPs6Bs+33zSgO9CJb58+8QstuX/9pRA0p7mLDkFEgvAUgTdU9T3v6H1fHyZ6/z127yzt01nAD0VkB57TfufhOX8e7T19AB1vnxcABar6dTdbs/EUho6+r78PbFfVYlWtBd7Ds/878r5u7Fj797T+xvlLIWhOcxftnve8+EtAnqr+pdGkxk153AC839rZfElVH1LVHqqaimffzlPV64D5eJougQ623aq6F9glIl93a3Y+sJ4Ovq/xnBIaIyLh3t/3r7e7w+7roxxr/84FJnnvHhoDlDU6hXRiquoXL+BiPB3lbAUedjqPj7bxbDyHimuBbO/rYjzny/8DbAb+DXRxOqsPfwbfBT70vu8FLAe2AO8AIU7na+FtzQCyvPv7n0CMP+xr4HFgA5ALvA6EdMR9DbyF5zpILZ4jwJuOtX8BwXNn5FYgB89dVc1elzUxYYwxfs5fTg0ZY4w5BisExhjj56wQGGOMn7NCYIwxfs4KgTHG+DkrBMaviUi9iGQ3erVYI20iktq45chmzN9JRP7tfb+40QNSxviU/aIZf3dEVTOcDuF1JrDU24zCIf1v2znG+JQdERjTBBHZISJ/FJEcEVkuIn2841NFZJ63zff/iEiKd3yCiMwRkTXe13e8iwoQkRe87ed/JiJhTayrt4hkA/8PuBZPI2rDvEcoXVtni40/s0Jg/F3YUaeGJjaaVqaq6cBzeFo3BXgWeE1VhwJvAH/zjv8b8IWqDsPT5s867/i+wPOqOhgoBa44OoCqbvUelazE02T6a8BNqpqhqh2trSDTBtmTxcaviUilqkY0MX4HcJ6qbvM25LdXVWNFpARIVNVa7/g9qhonIsVAD1WtbrSMVOBz9XQigog8AASp6pPHyLJCVUeKyLvAXapa0NLba0xT7IjAmGPTY7w/GdWN3tfTxHU5EZnmvajc13uKaBzwoYjcc4rrNOakWCEw5tgmNvp3qff9EjwtnAJcByzyvv8PcBs09J0c1dyVqOqteBpS+y2eHqc+8p4Wevq00hvTTHbXkPF3Yd5v4V/7l6p+fQtpjIisxfOt/hrvuDvx9Ap2P54ewqZ4x98FTBeRm/B8878NT8uRzXUuMAM4B/jiVDbEmFNl1wiMaYL3GkGmqpY4ncUYX7NTQ8YY4+fsiMAYY/ycHREYY4yfs0JgjDF+zgqBMcb4OSsExhjj56wQGGOMn/v/kI6sfXWHFskAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['masked_loss'], label='masked_loss')\n",
        "plt.plot(history.history['val_masked_loss'], label='val_masked_loss')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the aacuracy from the training"
      ],
      "metadata": {
        "id": "lUssYQFZet7E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "KkhXRASNG80_",
        "outputId": "bea69479-2ee6-42fd-d200-f9ebbc1b9329"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f75b7ff05e0>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(history.history['masked_acc'], label='accuracy')\n",
        "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "### Translate Module Development\n",
        "\n",
        "Now that the model is trained, implement a function to execute the full `text => text` translation. This code is basically identical to the [inference example](#inference) in the [decoder section](#the_decoder), but this also captures the attention weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "mmgYPCVgEwp_"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "@Translator.add_method\n",
        "def translate(self,\n",
        "              texts, *,\n",
        "              max_length=50,\n",
        "              temperature=0.0):\n",
        "  # Process the input texts\n",
        "  context = self.encoder.convert_input(texts)\n",
        "  batch_size = tf.shape(texts)[0]\n",
        "\n",
        "  # Setup the loop inputs\n",
        "  tokens = []\n",
        "  attention_weights = []\n",
        "  next_token, done, state = self.decoder.get_initial_state(context)\n",
        "\n",
        "  for _ in range(max_length):\n",
        "    # Generate the next token\n",
        "    next_token, done, state = self.decoder.get_next_token(\n",
        "        context, next_token, done,  state, temperature)\n",
        "        \n",
        "    # Collect the generated tokens\n",
        "    tokens.append(next_token)\n",
        "    attention_weights.append(self.decoder.last_attention_weights)\n",
        "    \n",
        "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  # Stack the lists of tokens and attention weights.\n",
        "  tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
        "  self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
        "\n",
        "  result = self.decoder.tokens_to_text(tokens)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4XufRntbbva"
      },
      "source": [
        "Here are the two helper methods, used above, to convert tokens to text, and to get the next token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "E5hqvbR5FUCD",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#Individual translator mechanism, can be used to translate each data separately\n",
        "\n",
        "\n",
        "result1 = model.translate([''])\n",
        "\n",
        "result2 = model.translate([''])\n",
        "\n",
        "result23 = model.translate([''])\n",
        "\n",
        "result222 = model.translate([''])\n",
        "#result1[0].numpy().decode()\n",
        "#result2[0].numpy().decode()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ1iU63cVgfs"
      },
      "source": [
        "### Attention plot generation after model training has been completed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "s5hQWlbN3jGF"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "@Translator.add_method\n",
        "def plot_attention(self, text, **kwargs):\n",
        "  assert isinstance(text, str)\n",
        "  output = self.translate([text], **kwargs)\n",
        "  output = output[0].numpy().decode()\n",
        "\n",
        "  attention = self.last_attention_weights[0]\n",
        "\n",
        "  context = tf_lower_and_split_punct(text)\n",
        "  context = context.numpy().decode().split()\n",
        "\n",
        "  output = tf_lower_and_split_punct(output)\n",
        "  output = output.numpy().decode().split()[1:]\n",
        "\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + context, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + output, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  ax.set_xlabel('Input text')\n",
        "  ax.set_ylabel('Output text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "rrGawQv2eiA4"
      },
      "outputs": [],
      "source": [
        "#model.plot_attention('') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHBdOf9duumm"
      },
      "source": [
        "Translate a few more sentences and plot them:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA3xI3NzrRJt"
      },
      "source": [
        "The short sentences often work well, but if the input is too long the model literally loses focus and stops providing reasonable predictions. There are two main reasons for this:\n",
        "\n",
        "1. The model was trained with teacher-forcing feeding the correct token at each step, regardless of the model's predictions. The model could be made more robust if it were sometimes fed its own predictions.\n",
        "2. The model only has access to its previous output through the RNN state. If the RNN state looses track of where it was in the context sequence there's no way for the model to recover. [Transformers](transformer.ipynb) improve on this by letting the decoder look at what it has output so far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtz6QBoGWqT2"
      },
      "source": [
        "The raw data is sorted by length, so try translating the longest sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "-FUHFLEvSMbG"
      },
      "outputs": [],
      "source": [
        "long_text = context_raw[-1]\n",
        "\n",
        "import textwrap\n",
        "#print('Expected output:\\n', '\\n'.join(textwrap.wrap(target_raw[-1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing unseen samples"
      ],
      "metadata": {
        "id": "Rc1aekzi9dLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dc = pd.read_csv('OS.csv')"
      ],
      "metadata": {
        "id": "6OIFQKZI9bc5"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dc.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Nsx0IyYZ9k3v",
        "outputId": "312176c8-de1f-4b2e-c8bc-11dfb223b186"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          OM_Regular  OM_Prediction\n",
              "0  moduleOM_nameopenDeclarationonesigclass1_namee...              1\n",
              "1  moduleOM_nameopenDeclarationonesigclass1_namee...              1\n",
              "2  moduleOM_nameopenDeclarationonesigclass1_namee...              0\n",
              "3  moduleOM_nameopenDeclarationonesigclass1_namee...              0\n",
              "4  moduleOM_nameopenDeclarationonesigclass1_namee...              1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4e9e552b-bbe3-4aea-8e99-04edb7c437c2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OM_Regular</th>\n",
              "      <th>OM_Prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>moduleOM_nameopenDeclarationonesigclass1_namee...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>moduleOM_nameopenDeclarationonesigclass1_namee...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>moduleOM_nameopenDeclarationonesigclass1_namee...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>moduleOM_nameopenDeclarationonesigclass1_namee...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>moduleOM_nameopenDeclarationonesigclass1_namee...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e9e552b-bbe3-4aea-8e99-04edb7c437c2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4e9e552b-bbe3-4aea-8e99-04edb7c437c2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4e9e552b-bbe3-4aea-8e99-04edb7c437c2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Separating Columns in X_test and y_test"
      ],
      "metadata": {
        "id": "er0zQybAgoJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test2 = dc['OM_Regular'].values\n",
        "y_test2 = dc['OM_Prediction'].values"
      ],
      "metadata": {
        "id": "naG54qF791Hs"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test2.shape)\n",
        "print(y_test2.shape)\n",
        "\n",
        "print(\"\\nX data type: \", X_test2.dtype)\n",
        "print(\"y data type: \", y_test2.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcNO_Ews2q8x",
        "outputId": "c2321002-6ba1-43b8-f74b-db53d72a4b10"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8,)\n",
            "(8,)\n",
            "\n",
            "X data type:  object\n",
            "y data type:  int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZFASLWP95TU",
        "outputId": "3d55fe0f-46de-4b6f-f073-2aa00541fdfe"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 0 0 1 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = X_test2"
      ],
      "metadata": {
        "id": "hgO5sa73-3f1"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Obtaining results from the model of the unseen dataset"
      ],
      "metadata": {
        "id": "K_yUzQq_gyYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# for t in inputs:\n",
        "#   mylist_res = model.translate([t])[0].numpy().decode()\n",
        "#   print(model.translate([t])[0].numpy().decode())\n",
        "\n",
        "# print()"
      ],
      "metadata": {
        "id": "4qjPTIDB-8UZ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Report (Unseen samples)\n"
      ],
      "metadata": {
        "id": "1t4_2FqbE9da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve, classification_report"
      ],
      "metadata": {
        "id": "fVaZsDnJhkz5"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The result is obtained and captured in a separate file, labels are converted to 1 and 0 . Where 1 denotes P and 0 denotes NP. "
      ],
      "metadata": {
        "id": "TbThCFoRhLHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###READING the predicted dataset"
      ],
      "metadata": {
        "id": "9Jz3Rt18lUtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dd = pd.read_csv('OS_Pred.csv')"
      ],
      "metadata": {
        "id": "jhKnUY4XFCSj"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dd.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "v9M2iW1MGjfM",
        "outputId": "56e9c8a3-3c43-4731-c06d-f3a6175781a6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          OM_Regular  OM_Prediction\n",
              "0  moduleOM_nameopenDeclarationonesigclass1_namee...              1\n",
              "1  moduleOM_nameopenDeclarationonesigclass1_namee...              1\n",
              "2  moduleOM_nameopenDeclarationonesigclass1_namee...              1\n",
              "3  moduleOM_nameopenDeclarationonesigclass1_namee...              1\n",
              "4  moduleOM_nameopenDeclarationonesigclass1_namee...              1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c085f067-296c-4d61-9e96-5b1efae451a4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OM_Regular</th>\n",
              "      <th>OM_Prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>moduleOM_nameopenDeclarationonesigclass1_namee...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>moduleOM_nameopenDeclarationonesigclass1_namee...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>moduleOM_nameopenDeclarationonesigclass1_namee...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>moduleOM_nameopenDeclarationonesigclass1_namee...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>moduleOM_nameopenDeclarationonesigclass1_namee...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c085f067-296c-4d61-9e96-5b1efae451a4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c085f067-296c-4d61-9e96-5b1efae451a4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c085f067-296c-4d61-9e96-5b1efae451a4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_pred2 = dd['OM_Regular'].values\n",
        "y_test_pred2 = dd['OM_Prediction'].values"
      ],
      "metadata": {
        "id": "1tO_WHmVHQDR"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Printing predicted labels"
      ],
      "metadata": {
        "id": "0nbGKNUjldCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (y_test_pred2 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy2Fvt1fHYJO",
        "outputId": "b1cde5b4-12ff-4a3e-a3c8-8da06bf50558"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 1 1 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision = precision_score(y_test2, y_test_pred2) \n",
        "print(\"Testing: Precision = %f\" % precision)\n",
        "\n",
        "\n",
        "recall = recall_score(y_test2, y_test_pred2)\n",
        "print(\"Testing: Recall = %f\" % recall)\n",
        "\n",
        "\n",
        "f1 = f1_score(y_test2, y_test_pred2)\n",
        "print(\"Testing: F1 Score = %f\" % f1)\n",
        "\n",
        "print(\"\\nConfusion Matrix (Test Data):\\n\", confusion_matrix(y_test2, y_test_pred2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7RY4modHkts",
        "outputId": "eaeef9b6-f130-46ed-b576-a34dbbfbcad0"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: Precision = 0.666667\n",
            "Testing: Recall = 1.000000\n",
            "Testing: F1 Score = 0.800000\n",
            "\n",
            "Confusion Matrix (Test Data):\n",
            " [[2 2]\n",
            " [0 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test2,y_test_pred2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd3P-TGIIN6b",
        "outputId": "54b5670e-6369-4bae-bb96-869ab386be6d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.50      0.67         4\n",
            "           1       0.67      1.00      0.80         4\n",
            "\n",
            "    accuracy                           0.75         8\n",
            "   macro avg       0.83      0.75      0.73         8\n",
            "weighted avg       0.83      0.75      0.73         8\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}