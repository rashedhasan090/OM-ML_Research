Exp 13 Jan23 - Regular Dataset  - Tokenization 3 - 6 OM combined


Notes
Optimizers used: 'Adam''Adadelta', 'Adagrad', 'Adam', ‘SGD’, ‘RMSProp’



Tokenization structure:

- Comma separated tokenization, section based separation noticed.
- entity separation noticed.
- Each section appeared as separated
- labels appended with each section
- organized tokenization approach



Ran multiple times to observe translations



- Run 1 : Adam (Sample 1 - 15) val steps=40, epochs 100

- Run 2 : Adadelta (Sample 16 - 30) val steps=40, epochs 100
- Run 3 : Adagrad(Sample 31 - 45) val steps=40, epochs 100
- Run 4 : SGD (Sample 46 - 60) val steps=40, epochs 100
- Run 5 : RMSProp (Sample 61 - 75) (no prediction)

- Run 6 : Adam (Sample 76 - 90) val steps=40, epochs 100
- Run 7 : Adadelta (Sample 91 - 101) val steps=40, epochs 100





---------------------------



Total Instance = 101

Pareto = 15
Not Pareto = 86


correctly predicted P = 2 (TP)
Incorrectly predicted P = 13 (FN)


correctly predicted NP = 68 (TN)
Incorrectly predicted NP = 18 (FP)

Incorrectly predicted = 13 predicted as NP but they were P | 2 predicted as P and they are P
18 predicted as P but they were NP | 68 predicted as NP and they are NP





Precision - TP / (TP + FP) = 2 / (2 + 18) = 0.025
Recall - TP / (TP + FN) = 2 / (2 + 13) = 0.13
Accuracy = (TN + TP ) / (TN + FP + TP + FN ) = (68 + 18) / (68 + 18 + 2 + 13) = 0.85
F1 = 2 * (Precision * Recall / Precision + Recall) = 2 * (0.025 * 0.13 / 0.025 + 0.13) = 2 * (0.00325 / 0.155) = 0.02096
