{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "[link text](https://)  \n",
        "#  Language Model Performance assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eLy6lCbgjwC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uReDnZIljwAl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAmSR1FaqKrl"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGFTkuRvzWqc",
        "outputId": "473deb32-9cc6-4a08-b68f-296d43189766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-text>=2.10\n",
            "  Downloading tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow<2.12,>=2.11.0\n",
            "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-text>=2.10) (0.12.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (3.19.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (4.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.15.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (0.2.0)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.6.3)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.51.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.3.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (3.3.0)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (21.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (15.0.6.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (0.29.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.14.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (2.2.0)\n",
            "Collecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m124.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (0.38.4)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (2.16.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (2.25.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (3.0.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (5.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (6.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (4.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text>=2.10) (3.2.2)\n",
            "Installing collected packages: flatbuffers, tensorflow-estimator, keras, tensorboard, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "Successfully installed flatbuffers-23.1.21 keras-2.11.0 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-text-2.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m132.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install \"tensorflow-text>=2.10\"\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pathlib\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KqFqKi4fqN9X"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    parsed = einops.parse_shape(tensor, names)\n",
        "\n",
        "    for name, new_dim in parsed.items():\n",
        "      old_dim = self.shapes.get(name, None)\n",
        "      \n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB7rgJDbeBDF"
      },
      "source": [
        "#### Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "daNcrh1lVej7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "ORM_data = pd.read_csv('dummy_data.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ve7kyoOxWY1u",
        "outputId": "faf2a32e-6271-49e8-cb11-8c626b7a60aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                OM_Regular              OM_Prediction\n",
              "0  class1,table1,obj1,atr1  class1,table1,obj1,atr1,P\n",
              "1  class1,table2,obj1,atr1  class1,table2,obj1,atr1,P\n",
              "2  class1,table3,obj1,atr1  class1,table3,obj1,atr1,P\n",
              "3  class1,table4,obj1,atr1  class1,table4,obj1,atr1,P\n",
              "4  class1,table5,obj1,atr1  class1,table5,obj1,atr1,P"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dfb8825f-ecf4-4af5-9f6f-eb9eb0a0bff8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OM_Regular</th>\n",
              "      <th>OM_Prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>class1,table1,obj1,atr1</td>\n",
              "      <td>class1,table1,obj1,atr1,P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>class1,table2,obj1,atr1</td>\n",
              "      <td>class1,table2,obj1,atr1,P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>class1,table3,obj1,atr1</td>\n",
              "      <td>class1,table3,obj1,atr1,P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>class1,table4,obj1,atr1</td>\n",
              "      <td>class1,table4,obj1,atr1,P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>class1,table5,obj1,atr1</td>\n",
              "      <td>class1,table5,obj1,atr1,P</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dfb8825f-ecf4-4af5-9f6f-eb9eb0a0bff8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dfb8825f-ecf4-4af5-9f6f-eb9eb0a0bff8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dfb8825f-ecf4-4af5-9f6f-eb9eb0a0bff8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "ORM_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V7OaHrVYV-Xd"
      },
      "outputs": [],
      "source": [
        "OM_Regular = ORM_data['OM_Regular'].values\n",
        "OM_Prediction = ORM_data['OM_Prediction'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jTBVOEjFWAI5"
      },
      "outputs": [],
      "source": [
        "X = OM_Regular\n",
        "Y = OM_Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOujEo2geGod"
      },
      "source": [
        "#### Dividing data as Target and Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTbSbBz55QtF",
        "outputId": "1ecb4ec2-e311-4e72-9927-90253c92f7c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class6,table18,obj1,atr1\n"
          ]
        }
      ],
      "source": [
        "target_raw =  Y\n",
        "context_raw = X\n",
        "print(context_raw[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH_dPY8TRp3c",
        "outputId": "2835e0d1-69b9-49f5-9e4a-91c87daf3afa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class6,table18,obj1,atr1,P\n"
          ]
        }
      ],
      "source": [
        "print(target_raw[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfVWx3WaI5Df"
      },
      "source": [
        "From these arrays of strings you can create a `tf.data.Dataset` of strings that shuffles and batches them efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3rZFgz69nMPa"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(context_raw)\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
        "\n",
        "train_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw[is_train], target_raw[is_train]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE))\n",
        "val_raw = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((context_raw[~is_train], target_raw[~is_train]))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc6-NK1GtWQt",
        "outputId": "d09686e7-1014-456e-83c1-3087454894f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([b'class4,table17,obj2,atr2'], shape=(1,), dtype=string)\n",
            "\n",
            "tf.Tensor([b'class4,table17,obj2,atr2,NP'], shape=(1,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for example_context_strings, example_target_strings in train_raw.take(1):\n",
        "  print(example_context_strings[:5])\n",
        "  print()\n",
        "  print(example_target_strings[:5])\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCoxLcuN3bwv"
      },
      "source": [
        "### Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kwdPcHvzz_a"
      },
      "source": [
        "One of the goals of this tutorial is to build a model that can be exported as a `tf.saved_model`. To make that exported model useful it should take `tf.string` inputs, and return `tf.string` outputs: All the text processing happens inside the model. Mainly using a `layers.TextVectorization` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOQ5n55X4uDB"
      },
      "source": [
        "#### Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upKhKAMK4zzI"
      },
      "source": [
        "The model is dealing with multilingual text with a limited vocabulary. So it will be important to standardize the input text.\n",
        "\n",
        "The first step is Unicode normalization to split accented characters and replace compatibility characters with their ASCII equivalents.\n",
        "\n",
        "The `tensorflow_text` package contains a unicode normalize operation, We may or may not decide to Use this for ORM data. I kept it in the experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD0e-DWGQ2Vo",
        "outputId": "dd843360-39e2-40f6-cb26-71aae199f5dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'class1,table2,obj1,atr1'\n",
            "b'class1,table2,obj1,atr1'\n"
          ]
        }
      ],
      "source": [
        "#example_text = tf.constant('moduleOM_nameopenDeclarationonesigclass1_nameextendsClassattrSet=c1_at1+c1_at2id=c1_at1noparentisAbstract=No}onesigc1_at1extendsc1_at1_typeonesigc1_at2extendsc1_at2_typeonesigclass2_nameextendsClassattrSet=c2_at1+c2_at2+c2_at3+c2_at4id=c2_at1noparentisAbstract=No}onesigc2_at1extendsc2_at1_typeonesigc2_at2extendsc2_at2_typeonesigc2_at3extendsc2_at3_typeonesigc2_at4extendsc2_at4_typeonesigclass3_nameextendsClassattrSet=c3_at1+c3_at2+c3_at3+c3_at4id=c3_at1noparentisAbstract=No}onesigc3_at1extendsc3_at1_typeonesigc3_at2extendsc3_at2_typeonesigc3_at3extendsc3_at3_typeonesigc3_at4extendsc3_at4_typeonesigclass4_nameextendsClassattrSet=c4_at1id=c4_at1noparentisAbstract=No}onesigc4_at1extendsc4_at1_typeonesigclass5_nameextendsClassattrSet=c5_at1+c5_at2+c5_at3+c5_at4id=c5_at1noparentisAbstract=No}onesigc5_at1extendsc5_at1_typeonesigc5_at2extendsc5_at2_typeonesigc5_at3extendsc5_at3_typeonesigc5_at4extendsc5_at4_typeonesigclass6_nameextendsClassattrSet=c6_at1+c6_at2+c6_at3+c6_at4id=c6_at1noparentisAbstract=No}onesigc6_at1extendsc6_at1_typeonesigc6_at2extendsc6_at2_typeonesigc6_at3extendsc6_at3_typeonesigc6_at4extendsc6_at4_typeonesigassoc1extendsAssociationsrc=class1_namedst=class5_namesrc_multiplicity=src_mlpcdst_multiplicity=dst_mlpc}onesigassoc2extendsAssociationsrc=class1_namedst=class5_namesrc_multiplicity=src_mlpcdst_multiplicity=dst_mlpc}onesigassoc3extendsAssociationsrc=class4_namedst=class5_namesrc_multiplicity=src_mlpcdst_multiplicity=dst_mlpc}onesigassoc4extendsAssociationsrc=class1_namedst=class6_namesrc_multiplicity=src_mlpcdst_multiplicity=dst_mlpc2}onesigassoc5extendsAssociationsrc=class1_namedst=class3_namesrc_multiplicity=src_mlpcdst_multiplicity=dst_mlpc2}predshowrunshowfor38,​OM_name_Solution:0Table:class1_nameAttributec1_at1:c1_at1_typePrimaryKeyTable:class1_nameAttributec1_at2:c1_at2_typeTable:class2_nameAttributec2_at1:c1_at1_typePrimaryKeyTable:class2_nameAttributec2_at2:c2_at2_typeTable:class2_nameAttributec2_at3:c2_at3_typeTable:class3_nameAttributec3_at1:c3_at1_typePrimaryKeyTable:class3_nameAttributec3_at4:c3_at4_typeTable:class3_nameAttributec3_at2:c3_at2_typeTable:class4_nameAttributec4_at1:c4_at1_typePrimaryKeyTable:class5_nameAttributec5_at3:c5_at3_typeTable:class5_nameAttributec5_at4:c5_at4_typeTable:class6_nameAttributec6_at1:c6_at1_typePrimaryKeyTable:class6_nameAttributec6_at2:c6_at2_typeTable:class6_nameAttributec6_at3:c6_at3_typeTable:class6_nameAttributec6_at4:c6_at4_typeTable:class1_nameAttributec1_at1:c1_at1_typePrimaryKeyTable:class2_nameAttributec2_at1:c1_at1_typePrimaryKeyTable:class3_nameAttributec3_at1:c3_at1_typePrimaryKeyTable:class4_nameAttributec4_at1:c4_at1_typePrimaryKeyTable:class6_nameAttributec6_at1:c6_at1_typePrimaryKeyTableName:class1_nameTableName:class2_nameTableName:class3_nameTableName:class4_nameTableName:class5_nameTableName:class6_nameMappingStrategyofTableclass1_name:map_str2MappingStrategyofTableclass2_name:map_str2MappingStrategyofTableclass3_name:map_str2MappingStrategyofTableclass4_name:map_str2MappingStrategyofTableclass6_name:map_str2AssociationStrategyforassoc1:assoc_type1AssociationStrategyforassoc2:assoc_type1AssociationStrategyforassoc3:assoc_type2AssociationStrategyforassoc4:assoc_type2AssociationStrategyforassoc5:assoc_type2,USEOM_name0----CREATETABLE`assoc1`(`c5_at1`c5_at1_type`c1_at1`c1_at1_typeKEY`FK_assoc1_c5_at1_idx`(`c5_at1`)KEY`FK_assoc1_c1_at1_idx`(`c1_at1`)PRIMARYKEY(`c5_at1``c1_at1`));----CREATETABLE`assoc3``c5_at1`c5_at1_type`c4_at1`c4_at1_typeKEY`FK_assoc3_c5_at1_idx`(`c5_at1`)KEY`FK_assoc3_c4_at1_idx`(`c4_at1`)PRIMARYKEY(`c5_at1``c4_at1`));----CREATETABLE`class5_name`(`c5_at4`c5_at4_type(64)`c5_at3`c5_at3_type(64)`c1_at1`c1_at1_type`c5_at1`c5_at1_typePRIMARYKEY(`c5_at1`));----CREATETABLE`class3_name`(`c3_at3`c3_at3_type(64)`c3_at2`c3_at2_type(64)`c3_at4`c3_at4_type`c3_at1`c3_at1_typeNOTNULLPRIMARYKEY(`c3_at1`));----CREATETABLE`class2_name`(`c2_at3`c2_at3_type(64)`c2_at2`c2_at2_type(64)`c2_at4`c2_at4_type`c2_at1`c2_at1_typePRIMARYKEY(`c2_at1`));----CREATETABLE`class4_name`(`c4_at1`c4_at1_typePRIMARYKEY(`c4_at1`));----CREATETABLE`class1_name`(`c1_at2`c1_at2_type(64)`c1_at1`c1_at1_typePRIMARYKEY(`c1_at1`));----CREATETABLE`class6_name`(`c6_at4`c6_at4_type`c6_at3`c6_at3_type`c6_at2`c6_at2_type`c6_at1`c6_at1_typeNOTNULLPRIMARYKEY(`c6_at1`));----CREATETABLE`assoc2`(`c5_at1`c5_at1_type`c2_at1`c2_at1_typeKEY`FK_assoc2_c5_at1_idx`(`c5_at1`)KEY`FK_assoc2_c2_at1_idx`(`c2_at1`)PRIMARYKEY(`c5_at1``c2_at1`));----CREATETABLE`assoc5`(`c3_at1`c3_at1_typeNOTNULL`c2_at1`c2_at1_typeKEY`FK_assoc5_c3_at1_idx`(`c3_at1`)KEY`FK_assoc5_c2_at1_idx`(`c2_at1`)PRIMARYKEY(`c3_at1``c2_at1`));----CREATETABLE`assoc4`(`c6_at1`c6_at1_typeNOTNULL`c2_at1`c2_at1_typeKEY`FK_assoc4_c6_at1_idx`(`c6_at1`)KEY`FK_assoc4_c2_at1_idx`(`c2_at1`)PRIMARYKEY(`c6_at1``c2_at1`));ALTERTABLE`assoc1`ADDCONSTRAINT`FK_assoc1_c5_at1`FOREIGNKEY(`c5_at1`)REFERENCES`class5_name`(`c5_at1`)ONDELETECASCADEONUPDATECASCADEADDCONSTRAINT`FK_assoc1_c1_at1`FOREIGNKEY(`c1_at1`)REFERENCES`class1_name`(`c1_at1`)ONDELETECASCADEONUPDATECASCADE;ALTERTABLE`assoc3`ADDCONSTRAINT`FK_assoc3_c5_at1`FOREIGNKEY(`c5_at1`)REFERENCES`class5_name`(`c5_at1`)ONDELETECASCADEONUPDATECASCADEADDCONSTRAINT`FK_assoc3_c4_at1`FOREIGNKEY(`c4_at1`)REFERENCES`class4_name`(`c4_at1`)ONDELETECASCADEONUPDATECASCADE;ALTERTABLE`assoc2`ADDCONSTRAINT`FK_assoc2_c5_at1`FOREIGNKEY(`c5_at1`)REFERENCES`class5_name`(`c5_at1`)ONDELETECASCADEONUPDATECASCADEADDCONSTRAINT`FK_assoc2_c2_at1`FOREIGNKEY(`c2_at1`)REFERENCES`class2_name`(`c2_at1`)ONDELETECASCADEONUPDATECASCADEALTERTABLE`assoc5`ADDCONSTRAINT`FK_assoc5_c3_at1`FOREIGNKEY(`c3_at1`)REFERENCES`class3_name`(`c3_at1`)ONDELETECASCADEONUPDATECASCADEADDCONSTRAINT`FK_assoc5_c2_at1`FOREIGNKEY(`c2_at1`)REFERENCES`class2_name`(`c2_at1`)ONDELETECASCADEONUPDATECASCADE;ALTERTABLE`assoc4`ADDCONSTRAINT`FK_assoc4_c6_at1`FOREIGNKEY(`c6_at1`)REFERENCES`class6_name`(`c6_at1`)ONDELETECASCADEONUPDATECASCADEADDCONSTRAINT`FK_assoc4_c2_at1`FOREIGNKEY(`c2_at1`)REFERENCES`class2_name`(`c2_at1`)ONDELETECASCADEONUPDATECASCADE')\n",
        "\n",
        "example_text = tf.constant('class1,table2,obj1,atr1')\n",
        "print(example_text.numpy())\n",
        "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "chTF5N885F0P"
      },
      "outputs": [],
      "source": [
        "#import re\n",
        "\n",
        "#def tf_lower_and_split_punct(text):\n",
        "  # Split accented characters.\n",
        "  #text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  #text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  #pattern = '\\s+'\n",
        "  #re.split(pattern, text, maxsplit=2)\n",
        "  #text = tf.strings.regex_replace(text, '\\s+', '')\n",
        "  #tf.strings.split(text, sep=', ', maxsplit=2, name=None)\n",
        "  #tf.strings.split (text, sep='\\s+', maxsplit=2, name=None)\n",
        "  #text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  #tf.strings.split(text, ',')\n",
        "  #text = tf.strings.split(text, sep=None, maxsplit=-1, name=None)\n",
        "  #text.tf.strings.split(', ')\n",
        "\n",
        "  # Add spaces around punctuation.\n",
        "  #text = tf.strings.regex_replace(text, '', r'')\n",
        "  # Strip whitespace.\n",
        "  #text = tf.strings.strip(text)\n",
        "\n",
        "  #text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  #return text\n",
        "\n",
        "\n",
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '', r'\\0')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UREvDg3sEKYa",
        "outputId": "3fc9cc77-5d19-45d1-ab69-26c8358512c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class1,table2,obj1,atr1\n",
            "[START] class1,table2,obj1,atr1 [END]\n"
          ]
        }
      ],
      "source": [
        "print(example_text.numpy().decode())\n",
        "print(tf_lower_and_split_punct(example_text).numpy().decode())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q-sKsSI7xRZ"
      },
      "source": [
        "#### Text Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aKn8qd37abi"
      },
      "source": [
        "This standardization function will be wrapped up in a `tf.keras.layers.TextVectorization` layer which will handle the vocabulary extraction and conversion of input text to sequences of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eAY9k49G3jE_"
      },
      "outputs": [],
      "source": [
        "max_vocab_size = 5000\n",
        "\n",
        "context_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size,\n",
        "    ragged=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kbC6ODP8IK_"
      },
      "source": [
        "The `TextVectorization` layer and many other [Keras preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) have an `adapt` method. This method reads one epoch of the training data, and works a lot like `Model.fit`. This `adapt` method initializes the layer based on the data. Here it determines the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmsI1Yql8FYe",
        "outputId": "23cad99c-e689-4c80-d2ea-676d24f81d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " '[START]',\n",
              " '[END]',\n",
              " 'class6,table9,obj1,atr1',\n",
              " 'class6,table7,obj1,atr1',\n",
              " 'class6,table6,obj1,atr1',\n",
              " 'class6,table5,obj1,atr1',\n",
              " 'class6,table3,obj1,atr1',\n",
              " 'class6,table2,obj1,atr1']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "context_text_processor.adapt(train_raw.map(lambda context, target: context))\n",
        "\n",
        "# Here are the first 10 words from the vocabulary:\n",
        "context_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kGjIFjX8_Wp"
      },
      "source": [
        "That's the context data  `TextVectorization` layer, now build and `.adapt()` for the Target Data one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlC4xuZnKLBS",
        "outputId": "a54292da-2e01-4fb7-9bd7-73360e1dc6a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " '[START]',\n",
              " '[END]',\n",
              " 'class6,table9,obj1,atr1,p',\n",
              " 'class6,table7,obj1,atr1,p',\n",
              " 'class6,table6,obj1,atr1,p',\n",
              " 'class6,table5,obj1,atr1,p',\n",
              " 'class6,table3,obj1,atr1,p',\n",
              " 'class6,table2,obj1,atr1,p']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "target_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size,\n",
        "    ragged=True)\n",
        "\n",
        "target_text_processor.adapt(train_raw.map(lambda context, target: target))\n",
        "target_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWQqlP_s9eIv"
      },
      "source": [
        "Now these layers can convert a batch of strings into a batch of token IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KZxj8IrNZ9S",
        "outputId": "a0f5ada0-eda2-4a7e-ba32-a0bbe8a30e65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[2, 40, 3]]>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "example_tokens = context_text_processor(example_context_strings)\n",
        "example_tokens[:3, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA9rUn9G9n78"
      },
      "source": [
        "The `get_vocabulary` method can be used to convert token IDs back to text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "98g9rcxGQY0I",
        "outputId": "3480201d-7aa1-4a7e-d764-cb15356c39fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[START] class4,table17,obj2,atr2 [END]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
        "tokens = context_vocab[example_tokens[0].numpy()]\n",
        "' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot0aCL9t-Ghi"
      },
      "source": [
        "The returned token IDs are zero-padded. This can easily be turned into a mask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "_jx4Or_eFRSz",
        "outputId": "33cfb732-1590-4ac1-9580-b7f496514437"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mask')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARG0lEQVR4nO3de/BndV3H8eer5aJyTfGCu6syCeYm5gUXJ2cUbwVaYDeDzEuhO10oS3OidFCpaTIbdRwp25JMVAjRabbaBjVRqhHb9YbCim54YcEJBRQQlUXe/fE963z9ufD77u753d48HzPfme855/M7531++/6+fuf3+f7Od1NVSJJ6+ZGlLkCSND7DXZIaMtwlqSHDXZIaMtwlqSHDXZIaMtwXUZITkuxY6jqklSTJh5O8eKnrWGkM972U5Napx51Jvj21/Lwlru37L4bhB8qdU7XtSHJhkicsZY3qJcmXktye5Ig56z+ZpJI8bGkqu+cy3PdSVR286wF8Bfi5qXXvWur65rhuqPMQ4InA54D/TPL0pS1LzXwROG3XQpJjgfssXTn3bIb7yJIcmORNSa4bHm9KcuBdjP29JFcmWTN83V8l+UqS/0vy1iT3HsadMFxxvzzJ9Um+muTX97S2mthRVWcBfw+8bth/krxx2PfNST6T5FH78n3QPdJ5wAumll8IvGPXQpJnD1fyNye5JslrprbdK8k7k9yQ5BtJtiR54NwDJDkyyeVJXrGQJ9KB4T6+VzK5On4M8JPAeuBVcwclOQt4EfCUqtoB/AVwzPB1DwdWA2dNfcmDgMOG9acD5yT50X2o833A45IcBPw08OTh+IcBzwVu2Id9657pMuDQJI9Msgo4FXjn1PZvMQn/w4FnA7+V5DnDthcy6b21wP2A3wS+Pb3zJEcBHwHeUlWvX8gT6cBwH9/zgLOr6vqq+hrwWuD5U9uT5A1MAvWpVfW1JAE2AH9QVTdW1S3AnzN5ceyyc9jvzqraDNwKPGIf6rwOCJMX2k4mUzY/DqSqtlXVV/dh37rn2nX1/kxgG3Dtrg1V9eGq+kxV3VlVlwPnA08ZNu9kEuoPr6rvVdXHq+rmqf2uAy4BXl1VGxfjRFa6/Za6gIYeDHx5avnLw7pdDmcS5L9SVd8c1t2fydzkxyc5D0yCd9XU191QVXdMLd8GHLwPda4GCvhGVX0oyVuAc4CHJnkf8IdzXlzSLM4DLgWOYmpKBiDJ8Ux+Q30UcABwIPCeqa9bC1yQ5HAmV/yvrKqdw/bnAduBixb6BLrwyn181wEPnVp+yLBul5uAnwX+IcmThnVfZ/Ir6E9U1eHD47DhTdCF8vPAJ6rqWwBV9eaqejyTK6RjAOc0tceq6stM3lh9FpOpv2nvBjYBa6vqMOCtTC5iGH4jfW1VrQN+islrZHr+/jVMXifvHqZ8NA/DfXznA69Kcv/hz8LO4gfnHamqDzO5EnlfkvVVdSfwd8AbkzwAIMnqJD8zZmHDG6erk7waeDHwJ8P6JyQ5Psn+TOZFvwPcOeaxdY9yOvC0XRcOUw4Bbqyq7yRZD/zqrg1Jnprk2CG4b2YyTTPdgzuBXwYOAt6RxOyah9+g8f0ZsBW4HPgM8Ilh3Q+oqg8AvwH8S5LHAX/E5NfOy5LcDHyQfZtTn/bgJLcymaffAhwLnFBV7x+2H8rkh8tNTKaRbgB8w0p7par+t6q27mbTbwNnJ7mFyUXPhVPbHsRkyuVmJnP1H2EyVTO939uBXwAeCJxrwN+9+J91SFI//uSTpIbmDfck5w43t3z2LrYnyZuTbB9uLnjc+GVK47O31dksV+5vB068m+0nAUcPjw3A3+x7WdKieDv2tpqaN9yr6lLgxrsZcgrwjuHW9suAw5McOVaB0kKxt9XZGDcxrQaumVreMaz7oTsck2xgcgXEKlY9/j4cOsLhl9Yxj75tqUsYzRc+e9BSlzCam++88etVdf993M09ure1PN3CTTP19qLeoTrcNrwR4NDct45v8KGEF1/86aUuYTQn/dgTl7qE0bz/tvO+PP+o8XTsbS1PH6yLZurtMf5a5lomtw3vsoapz5OQVjB7WyvWGOG+CXjB8JcFTwS+6YdOqQl7WyvWvNMySc4HTgCOyOS/iHs1sD9AVb0V2MzkcyS2M/kwqz3+nHFpKdjb6mzecK+q0+bZXsDvjFaRtEjsbXXmHaqS1JDhLkkNGe6S1JDhLkkNGe6S1JDhLkkNGe6S1JDhLkkNGe6S1JDhLkkNGe6S1JDhLkkNGe6S1JDhLkkNGe6S1JDhLkkNGe6S1JDhLkkNGe6S1JDhLkkNGe6S1JDhLkkNGe6S1JDhLkkNGe6S1JDhLkkNGe6S1JDhLkkNGe6S1JDhLkkNGe6S1JDhLkkNGe6S1JDhLkkNGe6S1NBM4Z7kxCRXJdme5MzdbH9IkkuSfDLJ5UmeNX6p0vjsbXU1b7gnWQWcA5wErANOS7JuzrBXARdW1WOBU4G/HrtQaWz2tjqb5cp9PbC9qq6uqtuBC4BT5owp4NDh+WHAdeOVKC0Ye1tt7TfDmNXANVPLO4Dj54x5DfD+JL8LHAQ8Y3c7SrIB2ABwL+6zp7VKY7O31dZYb6ieBry9qtYAzwLOS/JD+66qjVV1XFUdtz8HjnRoaUHZ21qRZgn3a4G1U8trhnXTTgcuBKiqjwL3Ao4Yo0BpAdnbamuWcN8CHJ3kqCQHMHlTadOcMV8Bng6Q5JFMXgBfG7NQaQHY22pr3nCvqjuAM4CLgW1M/nLgiiRnJzl5GPZy4CVJPg2cD7yoqmqhipbGYG+rs1neUKWqNgOb56w7a+r5lcCTxi1NWnj2trryDlVJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGZgr3JCcmuSrJ9iRn3sWY5ya5MskVSd49bpnS+OxrdbbffAOSrALOAZ4J7AC2JNlUVVdOjTka+GPgSVV1U5IHLFTB0hjsa3U3y5X7emB7VV1dVbcDFwCnzBnzEuCcqroJoKquH7dMaXT2tVqbJdxXA9dMLe8Y1k07BjgmyX8nuSzJibvbUZINSbYm2bqT7+5dxdI4RutrsLe1/Mw7LbMH+zkaOAFYA1ya5Niq+sb0oKraCGwEODT3rZGOLS2Umfoa7G0tP7NcuV8LrJ1aXjOsm7YD2FRVO6vqi8DnmbwopOXKvlZrs4T7FuDoJEclOQA4Fdg0Z8w/M7m6IckRTH6dvXrEOqWx2ddqbd5wr6o7gDOAi4FtwIVVdUWSs5OcPAy7GLghyZXAJcArquqGhSpa2lf2tbqbac69qjYDm+esO2vqeQEvGx7SimBfqzPvUJWkhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhmYK9yQnJrkqyfYkZ97NuF9MUkmOG69EaeHY2+pq3nBPsgo4BzgJWAeclmTdbsYdArwU+NjYRUoLwd5WZ7Ncua8HtlfV1VV1O3ABcMpuxv0p8DrgOyPWJy0ke1ttzRLuq4FrppZ3DOu+L8njgLVV9W93t6MkG5JsTbJ1J9/d42Klkdnbamu/fd1Bkh8B3gC8aL6xVbUR2AhwaO5b+3psaSHZ21rJZrlyvxZYO7W8Zli3yyHAo4APJ/kS8ERgk288aQWwt9XWLOG+BTg6yVFJDgBOBTbt2lhV36yqI6rqYVX1MOAy4OSq2rogFUvjsbfV1rzhXlV3AGcAFwPbgAur6ookZyc5eaELlBaKva3OZppzr6rNwOY56866i7En7HtZ0uKwt9WVd6hKUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1NFO4JzkxyVVJtic5czfbX5bkyiSXJ/mPJA8dv1RpXPa1Ops33JOsAs4BTgLWAaclWTdn2CeB46rq0cBFwF+OXag0Jvta3c1y5b4e2F5VV1fV7cAFwCnTA6rqkqq6bVi8DFgzbpnS6OxrtTZLuK8Grpla3jGsuyunA/++uw1JNiTZmmTrTr47e5XS+Ebra7C3tfzsN+bOkvwacBzwlN1tr6qNwEaAQ3PfGvPY0kKZr6/B3tbyM0u4XwusnVpeM6z7AUmeAbwSeEpVeemi5c6+VmuzTMtsAY5OclSSA4BTgU3TA5I8Fvhb4OSqun78MqXR2ddqbd5wr6o7gDOAi4FtwIVVdUWSs5OcPAx7PXAw8J4kn0qy6S52Jy0L9rW6m2nOvao2A5vnrDtr6vkzRq5LWnD2tTrzDlVJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJamimcE9yYpKrkmxPcuZuth+Y5J+G7R9L8rCxC5UWgr2truYN9ySrgHOAk4B1wGlJ1s0ZdjpwU1U9HHgj8LqxC5XGZm+rs1mu3NcD26vq6qq6HbgAOGXOmFOAfxyeXwQ8PUnGK1NaEPa22tpvhjGrgWumlncAx9/VmKq6I8k3gfsBX58elGQDsGFY/O4H66LP7k3Ry8mqIwE4gjnnujJ9AdqcC4+YYYy9ffe69AL0OpdZenumcB9NVW0ENgIk2VpVxy3m8ReK57L8JNm6mMfr2NtdzgP6ncss42aZlrkWWDu1vGZYt9sxSfYDDgNumKUAaQnZ22prlnDfAhyd5KgkBwCnApvmjNkEvHB4/kvAh6qqxitTWhD2ttqad1pmmGc8A7gYWAWcW1VXJDkb2FpVm4C3Aecl2Q7cyORFMp+N+1D3cuO5LD/znoe9Pa8u5wH3wHOJFyGS1I93qEpSQ4a7JDW0JOE+3y3fK0WSc5Ncn2RF/01zkrVJLklyZZIrkrx0qWvaW0nuleR/knx6OJfXLuKx7etlpktv701fL/qc+3DL9+eBZzK5aWQLcFpVXbmohYwgyZOBW4F3VNWjlrqevZXkSODIqvpEkkOAjwPPWaH/JgEOqqpbk+wP/Bfw0qq6bIGPa18vQ116e2/6eimu3Ge55XtFqKpLmfwFxYpWVV+tqk8Mz28BtjG5M3PFqYlbh8X9h8diXMHY18tQl97em75einDf3S3fK+6b3dXwqYePBT62tJXsvSSrknwKuB74QFUtxrnY18vcSu/tPe1r31DV9yU5GHgv8PtVdfNS17O3qup7VfUYJnecrk+yoqcWtO869Pae9vVShPsst3xrkQ3zeO8F3lVV71vqesZQVd8ALgFOXITD2dfLVLfenrWvlyLcZ7nlW4toeLPmbcC2qnrDUtezL5LcP8nhw/N7M3mD83OLcGj7ehnq0tt709eLHu5VdQew65bvbcCFVXXFYtcxhiTnAx8FHpFkR5LTl7qmvfQk4PnA05J8ang8a6mL2ktHApckuZxJ4H6gqv51oQ9qXy9bXXp7j/vajx+QpIZ8Q1WSGjLcJakhw12SGjLcJakhw12SGjLcJakhw12SGvp/arqZm1Cfk9UAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(example_tokens.to_tensor())\n",
        "plt.title('Token IDs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens.to_tensor() != 0)\n",
        "plt.title('Mask')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O0B4XdFlRgc"
      },
      "source": [
        "### Process the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVCuyuSp_whd"
      },
      "source": [
        "The `process_text` function below converts the `Datasets` of strings, into  0-padded tensors of token IDs. It also converts from a `(context, target)` pair to an `((context, target_in), target_out)` pair for training with `keras.Model.fit`. Keras expects `(inputs, labels)` pairs, the inputs are the `(context, target_in)` and the labels are `target_out`. The difference between `target_in` and `target_out` is that they are shifted by one step relative to eachother, so that at each location the label is the next token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wk5tbZWQl5u1"
      },
      "outputs": [],
      "source": [
        "def process_text(context, target):\n",
        "  context = context_text_processor(context).to_tensor()\n",
        "  target = target_text_processor(target)\n",
        "  targ_in = target[:,:-1].to_tensor()\n",
        "  targ_out = target[:,1:].to_tensor()\n",
        "  return (context, targ_in), targ_out\n",
        "\n",
        "\n",
        "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iGi7X2m_tbM"
      },
      "source": [
        "Here is the first sequence of each, from the first batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woQBWAjLsJkr",
        "outputId": "3b5ad24c-ae64-4436-d66b-8bfe6f8f7a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2 22  3]\n",
            "\n",
            "[ 2 22]\n",
            "[22  3]\n"
          ]
        }
      ],
      "source": [
        "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
        "  print(ex_context_tok[0, :10].numpy()) \n",
        "  print()\n",
        "  print(ex_tar_in[0, :10].numpy()) \n",
        "  print(ex_tar_out[0, :10].numpy()) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNfHIF71ulLu"
      },
      "source": [
        "## The encoder/decoder\n",
        "\n",
        "  <th colspan=1>This tutorial's model</th>\n",
        "<tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzQWx2saImMV"
      },
      "source": [
        "Before getting into it define constants for the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_a9uNz3-IrF-"
      },
      "outputs": [],
      "source": [
        "UNITS = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blNgVbLSzpsr"
      },
      "source": [
        "### The encoder\n",
        "\n",
        "\n",
        "The encoder:\n",
        "\n",
        "1. Takes a list of token IDs (from `context_text_processor`).\n",
        "3. Looks up an embedding vector for each token (Using a `layers.Embedding`).\n",
        "4. Processes the embeddings into a new sequence (Using a bidirectional `layers.GRU`).\n",
        "5. Returns the processed sequence. This will be passed to the attention head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.units = units\n",
        "    \n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,\n",
        "                                               mask_zero=True)\n",
        "\n",
        "    # The RNN layer processes those vectors sequentially.\n",
        "    self.rnn = tf.keras.layers.Bidirectional(\n",
        "        merge_mode='sum',\n",
        "        layer=tf.keras.layers.GRU(units,\n",
        "                            # Return the sequence and state\n",
        "                            return_sequences=True,\n",
        "                            recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "  def call(self, x):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(x, 'batch s')\n",
        "\n",
        "    # 2. The embedding layer looks up the embedding vector for each token.\n",
        "    x = self.embedding(x)\n",
        "    shape_checker(x, 'batch s units')\n",
        "\n",
        "    # 3. The GRU processes the sequence of embeddings.\n",
        "    x = self.rnn(x)\n",
        "    shape_checker(x, 'batch s units')\n",
        "\n",
        "    # 4. Returns the new sequence of embeddings.\n",
        "    return x\n",
        "\n",
        "  def convert_input(self, texts):\n",
        "    texts = tf.convert_to_tensor(texts)\n",
        "    if len(texts.shape) == 0:\n",
        "      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "    context = self.text_processor(texts).to_tensor()\n",
        "    context = self(context)\n",
        "    return context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60gSVh05Jl6l",
        "outputId": "81325f96-7c8a-4312-c10a-aab7f5017781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context tokens, shape (batch, s): (1, 3)\n",
            "Encoder output, shape (batch, s, units): (1, 3, 256)\n"
          ]
        }
      ],
      "source": [
        "# Encode the input sequence.\n",
        "encoder = Encoder(context_text_processor, UNITS)\n",
        "ex_context = encoder(ex_context_tok)\n",
        "\n",
        "print(f'Context tokens, shape (batch, s): {ex_context_tok.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45xM_Gl1MgXY"
      },
      "source": [
        "### The attention layer\n",
        "\n",
        "The attention layer lets the decoder access the information extracted by the encoder. It computes a vector from the entire context sequence, and adds that to the decoder's output. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-Ql3ymqwD8LS"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, x, context):\n",
        "    shape_checker = ShapeChecker()\n",
        " \n",
        "    shape_checker(x, 'batch t units')\n",
        "    shape_checker(context, 'batch s units')\n",
        "\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "    \n",
        "    shape_checker(x, 'batch t units')\n",
        "    shape_checker(attn_scores, 'batch heads t s')\n",
        "    \n",
        "    # Cache the attention scores for plotting later.\n",
        "    attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
        "    shape_checker(attn_scores, 'batch t s')\n",
        "    self.last_attention_weights = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y7hjPkNMmHh",
        "outputId": "b28b461a-e318-42fc-b232-562d531dbf1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context sequence, shape (batch, s, units): (1, 3, 256)\n",
            "Target sequence, shape (batch, t, units): (1, 2, 256)\n",
            "Attention result, shape (batch, t, units): (1, 2, 256)\n",
            "Attention weights, shape (batch, t, s):    (1, 2, 3)\n"
          ]
        }
      ],
      "source": [
        "attention_layer = CrossAttention(UNITS)\n",
        "\n",
        "# Attend to the encoded tokens\n",
        "embed = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(),\n",
        "                                  output_dim=UNITS, mask_zero=True)\n",
        "ex_tar_embed = embed(ex_tar_in)\n",
        "\n",
        "result = attention_layer(ex_tar_embed, ex_context)\n",
        "\n",
        "print(f'Context sequence, shape (batch, s, units): {ex_context.shape}')\n",
        "print(f'Target sequence, shape (batch, t, units): {ex_tar_embed.shape}')\n",
        "print(f'Attention result, shape (batch, t, units): {result.shape}')\n",
        "print(f'Attention weights, shape (batch, t, s):    {attention_layer.last_attention_weights.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx9fUhi3Pmwp"
      },
      "source": [
        "The attention weights will sum to `1` over the context sequence, at each location in the target sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxyR7cmQPn9P",
        "outputId": "31f327b2-7078-4abd-8b60-28f745cd4f49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "attention_layer.last_attention_weights[0].numpy().sum(axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AagyXMH-Jhqt"
      },
      "source": [
        "\n",
        "\n",
        "Here are the attention weights across the context sequences at `t=0`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "Rqr8XGsAJlf6",
        "outputId": "40e94156-47f6-40a7-e405-df412f158dad"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "attention_weights = attention_layer.last_attention_weights\n",
        "mask=(ex_context_tok != 0).numpy()\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(mask*attention_weights[:, 0, :])\n",
        "plt.title('Attention weights')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(mask)\n",
        "plt.title('Mask');\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Eil-C_NN1rp"
      },
      "source": [
        "Because of the small-random initialization the attention weights are initially all close to `1/(sequence_length)`. The model will learn to make these less uniform as training progresses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ638eHN4iCK"
      },
      "source": [
        "### The decoder\n",
        "\n",
        "The decoder's job is to generate predictions for the next token at each location in the target sequence.\n",
        "\n",
        "1. It looks up embeddings for each token in the target sequence.\n",
        "2. It uses an RNN to process the target sequence, and keep track of what it has generated so far.\n",
        "3. It uses RNN output as the \"query\" to the attention layer, when attending to the encoder's output.\n",
        "4. At each location in the output it predicts the next token.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZsQJMqNmg_L"
      },
      "source": [
        "Here is the `Decoder` class' initializer. The initializer creates all the necessary layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "erYvHIgAl8kh"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.word_to_id = tf.keras.layers.StringLookup(\n",
        "        vocabulary=text_processor.get_vocabulary(),\n",
        "        mask_token='', oov_token='[UNK]')\n",
        "    self.id_to_word = tf.keras.layers.StringLookup(\n",
        "        vocabulary=text_processor.get_vocabulary(),\n",
        "        mask_token='', oov_token='[UNK]',\n",
        "        invert=True)\n",
        "    self.start_token = self.word_to_id('[START]')\n",
        "    self.end_token = self.word_to_id('[END]')\n",
        "\n",
        "    self.units = units\n",
        "\n",
        "\n",
        "    # 1. The embedding layer converts token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
        "                                               units, mask_zero=True)\n",
        "\n",
        "    # 2. The RNN keeps track of what's been generated so far.\n",
        "    self.rnn = tf.keras.layers.GRU(units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # 3. The RNN output will be the query for the attention layer.\n",
        "    self.attention = CrossAttention(units)\n",
        "\n",
        "    # 4. This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.output_layer = tf.keras.layers.Dense(self.vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd8-nRNzFR8x"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPnaw583CpnY"
      },
      "source": [
        "Next, the `call` method, takes 3 arguments:\n",
        "\n",
        "* `inputs` -  a `context, x` pair where:\n",
        "  * `context` - is the context from the encoder's output.\n",
        "  * `x` - is the target sequence input.\n",
        "* `state` - Optional, the previous `state` output from the decoder (the internal state of the decoder's RNN). Pass the state from a previous run to continue generating text where you left off.\n",
        "* `return_state` - [Default: False] - Set this to `True` to return the RNN state. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "PJOi5btHAPNK"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def call(self,\n",
        "         context, x,\n",
        "         state=None,\n",
        "         return_state=False):  \n",
        "  shape_checker = ShapeChecker()\n",
        "  shape_checker(x, 'batch t')\n",
        "  shape_checker(context, 'batch s units')\n",
        "\n",
        "  # 1. Lookup the embeddings\n",
        "  x = self.embedding(x)\n",
        "  shape_checker(x, 'batch t units')\n",
        "\n",
        "  # 2. Process the target sequence.\n",
        "  x, state = self.rnn(x, initial_state=state)\n",
        "  shape_checker(x, 'batch t units')\n",
        "\n",
        "  # 3. Use the RNN output as the query for the attention over the context.\n",
        "  x = self.attention(x, context)\n",
        "  self.last_attention_weights = self.attention.last_attention_weights\n",
        "  shape_checker(x, 'batch t units')\n",
        "  shape_checker(self.last_attention_weights, 'batch t s')\n",
        "\n",
        "  # Step 4. Generate logit predictions for the next token.\n",
        "  logits = self.output_layer(x)\n",
        "  shape_checker(logits, 'batch t target_vocab_size')\n",
        "\n",
        "  if return_state:\n",
        "    return logits, state\n",
        "  else:\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1-mLAcUEXpK"
      },
      "source": [
        "That will be sufficient for training. Create an instance of the decoder to test out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4ZUMbYXIEVeA"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(target_text_processor, UNITS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFWaI4wqzt4t"
      },
      "source": [
        "Decoder usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YM-lD7bzx18",
        "outputId": "6ba202e9-2f3e-42e8-b3f0-5213e39c02b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder output shape: (batch, s, units) (1, 3, 256)\n",
            "input target tokens shape: (batch, t) (1, 2)\n",
            "logits shape shape: (batch, target_vocabulary_size) (1, 2, 95)\n"
          ]
        }
      ],
      "source": [
        "logits = decoder(ex_context, ex_tar_in)\n",
        "\n",
        "print(f'encoder output shape: (batch, s, units) {ex_context.shape}')\n",
        "print(f'input target tokens shape: (batch, t) {ex_tar_in.shape}')\n",
        "print(f'logits shape shape: (batch, target_vocabulary_size) {logits.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhS_tbk7VQkX"
      },
      "source": [
        "#### Inference\n",
        "\n",
        "For inference usage couple more methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "SPm12cnIVRQr"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def get_initial_state(self, context):\n",
        "  batch_size = tf.shape(context)[0]\n",
        "  start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "  embedded = self.embedding(start_tokens)\n",
        "  return start_tokens, done, self.rnn.get_initial_state(embedded)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "TzeOhpBvVS5L"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def tokens_to_text(self, tokens):\n",
        "  words = self.id_to_word(tokens)\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
        "  result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "v6ildnz_V1MA"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
        "  logits, state = self(\n",
        "    context, next_token,\n",
        "    state = state,\n",
        "    return_state=True) \n",
        "  \n",
        "  if temperature == 0.0:\n",
        "    next_token = tf.argmax(logits, axis=-1)\n",
        "  else:\n",
        "    logits = logits[:, -1, :]/temperature\n",
        "    next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "  # If a sequence produces an `end_token`, set it `done`\n",
        "  done = done | (next_token == self.end_token)\n",
        "  # Once a sequence is done it only produces 0-padding.\n",
        "  next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
        "  \n",
        "  return next_token, done, state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WiXLrVs-FTE"
      },
      "source": [
        "With those extra functions, you can write a generation loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuehagxL-JBZ",
        "outputId": "2f71773b-f0f6-49f9-a91c-1c4f9981baec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'class4,table7,obj2,atr2,np class2,table18,obj2,atr2,np class2,table1,obj2,atr2,np class4,table14,obj2,atr2,np class6,table3,obj1,atr1,p class5,table16,obj1,atr1,p class1,table1,obj18,atr1,p class3,table6,obj1,atr1,p class2,table1,obj2,atr2,np class2,table16,obj2,atr2,np'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# Setup the loop variables.\n",
        "next_token, done, state = decoder.get_initial_state(ex_context)\n",
        "tokens = []\n",
        "\n",
        "for n in range(10):\n",
        "  # Run one step.\n",
        "  next_token, done, state = decoder.get_next_token(\n",
        "      ex_context, next_token, done, state, temperature=1.0)\n",
        "  # Add the token to the output.\n",
        "  tokens.append(next_token)\n",
        "\n",
        "# Stack all the tokens together.\n",
        "tokens = tf.concat(tokens, axis=-1) # (batch, t)\n",
        "\n",
        "# Convert the tokens back to a a string\n",
        "result = decoder.tokens_to_text(tokens)\n",
        "result[:3].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ALTdqCMLGSY"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "**Since the model's untrained, it outputs items from the vocabulary almost uniformly at random. **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6xyru86m914"
      },
      "source": [
        "## The model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "WWIyuy71TkJT"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, units,\n",
        "               context_text_processor,\n",
        "               target_text_processor):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(context_text_processor, units)\n",
        "    decoder = Decoder(target_text_processor, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def call(self, inputs):\n",
        "    context, x = inputs\n",
        "    context = self.encoder(context)\n",
        "    logits = self.decoder(context, x)\n",
        "\n",
        "    #TODO(b/250038731): remove this\n",
        "    try:\n",
        "      # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rPi0FkS2iA5"
      },
      "source": [
        "During training the model will be used like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vhjTh84K6Mg",
        "outputId": "f354c5c4-968c-4187-aa54-0597cefeb13c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context tokens, shape: (batch, s, units) (1, 3)\n",
            "Target tokens, shape: (batch, t) (1, 2)\n",
            "logits, shape: (batch, t, target_vocabulary_size) (1, 2, 95)\n"
          ]
        }
      ],
      "source": [
        "model = Translator(UNITS, context_text_processor, target_text_processor)\n",
        "\n",
        "logits = model((ex_context_tok, ex_tar_in))\n",
        "\n",
        "print(f'Context tokens, shape: (batch, s, units) {ex_context_tok.shape}')\n",
        "print(f'Target tokens, shape: (batch, t) {ex_tar_in.shape}')\n",
        "print(f'logits, shape: (batch, t, target_vocabulary_size) {logits.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FmzjGmprVmE"
      },
      "source": [
        "For training, you'll want to implement your own masked loss and accuracy functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "outputs": [],
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "nRB1CTmQWOIL"
      },
      "outputs": [],
      "source": [
        "def masked_acc(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "    \n",
        "    match = tf.cast(y_true == y_pred, tf.float32)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    \n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f32GuAhw2nXm"
      },
      "source": [
        "Configure the model for training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "9g0DRRvm3l9X"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='Adam',\n",
        "              loss=masked_loss, \n",
        "              metrics=[masked_acc, masked_loss])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DWLI3pssjnx"
      },
      "source": [
        "The model is randomly initialized, and should give roughly uniform output probabilities. So it's easy to predict what the initial values of the metrics should be:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuP3_LFENMJG",
        "outputId": "c7936c77-fcbb-4cfa-9f53-a36302abb705"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'expected_loss': 4.553877, 'expected_acc': 0.010526315789473684}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "vocab_size = 1.0 * target_text_processor.vocabulary_size()\n",
        "\n",
        "{\"expected_loss\": tf.math.log(vocab_size).numpy(),\n",
        " \"expected_acc\": 1/vocab_size}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frVba49Usd0Z"
      },
      "source": [
        "That should roughly match the values returned by running a few steps of evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rJITfxEsHKR",
        "outputId": "7a025d00-ac04-4558-d45f-2db181d60430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 9/70 [==>...........................] - ETA: 0s - loss: 4.8377 - masked_acc: 0.0000e+00 - masked_loss: 4.8377  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 70 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r70/70 [==============================] - 7s 6ms/step - loss: 4.8377 - masked_acc: 0.0000e+00 - masked_loss: 4.8377\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': 4.83773136138916, 'masked_acc': 0.0, 'masked_loss': 4.83773136138916}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "model.evaluate(val_ds, steps=70, return_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQd_esVVoSf3",
        "outputId": "654fcaa8-baf0-425f-d488-38c8d9d21616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.0782 - masked_acc: 0.4899 - masked_loss: 3.0782"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 70 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 16s 18ms/step - loss: 3.0798 - masked_acc: 0.4900 - masked_loss: 3.0798 - val_loss: 3.4550 - val_masked_acc: 0.5000 - val_masked_loss: 3.4550\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.9642 - masked_acc: 0.5000 - masked_loss: 2.9642"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 2.9642 - masked_acc: 0.5000 - masked_loss: 2.9642\n",
            "Epoch 3/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.8639 - masked_acc: 0.5051 - masked_loss: 2.8639"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 2.8532 - masked_acc: 0.5050 - masked_loss: 2.8532\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.5746 - masked_acc: 0.5000 - masked_loss: 2.5746"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 2.5746 - masked_acc: 0.5000 - masked_loss: 2.5746\n",
            "Epoch 5/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 2.5725 - masked_acc: 0.5000 - masked_loss: 2.5725"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 2.5744 - masked_acc: 0.5000 - masked_loss: 2.5744\n",
            "Epoch 6/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 2.3265 - masked_acc: 0.5000 - masked_loss: 2.3265"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 2.3276 - masked_acc: 0.5000 - masked_loss: 2.3276\n",
            "Epoch 7/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 2.1434 - masked_acc: 0.5155 - masked_loss: 2.1434"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 2.1432 - masked_acc: 0.5150 - masked_loss: 2.1432\n",
            "Epoch 8/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 1.8990 - masked_acc: 0.5361 - masked_loss: 1.8990"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 1.8932 - masked_acc: 0.5350 - masked_loss: 1.8932\n",
            "Epoch 9/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 1.5316 - masked_acc: 0.5722 - masked_loss: 1.5316"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 1.5277 - masked_acc: 0.5700 - masked_loss: 1.5277\n",
            "Epoch 10/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 1.0457 - masked_acc: 0.6701 - masked_loss: 1.0457"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 1.0348 - masked_acc: 0.6750 - masked_loss: 1.0348\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.4641 - masked_acc: 0.8850 - masked_loss: 0.4641"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 0.4641 - masked_acc: 0.8850 - masked_loss: 0.4641\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1493 - masked_acc: 0.9850 - masked_loss: 0.1493"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 18ms/step - loss: 0.1493 - masked_acc: 0.9850 - masked_loss: 0.1493\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0524 - masked_acc: 0.9950 - masked_loss: 0.0524"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 0.0524 - masked_acc: 0.9950 - masked_loss: 0.0524\n",
            "Epoch 14/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0216 - masked_acc: 1.0000 - masked_loss: 0.0216"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 21ms/step - loss: 0.0215 - masked_acc: 1.0000 - masked_loss: 0.0215\n",
            "Epoch 15/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0147 - masked_acc: 1.0000 - masked_loss: 0.0147"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 0.0147 - masked_acc: 1.0000 - masked_loss: 0.0147\n",
            "Epoch 16/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0115 - masked_acc: 1.0000 - masked_loss: 0.0115"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 31ms/step - loss: 0.0114 - masked_acc: 1.0000 - masked_loss: 0.0114\n",
            "Epoch 17/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0096 - masked_acc: 1.0000 - masked_loss: 0.0096"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 35ms/step - loss: 0.0096 - masked_acc: 1.0000 - masked_loss: 0.0096\n",
            "Epoch 18/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.0083 - masked_acc: 1.0000 - masked_loss: 0.0083"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 25ms/step - loss: 0.0083 - masked_acc: 1.0000 - masked_loss: 0.0083\n",
            "Epoch 19/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.0074 - masked_acc: 1.0000 - masked_loss: 0.0074"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 0.0074 - masked_acc: 1.0000 - masked_loss: 0.0074\n",
            "Epoch 20/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0063 - masked_acc: 1.0000 - masked_loss: 0.0063"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 0.0063 - masked_acc: 1.0000 - masked_loss: 0.0063\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0058 - masked_acc: 1.0000 - masked_loss: 0.0058"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 0.0058 - masked_acc: 1.0000 - masked_loss: 0.0058\n",
            "Epoch 22/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.0051 - masked_acc: 1.0000 - masked_loss: 0.0051"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 0.0051 - masked_acc: 1.0000 - masked_loss: 0.0051\n",
            "Epoch 23/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0046 - masked_acc: 1.0000 - masked_loss: 0.0046"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 0.0046 - masked_acc: 1.0000 - masked_loss: 0.0046\n",
            "Epoch 24/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0041 - masked_acc: 1.0000 - masked_loss: 0.0041"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 0.0041 - masked_acc: 1.0000 - masked_loss: 0.0041\n",
            "Epoch 25/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0039 - masked_acc: 1.0000 - masked_loss: 0.0039"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 0.0039 - masked_acc: 1.0000 - masked_loss: 0.0039\n",
            "Epoch 26/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.0035 - masked_acc: 1.0000 - masked_loss: 0.0035"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 0.0035 - masked_acc: 1.0000 - masked_loss: 0.0035\n",
            "Epoch 27/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.0032 - masked_acc: 1.0000 - masked_loss: 0.0032"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 0.0032 - masked_acc: 1.0000 - masked_loss: 0.0032\n",
            "Epoch 28/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.0029 - masked_acc: 1.0000 - masked_loss: 0.0029"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 0.0029 - masked_acc: 1.0000 - masked_loss: 0.0029\n",
            "Epoch 29/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.0028 - masked_acc: 1.0000 - masked_loss: 0.0028"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 0.0028 - masked_acc: 1.0000 - masked_loss: 0.0028\n",
            "Epoch 30/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.0025 - masked_acc: 1.0000 - masked_loss: 0.0025"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 0.0025 - masked_acc: 1.0000 - masked_loss: 0.0025\n",
            "Epoch 31/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.0023 - masked_acc: 1.0000 - masked_loss: 0.0023"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 0.0023 - masked_acc: 1.0000 - masked_loss: 0.0023\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0021 - masked_acc: 1.0000 - masked_loss: 0.0021"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 0.0021 - masked_acc: 1.0000 - masked_loss: 0.0021\n",
            "Epoch 33/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0020 - masked_acc: 1.0000 - masked_loss: 0.0020"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 0.0020 - masked_acc: 1.0000 - masked_loss: 0.0020\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0018 - masked_acc: 1.0000 - masked_loss: 0.0018"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 0.0018 - masked_acc: 1.0000 - masked_loss: 0.0018\n",
            "Epoch 35/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.0017 - masked_acc: 1.0000 - masked_loss: 0.0017"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 0.0017 - masked_acc: 1.0000 - masked_loss: 0.0017\n",
            "Epoch 36/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.0016 - masked_acc: 1.0000 - masked_loss: 0.0016"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 0.0016 - masked_acc: 1.0000 - masked_loss: 0.0016\n",
            "Epoch 37/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 0.0015 - masked_acc: 1.0000 - masked_loss: 0.0015"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 0.0015 - masked_acc: 1.0000 - masked_loss: 0.0015\n",
            "Epoch 38/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.0014 - masked_acc: 1.0000 - masked_loss: 0.0014"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 0.0014 - masked_acc: 1.0000 - masked_loss: 0.0014\n",
            "Epoch 39/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0013 - masked_acc: 1.0000 - masked_loss: 0.0013"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 0.0013 - masked_acc: 1.0000 - masked_loss: 0.0013\n",
            "Epoch 40/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0012 - masked_acc: 1.0000 - masked_loss: 0.0012"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 0.0012 - masked_acc: 1.0000 - masked_loss: 0.0012\n",
            "Epoch 41/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.0011 - masked_acc: 1.0000 - masked_loss: 0.0011"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 0.0011 - masked_acc: 1.0000 - masked_loss: 0.0011\n",
            "Epoch 42/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 0.0011 - masked_acc: 1.0000 - masked_loss: 0.0011"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 0.0011 - masked_acc: 1.0000 - masked_loss: 0.0011\n",
            "Epoch 43/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 9.9067e-04 - masked_acc: 1.0000 - masked_loss: 9.9067e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 9.8834e-04 - masked_acc: 1.0000 - masked_loss: 9.8834e-04\n",
            "Epoch 44/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 9.3639e-04 - masked_acc: 1.0000 - masked_loss: 9.3639e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 9.3639e-04 - masked_acc: 1.0000 - masked_loss: 9.3639e-04\n",
            "Epoch 45/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 8.6995e-04 - masked_acc: 1.0000 - masked_loss: 8.6995e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 8.7207e-04 - masked_acc: 1.0000 - masked_loss: 8.7207e-04\n",
            "Epoch 46/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 8.2869e-04 - masked_acc: 1.0000 - masked_loss: 8.2869e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 8.2869e-04 - masked_acc: 1.0000 - masked_loss: 8.2869e-04\n",
            "Epoch 47/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 7.7287e-04 - masked_acc: 1.0000 - masked_loss: 7.7287e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 7.7287e-04 - masked_acc: 1.0000 - masked_loss: 7.7287e-04\n",
            "Epoch 48/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 7.2407e-04 - masked_acc: 1.0000 - masked_loss: 7.2407e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 7.2554e-04 - masked_acc: 1.0000 - masked_loss: 7.2554e-04\n",
            "Epoch 49/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 6.8236e-04 - masked_acc: 1.0000 - masked_loss: 6.8236e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 6.8535e-04 - masked_acc: 1.0000 - masked_loss: 6.8535e-04\n",
            "Epoch 50/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 6.4709e-04 - masked_acc: 1.0000 - masked_loss: 6.4709e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 6.4552e-04 - masked_acc: 1.0000 - masked_loss: 6.4552e-04\n",
            "Epoch 51/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 6.1010e-04 - masked_acc: 1.0000 - masked_loss: 6.1010e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 6.0891e-04 - masked_acc: 1.0000 - masked_loss: 6.0891e-04\n",
            "Epoch 52/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 5.6667e-04 - masked_acc: 1.0000 - masked_loss: 5.6667e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 5.6590e-04 - masked_acc: 1.0000 - masked_loss: 5.6590e-04\n",
            "Epoch 53/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.3673e-04 - masked_acc: 1.0000 - masked_loss: 5.3673e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 5.3673e-04 - masked_acc: 1.0000 - masked_loss: 5.3673e-04\n",
            "Epoch 54/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 5.0170e-04 - masked_acc: 1.0000 - masked_loss: 5.0170e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 5.0124e-04 - masked_acc: 1.0000 - masked_loss: 5.0124e-04\n",
            "Epoch 55/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 4.8004e-04 - masked_acc: 1.0000 - masked_loss: 4.8004e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 4.7965e-04 - masked_acc: 1.0000 - masked_loss: 4.7965e-04\n",
            "Epoch 56/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 4.5595e-04 - masked_acc: 1.0000 - masked_loss: 4.5595e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 4.5507e-04 - masked_acc: 1.0000 - masked_loss: 4.5507e-04\n",
            "Epoch 57/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 4.2017e-04 - masked_acc: 1.0000 - masked_loss: 4.2017e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 4.1977e-04 - masked_acc: 1.0000 - masked_loss: 4.1977e-04\n",
            "Epoch 58/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 4.0131e-04 - masked_acc: 1.0000 - masked_loss: 4.0131e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 4.0003e-04 - masked_acc: 1.0000 - masked_loss: 4.0003e-04\n",
            "Epoch 59/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 3.8017e-04 - masked_acc: 1.0000 - masked_loss: 3.8017e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 3.7986e-04 - masked_acc: 1.0000 - masked_loss: 3.7986e-04\n",
            "Epoch 60/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 3.5839e-04 - masked_acc: 1.0000 - masked_loss: 3.5839e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 3.5866e-04 - masked_acc: 1.0000 - masked_loss: 3.5866e-04\n",
            "Epoch 61/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 3.3658e-04 - masked_acc: 1.0000 - masked_loss: 3.3658e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 3.3693e-04 - masked_acc: 1.0000 - masked_loss: 3.3693e-04\n",
            "Epoch 62/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.1470e-04 - masked_acc: 1.0000 - masked_loss: 3.1470e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 3.1419e-04 - masked_acc: 1.0000 - masked_loss: 3.1419e-04\n",
            "Epoch 63/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.0197e-04 - masked_acc: 1.0000 - masked_loss: 3.0197e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 3.0177e-04 - masked_acc: 1.0000 - masked_loss: 3.0177e-04\n",
            "Epoch 64/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.8362e-04 - masked_acc: 1.0000 - masked_loss: 2.8362e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 2.8362e-04 - masked_acc: 1.0000 - masked_loss: 2.8362e-04\n",
            "Epoch 65/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 2.7018e-04 - masked_acc: 1.0000 - masked_loss: 2.7018e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 2.6984e-04 - masked_acc: 1.0000 - masked_loss: 2.6984e-04\n",
            "Epoch 66/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.5237e-04 - masked_acc: 1.0000 - masked_loss: 2.5237e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 2.5244e-04 - masked_acc: 1.0000 - masked_loss: 2.5244e-04\n",
            "Epoch 67/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 2.3927e-04 - masked_acc: 1.0000 - masked_loss: 2.3927e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 2.3924e-04 - masked_acc: 1.0000 - masked_loss: 2.3924e-04\n",
            "Epoch 68/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.2491e-04 - masked_acc: 1.0000 - masked_loss: 2.2491e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 2.2491e-04 - masked_acc: 1.0000 - masked_loss: 2.2491e-04\n",
            "Epoch 69/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 2.1992e-04 - masked_acc: 1.0000 - masked_loss: 2.1992e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 2.1941e-04 - masked_acc: 1.0000 - masked_loss: 2.1941e-04\n",
            "Epoch 70/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 2.0125e-04 - masked_acc: 1.0000 - masked_loss: 2.0125e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 2.0134e-04 - masked_acc: 1.0000 - masked_loss: 2.0134e-04\n",
            "Epoch 71/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.9261e-04 - masked_acc: 1.0000 - masked_loss: 1.9261e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 1.9232e-04 - masked_acc: 1.0000 - masked_loss: 1.9232e-04\n",
            "Epoch 72/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.8229e-04 - masked_acc: 1.0000 - masked_loss: 1.8229e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 1.8229e-04 - masked_acc: 1.0000 - masked_loss: 1.8229e-04\n",
            "Epoch 73/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 1.6958e-04 - masked_acc: 1.0000 - masked_loss: 1.6958e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 1.6924e-04 - masked_acc: 1.0000 - masked_loss: 1.6924e-04\n",
            "Epoch 74/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.6317e-04 - masked_acc: 1.0000 - masked_loss: 1.6317e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 1.6317e-04 - masked_acc: 1.0000 - masked_loss: 1.6317e-04\n",
            "Epoch 75/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.5586e-04 - masked_acc: 1.0000 - masked_loss: 1.5586e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 1.5586e-04 - masked_acc: 1.0000 - masked_loss: 1.5586e-04\n",
            "Epoch 76/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 1.4458e-04 - masked_acc: 1.0000 - masked_loss: 1.4458e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 1.4462e-04 - masked_acc: 1.0000 - masked_loss: 1.4462e-04\n",
            "Epoch 77/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 1.3900e-04 - masked_acc: 1.0000 - masked_loss: 1.3900e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 1.3921e-04 - masked_acc: 1.0000 - masked_loss: 1.3921e-04\n",
            "Epoch 78/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.3007e-04 - masked_acc: 1.0000 - masked_loss: 1.3007e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 1.2998e-04 - masked_acc: 1.0000 - masked_loss: 1.2998e-04\n",
            "Epoch 79/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.2318e-04 - masked_acc: 1.0000 - masked_loss: 1.2318e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 1.2349e-04 - masked_acc: 1.0000 - masked_loss: 1.2349e-04\n",
            "Epoch 80/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.1759e-04 - masked_acc: 1.0000 - masked_loss: 1.1759e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 1.1763e-04 - masked_acc: 1.0000 - masked_loss: 1.1763e-04\n",
            "Epoch 81/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.1208e-04 - masked_acc: 1.0000 - masked_loss: 1.1208e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 1.1208e-04 - masked_acc: 1.0000 - masked_loss: 1.1208e-04\n",
            "Epoch 82/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 1.0442e-04 - masked_acc: 1.0000 - masked_loss: 1.0442e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 22ms/step - loss: 1.0425e-04 - masked_acc: 1.0000 - masked_loss: 1.0425e-04\n",
            "Epoch 83/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 1.0023e-04 - masked_acc: 1.0000 - masked_loss: 1.0023e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 1.0023e-04 - masked_acc: 1.0000 - masked_loss: 1.0023e-04\n",
            "Epoch 84/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 9.3954e-05 - masked_acc: 1.0000 - masked_loss: 9.3954e-05"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 9.3976e-05 - masked_acc: 1.0000 - masked_loss: 9.3976e-05\n",
            "Epoch 85/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 8.9222e-05 - masked_acc: 1.0000 - masked_loss: 8.9222e-05"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 8.9208e-05 - masked_acc: 1.0000 - masked_loss: 8.9208e-05\n",
            "Epoch 86/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 8.4829e-05 - masked_acc: 1.0000 - masked_loss: 8.4829e-05"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 3s 29ms/step - loss: 8.4818e-05 - masked_acc: 1.0000 - masked_loss: 8.4818e-05\n",
            "Epoch 87/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 8.0110e-05 - masked_acc: 1.0000 - masked_loss: 8.0110e-05"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 23ms/step - loss: 8.0110e-05 - masked_acc: 1.0000 - masked_loss: 8.0110e-05\n",
            "Epoch 88/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 7.6066e-05 - masked_acc: 1.0000 - masked_loss: 7.6066e-05"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 7.5984e-05 - masked_acc: 1.0000 - masked_loss: 7.5984e-05\n",
            "Epoch 89/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 7.2354e-05 - masked_acc: 1.0000 - masked_loss: 7.2354e-05"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 7.2233e-05 - masked_acc: 1.0000 - masked_loss: 7.2233e-05\n",
            "Epoch 90/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 6.8777e-05 - masked_acc: 1.0000 - masked_loss: 6.8777e-05"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 6.8661e-05 - masked_acc: 1.0000 - masked_loss: 6.8661e-05\n",
            "Epoch 91/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 6.4656e-05 - masked_acc: 1.0000 - masked_loss: 6.4656e-05"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 22ms/step - loss: 6.4597e-05 - masked_acc: 1.0000 - masked_loss: 6.4597e-05\n",
            "Epoch 92/100\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 6.0998e-05 - masked_acc: 1.0000 - masked_loss: 6.0998e-05"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 6.0967e-05 - masked_acc: 1.0000 - masked_loss: 6.0967e-05\n",
            "Epoch 93/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.7540e-05 - masked_acc: 1.0000 - masked_loss: 5.7540e-05"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 5.7540e-05 - masked_acc: 1.0000 - masked_loss: 5.7540e-05\n",
            "Epoch 94/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.5881e-05 - masked_acc: 1.0000 - masked_loss: 5.5881e-05"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 5.5881e-05 - masked_acc: 1.0000 - masked_loss: 5.5881e-05\n",
            "Epoch 95/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.2085e-05 - masked_acc: 1.0000 - masked_loss: 5.2085e-05"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 5.2085e-05 - masked_acc: 1.0000 - masked_loss: 5.2085e-05\n",
            "Epoch 96/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 4.8767e-05 - masked_acc: 1.0000 - masked_loss: 4.8767e-05"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 4.8707e-05 - masked_acc: 1.0000 - masked_loss: 4.8707e-05\n",
            "Epoch 97/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 4.7114e-05 - masked_acc: 1.0000 - masked_loss: 4.7114e-05"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 4.7171e-05 - masked_acc: 1.0000 - masked_loss: 4.7171e-05\n",
            "Epoch 98/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.4518e-05 - masked_acc: 1.0000 - masked_loss: 4.4518e-05"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 4.4518e-05 - masked_acc: 1.0000 - masked_loss: 4.4518e-05\n",
            "Epoch 99/100\n",
            " 97/100 [============================>.] - ETA: 0s - loss: 4.2279e-05 - masked_acc: 1.0000 - masked_loss: 4.2279e-05"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 17ms/step - loss: 4.2082e-05 - masked_acc: 1.0000 - masked_loss: 4.2082e-05\n",
            "Epoch 100/100\n",
            " 98/100 [============================>.] - ETA: 0s - loss: 3.9907e-05 - masked_acc: 1.0000 - masked_loss: 3.9907e-05"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,masked_acc,masked_loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 2s 16ms/step - loss: 3.9880e-05 - masked_acc: 1.0000 - masked_loss: 3.9880e-05\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_ds.repeat(), \n",
        "    epochs=100,\n",
        "    steps_per_epoch = 100,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps = 70,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=3)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "38rLdlmtQHCm",
        "outputId": "adb74312-40c7-4a3e-b795-c4fb5152fe61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f7967fc7730>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhV9bn28e+TgYQIIZCEEMYgUlAIgwXUWmnV41hb21qLQx2rnFoHWn1btJPD8bSnw6m16luKM32xDjjhVLWKIuIUEAiIIiJIkCEEkhAgZHreP/YmxhjIBrKysrPvz3XtizXtte6VxZUna/r9zN0REZHElRR2ABERCZcKgYhIglMhEBFJcCoEIiIJToVARCTBpYQdYF/l5OR4QUFB2DFEROLKggULNrt7bkvz4q4QFBQUUFRUFHYMEZG4YmZr9jRPl4ZERBKcCoGISIJTIRARSXCB3SMws3RgLpAW3c4sd7++2TIXAn8E1kUn3e7udwWVSUQ+U1tbS0lJCdXV1WFHkTaUnp5O//79SU1Njfk7Qd4s3gUc5+5VZpYKzDOz59z9zWbLPeTuVwSYQ0RaUFJSQvfu3SkoKMDMwo4jbcDdKSsro6SkhMGDB8f8vcAuDXlEVXQ0NfpRC3ciHUR1dTXZ2dkqAp2ImZGdnb3PZ3mB3iMws2QzWwRsAl5097daWOwMM1tiZrPMbMAe1jPZzIrMrKi0tDTIyCIJRUWg89mfYxpoIXD3encfA/QHJpjZyGaLPAUUuPso4EXg/j2sZ7q7j3P3cbm5Lb4PISIi+6ldnhpy93JgDnBys+ll7r4rOnoX8OX2yCMiHUO3bt3CjiAEWAjMLNfMsqLDXYETgPebLZPfZPRbwPKg8oiISMuCPCPIB+aY2RLgHSL3CJ42s5vM7FvRZa4ys2Vmthi4CrgwwDwi0kG5Oz/72c8YOXIkhYWFPPTQQwCsX7+eiRMnMmbMGEaOHMlrr71GfX09F154YeOyt9xyS8jp419gj4+6+xJgbAvTf9Nk+DrguqAyiEhsbnxqGe99Wtmm6zysbybXf3NETMs+9thjLFq0iMWLF7N582bGjx/PxIkTeeCBBzjppJP45S9/SX19PTt27GDRokWsW7eOpUuXAlBeXt6muROR3iwWkdDNmzePs88+m+TkZPLy8vja177GO++8w/jx47n33nu54YYbKC4upnv37hx88MGsWrWKK6+8kn/9619kZmaGHT/uxV3royLS9mL9y729TZw4kblz5/LMM89w4YUXcvXVV3P++eezePFinn/+eaZNm8bDDz/MPffcE3bUuKYzAhEJ3THHHMNDDz1EfX09paWlzJ07lwkTJrBmzRry8vK49NJLueSSS1i4cCGbN2+moaGBM844g5tvvpmFCxeGHT/u6YxAREL3ne98hzfeeIPRo0djZvzhD3+gT58+3H///fzxj38kNTWVbt26MWPGDNatW8dFF11EQ0MDAL/73e9CTh//zD2+Wn0YN26cq2MakQO3fPlyDj300LBjSABaOrZmtsDdx7W0vC4NiYgkOBUCEZEEp0IgIpLgVAhERBKcCoGISIJTIRARSXAqBCIiCU6FQEQ6jdWrVzNyZPP+r2K3t/4RDnTdHZkKgYhIglMTEyICz10LG4rbdp19CuGU/9nrIqtXr+bkk0/myCOPZP78+YwfP56LLrqI66+/nk2bNjFz5kwApkyZQnV1NV27duXee+9l2LBhLFu2jIsuuoiamhoaGhp49NFHSU1NbVz3qlWrOOOMM5g+fTq9evXi8ssvp7S0lIyMDO68806GDx/Oxx9/zDnnnENVVRWnn356zLtWXV3NZZddRlFRESkpKfz5z3/m2GOPbTFT3759+f73v09JSQn19fX8+te/ZtKkSfv3Mw2ICoGIhGrlypU88sgj3HPPPYwfP54HHniAefPmMXv2bH77298yY8YMXnvtNVJSUvj3v//NL37xCx599FGmTZvGlClTOPfcc6mpqaG+vp6NGzcC8MEHH3DWWWdx3333MXr0aI4//nimTZvG0KFDeeutt/jxj3/Myy+/zJQpU7jssss4//zzueOOO2LOfMcdd2BmFBcX8/7773PiiSeyYsWKFjM9++yz9O3bl2eeeQaAioqKQH6OB0KFQERa/cs9SIMHD6awsBCAESNGcPzxx2NmFBYWsnr1aioqKrjgggv48MMPMTNqa2sBOOqoo/jv//5vSkpK+O53v8vQoUMBKC0t5fTTT+exxx7jsMMOo6qqivnz53PmmWc2bnPXrkhX6a+//jqPPvooAOeddx5Tp06NKfO8efO48sorARg+fDiDBg1ixYoVLWYqLCzkmmuuYerUqZx22mkcc8wxbfODa0O6RyAioUpLS2scTkpKahxPSkqirq6OX//61xx77LEsXbqUp556iurqagDOOeccZs+eTdeuXTn11FN5+eWXAejRowcDBw5k3rx5ADQ0NJCVlcWiRYsaP8uXf9Y9upm12b60lOlLX/oSCxcupLCwkF/96lfcdNNNbba9tpIwhWD18iLevvVcdlXvCDuKiOyDiooK+vXrB8B9993XOH3VqlUcfPDBXHXVVZx++uksWbIEgC5duvD4448zY8YMHnjgATIzMxk8eDCPPPIIEOkfefHixQAcffTRPPjggwCN9yNiccwxxzQuv2LFCj755BOGDRvWYqZPP/2UjIwMfvCDH/Czn/2sQ/afkDCFYNumT5iw9WmW/Es9GYnEk5///Odcd911jB07lrq6usbpDz/8MCNHjmTMmDEsXbqU888/v3HeQQcdxNNPP80tt9zC7NmzmTlzJnfffTejR49mxIgRPPnkkwDceuut3HHHHRQWFrJu3bqYM/34xz+moaGBwsJCJk2axH333UdaWlqLmYqLi5kwYQJjxozhxhtv5Fe/+lXb/XDaSML0R+ANDay5eTQNJDH4V+9iSQlTA0VapP4IOq8O0x+BmaWb2dtmttjMlpnZjS0sk2ZmD5nZSjN7y8wKAsuTlMSmET/k4IbVLJv/dFCbERGJO0H+WbwLOM7dRwNjgJPN7Mhmy/wQ2OruhwC3AL8PMA+jTr2ULWRS//rtQW5GROJYcXExY8aM+dzniCOOCDtWoAJ7fNQj15yqoqOp0U/z61CnAzdEh2cBt5uZeUDXq9K7HsS7AyZx1No7+WTFIgZ+aUwQmxGROFZYWMiiRYvCjtGuAr1QbmbJZrYI2AS86O5vNVukH7AWwN3rgAogu4X1TDazIjMrKi0tPaBMQ78xhV2eyvrnbzmg9YiIdBaBFgJ3r3f3MUB/YIKZ7VeLTe4+3d3Hufu43NzcA8qU02cAi3uewKjNz1K+ecMBrUtEpDNol0dn3L0cmAOc3GzWOmAAgJmlAD2AsqDz5J54NV2thuVP6axARCTIp4ZyzSwrOtwVOAF4v9lis4ELosPfA14O6v5AU4MPG8/irkcwfM0DVO+oav0LIiKdWJBnBPnAHDNbArxD5B7B02Z2k5l9K7rM3UC2ma0ErgauDTDP56RO/Ck9qWTx0/+3vTYpIgdgb30FBOGVV17htNNO26/vttZ3wYGsOwhBPjW0BBjbwvTfNBmuBs5svkx7OPSIk/jgpWH0X34X9XVXk5yi9vdEJDEl7G8/S0pi+/grGfbGFSx44R98+dSLwo4kEprfv/173t/S/MrtgRneazhTJ+y5Nc9rr72WAQMGcPnllwNwww03kJKSwpw5c9i6dSu1tbXcfPPNMfUT8Morr3D99deTlZVFcXEx3//+9yksLOTWW29l586dPPHEEwwZMoSnnnqKm2++mZqaGrKzs5k5cyZ5eXm8+uqrTJkyBYg0Qjd37tzPrf+dd95h8uTJzJo1i/Lycq6++mqqqqrIycnhvvvuIz8/nwULFnDxxRcDcOKJJ8b8c9qyZQsXX3wxq1atIiMjg+nTpzNq1KgWM1VVVTFp0iQqKyupq6vjb3/7W5u0ZprQ7SyMPv5s1lpfMhfcjjc0hB1HJKFMmjSJhx9+uHH84Ycf5oILLuDxxx9n4cKFzJkzh2uuuYZYbxsuXryYadOmsXz5cv7xj3+wYsUK3n77bS655BJuu+02AL761a/y5ptv8u6773LWWWfxhz/8AYA//elP3HHHHSxatIjXXnuNrl27Nq53/vz5/OhHP+LJJ59k4MCBXHnllcyaNavxF/8vf/lLAC666CJuu+22xgbtYnX99dczduxYlixZwm9/+9vGNpNayvTAAw9w0kknsWjRIhYvXsyYMW3zLlTCnhEAJKeksH7EpUxYeiPL33mRQ484KexIIqHY21/uQRk7diybNm3i008/pbS0lJ49e9KnTx9++tOfMnfuXJKSkli3bh0bN26kT58+ra5v/Pjx5OfnAzBkyJDGv8oLCwuZM2cOACUlJUyaNIn169dTU1PD4MGDgUgrpFdffTXnnnsu3/3ud+nfvz8QabNn8uTJvPDCC/Tt25elS5eydOlSTjjhBADq6+vJz8+nvLyc8vJyJk6cCET6Nnjuuedi+jnMmzevsU+E4447jrKyMiorK1vMNH78eC6++GJqa2v59re/3WaFIKHPCACGHXceABUfzAs5iUjiOfPMM5k1axYPPfQQkyZNYubMmZSWlrJgwQIWLVpEXl5eY/8DrWmtXwOAK6+8kiuuuILi4mL+/ve/N6772muv5a677mLnzp0cffTRvP9+5DJZfn4+6enpvPvuu0CkCesRI0Y09mtQXFzMCy+80GY/j6ZayjRx4kTmzp1Lv379uPDCC5kxY0abbCvhC0GPXrlsIIeUze+FHUUk4UyaNIkHH3yQWbNmceaZZ1JRUUHv3r1JTU1lzpw5rFmzpk2317Rvg/vvv79x+kcffURhYSFTp05l/PjxjYUgKyuLZ555huuuu45XXnmFYcOGUVpayhtvvAFAbW0ty5YtIysri6ysrMbOcPa3b4NXXnmFnJwcMjMzW8y0Zs0a8vLyuPTSS7nkkkvarG+DhC8EABu6HkJO1YqwY4gknBEjRrBt2zb69etHfn4+5557LkVFRRQWFjJjxgyGDx/eptu74YYbOPPMM/nyl79MTk5O4/S//OUvjBw5klGjRpGamsopp5zSOC8vL4+nn36ayy+/nHfffZdZs2YxdepURo8ezZgxY5g/fz4A9957L5dffjljxoyJ+b7G7kwLFixg1KhRXHvttY0FqqVMr7zyCqNHj2bs2LE89NBDjTeTD1TC9EewN2/c+RPGl9xP3bUlpHc9qE3XLdJRqT+CzqvD9EcQT7r0G02KNVCy4t2wo4iItLuEfmpot95DD4e3YetHC2D0V8OOIyJ7UFxczHnnnfe5aWlpabz1VvOGjTuG559/nqlTP/9E1uDBg3n88cdDStQyFQKg7+AR7PA06tcXhx1FpF25O2YWdoyYxVtfASeddBInndS+j6Xvz+V+XRoi8j7B2tTBdK9o2zcrRTqy9PR0ysrK9usXh3RM7k5ZWRnp6en79D2dEUSV9xjGoWX/xhsa1LG9JIT+/ftTUlLCgXb2JB1Lenp64wtxsVIh2C1vJJllT7Kh5CP6DBwadhqRwKWmpja+WSuJTX/6RvUoiDSUumFF2z6aKiLS0akQRPUfHnm8dufa+LkRJSLSFlQIorpl9qTE+pBWpqYmRCSxqBA0sSljKL23fxh2DBGRdqVC0MSu7MPo27CB7dvKw44iItJu9NRQE+n9R5H0ibP02b+TkT+cpJRUDjn866SlZ4QdTUQkMCoETfQbcTT1rxtHLP8tLI9Me3PlVRx5/n+FG0xEJEAqBE307jeYNee8wvatGwDo8fxVpG3Q46Qi0rkFVgjMbAAwA8gDHJju7rc2W+brwJPAx9FJj7n7TUFlisWgYZ91/VY0fxSDKlUIRKRzC/KMoA64xt0Xmll3YIGZvejuzZ/PfM3dTwswx36ryx9LbuWLbFr3Mb376Q1MEemcAntqyN3Xu/vC6PA2Ilfd+wW1vSBkDT0SgHXLXg85iYhIcNrl8VEzKwDGAi01Gn6UmS02s+fMbMQevj/ZzIrMrKg9G8gqGHEktZ5M9eq3222bIiLtLfBCYGbdgEeBn7h7ZbPZC4FB7j4auA14oqV1uPt0dx/n7uNyc3ODDdxEeteDWJNSQLeyJe22TRGR9hZoITCzVCJFYKa7P9Z8vrtXuntVdPhZINXMcpovF6ayHiMYtGsF3tAQdhQRkUAEVggs0u3R3cByd//zHpbpE10OM5sQzVMWVKb9Yf0OJ5PtlKxaFnYUEZFABPnU0NHAeUCxme1u0vMXwEAAd58GfA+4zMzqgJ3AWd7BukvKGfYVKIaNy+cz4JDCsOOIiLS5wAqBu88D9toZqrvfDtweVIa2MHDYWHZ6F+rWFgH/GXYcEZE2p0bnWpGS2oXVXQ4ha+vSsKOIiARChSAGFT0LGVSzkrramrCjiIi0ORWCGKQMHEdXq2HN+wvDjiIi0uZUCGLQZ/hXAChb8UbISURE2p4KQQz6HXwYW8nEPlEhEJHOR4UgBpaUxKru4xhc8ZZeLBORTkeFIEb1Bx9HDuWsWqZ2h0Skc1EhiNHgI74JQOm7z4ScRESkbakQxCi3bwEfJxXQveTVsKOIiLQpFYJ9sLH30QzdtZTt28rDjiIi0mZUCPZBtxEn0cXq+fDtf4UdRUSkzagQ7IOh409gh6exa/kLYUcREWkzKgT7IC09gw8zRtO3TO8TiEjnoUKwj3YOPJYB/inrVi0PO4qISJtQIdhH/cZ9A4CSoqdCTiIi0jZUCPZR/yGFbCCX1E/mhR1FRKRNxNwxjZl9BSho+h13nxFApg7NkpL4tNth9Nn+fthRRETaREyFwMz+AQwBFgH10ckOJFwhAKjpPZq+q15la+l6eubmhx1HROSAxHpGMA44rKP1JxyWbkMmwCpYu2w+Pb9+RthxREQOSKz3CJYCfYIMEk8Gjoj0T7D9YzVAJyLxL9YzghzgPTN7G9i1e6K7fyuQVB1cZlY2nyT1I710SdhRREQOWKyF4IZ9XbGZDSByDyGPyP2E6e5+a7NlDLgVOBXYAVzo7nHRH+SmbocysDIuooqI7FVMl4bc/VVgNZAaHX4HaO23YB1wjbsfBhwJXG5mhzVb5hRgaPQzGfhb7NHDVddnLL3ZQumnq8OOIiJyQGIqBGZ2KTAL+Ht0Uj/gib19x93X7/7r3t23Acuj32vqdGCGR7wJZJlZXDyGkzVkAgDrls0POYmIyIGJ9Wbx5cDRQCWAu38I9I51I2ZWAIwF3mo2qx+wtsl4CV8sFpjZZDMrMrOi0tLSWDcbqEEjj6Lejeo1RWFHERE5ILEWgl3uXrN7xMxSiFz3b5WZdQMeBX7i7pX7HhHcfbq7j3P3cbm5ufuzijbX9aDufJI8iIzNumEsIvEt1kLwqpn9AuhqZicAjwCtNrZjZqlEisBMd3+shUXWAQOajPePTosLpZmHMaD6A3VoLyJxLdZCcC1QChQD/wk86+6/3NsXok8E3Q0sd/c/72Gx2cD5FnEkUOHu62PMFDrvO5aeVLL+kw/DjiIist9ifnzU3X8D3AlgZslmNtPdz93Ld44GzgOKzWxRdNovgIEA7j4NeJbIo6MriTw+etG+70J4eg09Et6D9e/Np2/BsLDjiIjsl1gLwQAzu87df2dmXYCHibQ7tEfuPg+wVpZxIjei49LAQ8dR80QyNWuLiLMaJiLSKNZLQxcDhWZ2HfA08Kq73xBYqjiRlp7BJykFHLRVndSISPzaayEws8PN7HAij37eCkwCPiRy8/jwdsjX4VVkDKTXrri5vy0i8gWtXRr632bjW4HDotMdOC6IUPGkJrOAPpWvUluzi9QuaWHHERHZZ3stBO5+bHsFiVfJOUNIWddAyScf0v+QkWHHERHZZ7E2MdHDzP68++1eM/tfM+sRdLh40L3vlwDYslY9lolIfIr1ZvE9wDbg+9FPJXBvUKHiSe6gQwHYuVHvEohIfIr18dEh7t60K64bm7wbkNCye/dnh6fhW1aFHUVEZL/Eekaw08y+unvEzI4GdgYTKb5YUhLrU/rSdduasKOIiOyXWM8IfgTMaHJfYCtwQTCR4k9F+gBydn4UdgwRkf0SayGodPfRZpYJ4O6VZjY4wFxxZVdmAX2qXqe+ro7klFh/pCIiHUOsl4YehUgBaNKU9KxgIsWf5OzBdLF6Nq3TWYGIxJ+9/vlqZsOBEUAPM/tuk1mZQHqQweJJRv5QWApln7xP/iA1Pici8aW16xjDgNOALOCbTaZvAy4NKlS8yRkYeYR0+wY9Qioi8ae1QpAB/B9guru/0Q554lLvvoPZ5al4mR4hFZH401ohGEikN7JUM3sJeA54O9p8tEQlJSezPrkPaZWrw44iIrLP9nqz2N1/7+7HEek8ZjGR5qgXmtkDZna+meW1R8h4sDV9AFnVJWHHEBHZZzE9NeTu29z9cXf/T3cfC9wM5AIzAk0XR3Z1H0Sf+vU01NeHHUVEZJ+01h/BD5oMH7172N3fA3a5+0kBZosrln0wXa2GzRs+CTuKiMg+ae2M4Oomw7c1m3dxG2eJaxl9hgJQuka9lYlIfGmtENgehlsaT2jZA6KPkK5fEXISEZF901oh8D0MtzSe0Hr3P5gaT6Zej5CKSJxprRAMN7MlZlbcZHj3+F5foTWze8xsk5kt3cP8r5tZhZktin5+s5/70CGkpHZhY1KeHiEVkbjT2nsEo4E8YG2z6QOADa189z7gdvb+ZNFr7n5aK+uJG1vT+tK9+tOwY4iI7JPWzghuASrcfU3TD1ARnbdH7j4X2NJGOeNCdXpvsuo2hx1DRGSftFYI8ty9uPnE6LSCNtj+UWa22MyeM7MRe1rIzCbv7i+5tLS0DTYbjIZu+fTycupqa8KOIiISs9YKQdZe5nU9wG0vBAa5+2gij6Y+sacF3X26u49z93G5ubkHuNngWI++JJuzZdO6sKOIiMSstUJQZGZfaGXUzC4BFhzIhqN9G1RFh58l0p5RzoGsM2xpPfsBUL5R3VaKSPxo7WbxT4DHzexcPvvFPw7oAnznQDZsZn2Aje7uZjaBSFEqO5B1hq1b7gAAqkqb31sXEem49loI3H0j8BUzOxYYGZ38jLu/3NqKzeyfwNeBHDMrAa4HUqPrnQZ8D7jMzOqAncBZ8d6qaVbeIABqy3VpSETiR0wd7Lr7HGDOvqzY3c9uZf7tRB4v7TR65fal1pNpqNAjpCISP2Lts1hikJScTJn1JGV7a69YiIh0HCoEbawiJZv06k1hxxARiZkKQRvbntabzFq9VCYi8UOFoI3VZuTRq16FQETihwpBG2vo1ofutpPt28rDjiIiEhMVgjaWkhV5qWzLBr1UJiLxQYWgjXXN7g9AxSZ1WSki8UGFoI11zx0IQHWZXioTkfigQtDGsvMjbxfX6e1iEYkTKgRtrFtmT7Z7OmxbH3YUEZGYqBAEoCw5my47N4YdQ0QkJioEAahMzSWjuuN2oCMi0pQKQQCq03LpoS4rRSROqBAEoPagfLJ9Cw319WFHERFplQpBACwzny5Wz9bNumEsIh2fCkEAuvTsC0D5Rr1UJiIdnwpBADJyol1WblYhEJGOT4UgAFm99XaxiMQPFYIAZPcZSIMbDZW6RyAiHZ8KQQBSu6SxxXqQXKVCICIdnwpBQMqTs0nT28UiEgcCKwRmdo+ZbTKzpXuYb2b2VzNbaWZLzOzwoLKEoapLLt1r9HaxiHR8QZ4R3AecvJf5pwBDo5/JwN8CzNLuatJzyGxQL2Ui0vEFVgjcfS6wZS+LnA7M8Ig3gSwzyw8qT3urz8ilp1dQX1cXdhQRkb0K8x5BP2Btk/GS6LQvMLPJZlZkZkWlpfFxucW69SbZnPKyDWFHERHZq7i4Wezu0919nLuPy83NDTtOTFJ75AFQuVnvEohIxxZmIVgHDGgy3j86rVNIz4pc5arSGYGIdHBhFoLZwPnRp4eOBCrcvdM8eN8tO9Le0K7yTrNLItJJpQS1YjP7J/B1IMfMSoDrgVQAd58GPAucCqwEdgAXBZUlDFm5kdsdddv0LoGIdGyBFQJ3P7uV+Q5cHtT2w5aZlU2Np+BVm8KOIiKyV3FxszgeWVISW60HKTvUU5mIdGwqBAGqTO5J2q6ysGOIiOyVCkGAdnTpRUbt3t6pExEJnwpBgHal5ZBZvzXsGCIie6VCEKD6jBx6eoU6sReRDk2FIEDWrTepVk/l1vhoFkNEEpMKQYBSMiPNTFSUdpoXpkWkE1IhCFB6Vh8Atm35NOQkIiJ7pkIQoN3NTFSXq70hEem4VAgC1CMn2sxEhZqZEJGOS4UgQD169abOk9TMhIh0aCoEAUpKTmar9SB5h54aEpGOS4UgYGpmQkQ6OhWCgG1PVTMTItKxqRAEbFdaNt3r1MyEiHRcKgQBq++aQy8vxxsawo4iItIiFYKgdc+ji9VRWaHLQyLSMakQBCyl++5mJkpCTiIi0jIVgoClZeUDUFWmTuxFpGNSIQhYt+xIe0M7t6qZCRHpmFQIApa5u5mJShUCEemYAi0EZnaymX1gZivN7NoW5l9oZqVmtij6uSTIPGHIyu5DvRsNamZCRDqolKBWbGbJwB3ACUAJ8I6ZzXb395ot+pC7XxFUjrAlp6SwWc1MiEgHFuQZwQRgpbuvcvca4EHg9AC312FVJmWRWq1mJkSkYwqyEPQD1jYZL4lOa+4MM1tiZrPMbEBLKzKzyWZWZGZFpaXx95f19tReZNSoEIhIxxT2zeKngAJ3HwW8CNzf0kLuPt3dx7n7uNzc3HYN2BZ2pWXTvV7NTIhIxxRkIVgHNP0Lv390WiN3L3P3XdHRu4AvB5gnNLXd+9O7YTM1u6rDjiIi8gVBFoJ3gKFmNtjMugBnAbObLmBm+U1GvwUsDzBPaFL7HEqKNfDpqqVhRxER+YLACoG71wFXAM8T+QX/sLsvM7ObzOxb0cWuMrNlZrYYuAq4MKg8YcoaOBKAso+LQ04iIvJFgT0+CuDuzwLPNpv2mybD1wHXBZmhI+h3yCga3KjZ0ClPeEQkzoV9szghdD2oO+uTetNl64qwo4iIfIEKQTspTR9Mr+0fhx1DROQLVAjaSXWPIfStX0ddbU3YUUREPkeFoJ0k5R1KmtWyfijYCzQAAAk0SURBVM0HYUcREfkcFYJ2kjWoEIDNqxaHnERE5PNUCNpJ/pBRAFSvb97mnohIuFQI2kn3Hr3YSDYpWz4MO4qIyOeoELSjjekFZFWtCjuGiMjnqBC0ox2Zh9Cvbi0N9fVhRxERaaRC0I4sdxgZtosNa1eGHUVEpJEKQTvKjLY5VPrxkpCTiIh8RoWgHeUPGQ3AznXLQk4iIvIZFYJ2lJXThzJ6kLRZL5WJSMehQtDONnQZRA89OSQiHYgKQTuryjyEfrVr2FW9I+woIiKACkG7yxj1bbrZTt59/Jawo4iIACoE7W7kV7/Jsi6j+dIH06iqVIf2IhI+FYJ2ZklJpJx4A72opPjR/wk7joiICkEYho07jnczjmbk6vsp37wh7DgikuBUCELS85s3kUE178+6KewoIpLgVAhCUnDoOBZmncTh6x/kjbv/D9U7qsKOJCIJKtBCYGYnm9kHZrbSzK5tYX6amT0Unf+WmRUEmaej+dL5f2VJj2M5au2dbPnj4Sx49l5KP12tRulEpF2ZuwezYrNkYAVwAlACvAOc7e7vNVnmx8Aod/+RmZ0FfMfdJ+1tvePGjfOioqJAModl2evPcNBLUyloWAtAtaeyKbk3O5J7sCulG7Up3WlI6YqnpNOQko4lp+HJqVhKGiSnYsmpkJSKpaSSlJwKySkkJaVgyalYUnKTTwqWnIRZMknJyZglYUnJJCWngBlmBlhkWlLkX8MgKRkzGsctKTmyrBlmkX2IrCsp8q9Z9HvR9dnuT1J02ei2LKnJdmmc1/zfz4aTPlu2he/sXqbl6S2so5nPLZ+kk2XpXMxsgbuPa2leSoDbnQCsdPdV0RAPAqcDTbvoOh24ITo8C7jdzMyDqk4d1Iijv0Ht+P+g+I1n2LHhQ3zLx3SpWkeXukoOqimja/UaungNadSQ5jWkWW3YkRNWg7dcRAD29J/WsRaH93U9xPTd1peJfXuxfHfftxfkejq7JQN+wJGX/LnN1xtkIegHrG0yXgIcsadl3L3OzCqAbGBz04XMbDIwOTpaZWb721hPTvN1J4hE3O9E3GdIzP1OoH2+BS5tfBl1X/d70J5mBFkI2oy7TwemH+h6zKxoT6dGnVki7nci7jMk5n4n4j5D2+53kBdC1wEDmoz3j05rcRkzSwF6AGUBZhIRkWaCLATvAEPNbLCZdQHOAmY3W2Y2cEF0+HvAy4l2f0BEJGyBXRqKXvO/AngeSAbucfdlZnYTUOTus4G7gX+Y2UpgC5FiEaQDvrwUpxJxvxNxnyEx9zsR9xnacL8De3xURETigx6WFhFJcCoEIiIJLmEKQWvNXXQGZjbAzOaY2XtmtszMpkSn9zKzF83sw+i/PcPOGgQzSzazd83s6ej44GjTJSujTZl0CTtjWzKzLDObZWbvm9lyMzsqEY61mf00+v97qZn908zSO+OxNrN7zGyTmS1tMq3F42sRf43u/xIzO3xftpUQhSDa3MUdwCnAYcDZZnZYuKkCUQdc4+6HAUcCl0f381rgJXcfCrwUHe+MpgDLm4z/HrjF3Q8BtgI/DCVVcG4F/uXuw4HRRPa9Ux9rM+sHXAWMc/eRRB5EOYvOeazvA05uNm1Px/cUYGj0Mxn4275sKCEKAU2au3D3GmB3cxediruvd/eF0eFtRH4x9COyr/dHF7sf+HY4CYNjZv2BbwB3RccNOI5I0yXQyfbbzHoAE4k8eYe717h7OQlwrIk87dg1+u5RBrCeTnis3X0ukacpm9rT8T0dmOERbwJZZpYf67YSpRC01NxFv5CytItoS65jgbeAPHdfH521AcgLKVaQ/gL8HGiIjmcD5e5eFx3vbMd8MFAK3Bu9HHaXmR1EJz/W7r4O+BPwCZECUAEsoHMf66b2dHwP6HdcohSChGJm3YBHgZ+4e2XTedEX9jrVM8Nmdhqwyd0XhJ2lHaUAhwN/c/exwHaaXQbqpMe6J5G/fgcDfYGD+OLlk4TQlsc3UQpBLM1ddApmlkqkCMx098eikzfuPk2M/rsprHwBORr4lpmtJnLZ7zgi18+zopcPoPMd8xKgxN3fio7PIlIYOvux/g/gY3cvdfda4DEix78zH+um9nR8D+h3XKIUgliau4h70evidwPL3b1pW7VNm/K4AHiyvbMFyd2vc/f+7l5A5Ni+7O7nAnOINF0CnWy/3X0DsNbMhkUnHU+kifdOfayJXBI60swyov/fd+93pz3Wzezp+M4Gzo8+PXQkUNHkElLr3D0hPsCpRDrK+Qj4Zdh5AtrHrxI5VVwCLIp+TiVyvfwl4EPg30CvsLMG+DP4OvB0dPhg4G1gJfAIkBZ2vjbe1zFAUfR4PwH0TIRjDdwIvA8sBf4BpHXGYw38k8h9kFoiZ4A/3NPxJdJZxR3R32/FRJ6qinlbamJCRCTBJcqlIRER2QMVAhGRBKdCICKS4FQIREQSnAqBiEiCUyGQhGZm9Wa2qMmnzRppM7OCpi1HxrD8QWb27+jwvCYvSIkESv/RJNHtdPcxYYeIOgp4I9qMwnb/rO0ckUDpjECkBWa22sz+YGbFZva2mR0SnV5gZi9H23x/ycwGRqfnmdnjZrY4+vlKdFXJZnZntP38F8ysawvbGmJmi4D/B5xDpBG10dEzlN7ttMuSwFQIJNF1bXZpaFKTeRXuXgjcTqR1U4DbgPvdfRQwE/hrdPpfgVfdfTSRNn+WRacPBe5w9xFAOXBG8wDu/lH0rGQBkSbT7wd+6O5j3L2ztRUkHZDeLJaEZmZV7t6themrgePcfVW0Ib8N7p5tZpuBfHevjU5f7+45ZlYK9Hf3XU3WUQC86JFORDCzqUCqu9+8hyzvuPt4M3sUmOLuJW28uyIt0hmByJ75Hob3xa4mw/W0cF/OzKZFbyoPjV4iOhl42sx+up/bFNknKgQiezapyb9vRIfnE2nhFOBc4LXo8EvAZdDYd3KPWDfi7j8i0pDafxHpceqZ6GWhWw4svkhs9NSQJLqu0b/Cd/uXu+9+hLSnmS0h8lf92dFpVxLpFexnRHoIuyg6fQow3cx+SOQv/8uItBwZq68BM4BjgFf3a09E9pPuEYi0IHqPYJy7bw47i0jQdGlIRCTB6YxARCTB6YxARCTBqRCIiCQ4FQIRkQSnQiAikuBUCEREEtz/B87VeJ31rMdXAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['masked_loss'], label='masked_loss')\n",
        "plt.plot(history.history['val_masked_loss'], label='val_masked_loss')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "KkhXRASNG80_",
        "outputId": "9ba9f02f-8860-44b1-a28f-c2fdbe1c7472"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f7967fdb3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(history.history['masked_acc'], label='accuracy')\n",
        "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "### Translate\n",
        "\n",
        "Now that the model is trained, implement a function to execute the full `text => text` translation. This code is basically identical to the [inference example](#inference) in the [decoder section](#the_decoder), but this also captures the attention weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "mmgYPCVgEwp_"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "@Translator.add_method\n",
        "def translate(self,\n",
        "              texts, *,\n",
        "              max_length=50,\n",
        "              temperature=0.0):\n",
        "  # Process the input texts\n",
        "  context = self.encoder.convert_input(texts)\n",
        "  batch_size = tf.shape(texts)[0]\n",
        "\n",
        "  # Setup the loop inputs\n",
        "  tokens = []\n",
        "  attention_weights = []\n",
        "  next_token, done, state = self.decoder.get_initial_state(context)\n",
        "\n",
        "  for _ in range(max_length):\n",
        "    # Generate the next token\n",
        "    next_token, done, state = self.decoder.get_next_token(\n",
        "        context, next_token, done,  state, temperature)\n",
        "        \n",
        "    # Collect the generated tokens\n",
        "    tokens.append(next_token)\n",
        "    attention_weights.append(self.decoder.last_attention_weights)\n",
        "    \n",
        "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  # Stack the lists of tokens and attention weights.\n",
        "  tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
        "  self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
        "\n",
        "  result = self.decoder.tokens_to_text(tokens)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4XufRntbbva"
      },
      "source": [
        "Here are the two helper methods, used above, to convert tokens to text, and to get the next token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "E5hqvbR5FUCD",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1508c39c-54c7-4720-ef19-5c7cd4ff3a26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'class2,table10,obj2,atr2,np '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "#sample Dataset Library_Management_Dataset_1_input_prerp_each_class_and_entitities_sep_experiment \n",
        "\n",
        "\n",
        "\n",
        "result1 = model.translate(['class1,table6,obj1,atr1'])\n",
        "\n",
        "result2 = model.translate(['class4,table17,obj2,atr2'])\n",
        "\n",
        "result23 = model.translate(['class14,table2,obj1,atr1'])\n",
        "\n",
        "result222 = model.translate(['class17,table4,obj2,atr2'])\n",
        "#result1[0].numpy().decode()\n",
        "result222[0].numpy().decode()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ1iU63cVgfs"
      },
      "source": [
        "Use that to generate the attention plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "s5hQWlbN3jGF"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "@Translator.add_method\n",
        "def plot_attention(self, text, **kwargs):\n",
        "  assert isinstance(text, str)\n",
        "  output = self.translate([text], **kwargs)\n",
        "  output = output[0].numpy().decode()\n",
        "\n",
        "  attention = self.last_attention_weights[0]\n",
        "\n",
        "  context = tf_lower_and_split_punct(text)\n",
        "  context = context.numpy().decode().split()\n",
        "\n",
        "  output = tf_lower_and_split_punct(output)\n",
        "  output = output.numpy().decode().split()[1:]\n",
        "\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + context, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + output, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  ax.set_xlabel('Input text')\n",
        "  ax.set_ylabel('Output text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "rrGawQv2eiA4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "outputId": "a5c97beb-a182-45af-b280-60e0461a8b80"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "model.plot_attention('class1,table10,obj1,atr1') # Are you still home"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHBdOf9duumm"
      },
      "source": [
        "Translate a few more sentences and plot them:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA3xI3NzrRJt"
      },
      "source": [
        "The short sentences often work well, but if the input is too long the model literally loses focus and stops providing reasonable predictions. There are two main reasons for this:\n",
        "\n",
        "1. The model was trained with teacher-forcing feeding the correct token at each step, regardless of the model's predictions. The model could be made more robust if it were sometimes fed its own predictions.\n",
        "2. The model only has access to its previous output through the RNN state. If the RNN state looses track of where it was in the context sequence there's no way for the model to recover. [Transformers](transformer.ipynb) improve on this by letting the decoder look at what it has output so far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtz6QBoGWqT2"
      },
      "source": [
        "The raw data is sorted by length, so try translating the longest sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "-FUHFLEvSMbG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6edf96ff-9cb2-46b6-e8d6-818d01ad5c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected output:\n",
            " class6,table18,obj1,atr1,P\n"
          ]
        }
      ],
      "source": [
        "long_text = context_raw[-1]\n",
        "\n",
        "import textwrap\n",
        "print('Expected output:\\n', '\\n'.join(textwrap.wrap(target_raw[-1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing existing samples"
      ],
      "metadata": {
        "id": "wmCM_ou4X9EP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve, classification_report"
      ],
      "metadata": {
        "id": "wnNs8LbU66N-"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('dummy_data_test.csv')"
      ],
      "metadata": {
        "id": "R_4M4qPQdG9l"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "o23nQwO3SKk7",
        "outputId": "278ff254-7ead-4973-8972-ab8a53f252eb"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                OM_Regular  OM_Prediction\n",
              "0  class1,table1,obj1,atr1              1\n",
              "1  class1,table2,obj1,atr1              1\n",
              "2  class1,table3,obj1,atr1              1\n",
              "3  class1,table4,obj1,atr1              1\n",
              "4  class1,table5,obj1,atr1              1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-85dced5e-2ea4-4afb-ab2e-7a967db186ec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OM_Regular</th>\n",
              "      <th>OM_Prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>class1,table1,obj1,atr1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>class1,table2,obj1,atr1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>class1,table3,obj1,atr1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>class1,table4,obj1,atr1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>class1,table5,obj1,atr1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85dced5e-2ea4-4afb-ab2e-7a967db186ec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-85dced5e-2ea4-4afb-ab2e-7a967db186ec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-85dced5e-2ea4-4afb-ab2e-7a967db186ec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = df['OM_Regular'].values\n",
        "y_test = df['OM_Prediction'].values"
      ],
      "metadata": {
        "id": "a3VgzasFdlTz"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "print(\"\\nX data type: \", X_test.dtype)\n",
        "print(\"y data type: \", y_test.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp0Z6jW5d9v9",
        "outputId": "73426ba5-4f06-4833-9953-ce9cadfdca9e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(107,)\n",
            "(107,)\n",
            "\n",
            "X data type:  object\n",
            "y data type:  int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KL_C7GZ627i",
        "outputId": "10fff3ef-999b-4559-943d-94f39ce30ba2"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = X_test"
      ],
      "metadata": {
        "id": "-qZlNac5eSDL"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PToqG3GiIUPM"
      },
      "source": [
        "The `translate` function works on batches, so if you have multiple texts to translate you can pass them all at once, which is much more efficient than translating them one at a time:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_train_pred = model.predict(X_train)"
      ],
      "metadata": {
        "id": "bNp4BPta5_4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "sT68i4jYEQ7q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d64f170d-9bff-47e0-ea48-e5dbfb4628de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class1,table1,obj1,atr1,p \n",
            "class1,table2,obj1,atr1,p \n",
            "class1,table3,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class1,table5,obj1,atr1,p \n",
            "class1,table6,obj1,atr1,p \n",
            "class1,table7,obj1,atr1,p \n",
            "class1,table8,obj1,atr1,p \n",
            "class1,table9,obj1,atr1,p \n",
            "class1,table10,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class1,table12,obj1,atr1,p \n",
            "class1,table13,obj1,atr1,p \n",
            "class1,table14,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class2,table10,obj2,atr2,np \n",
            "class1,table17,obj1,atr1,p \n",
            "class1,table1,obj18,atr1,p \n",
            "class2,table1,obj2,atr2,np \n",
            "class2,table2,obj2,atr2,np \n",
            "class2,table3,obj2,atr2,np \n",
            "class2,table4,obj2,atr2,np \n",
            "class2,table10,obj2,atr2,np \n",
            "class2,table6,obj2,atr2,np \n",
            "class2,table7,obj2,atr2,np \n",
            "class2,table8,obj2,atr2,np \n",
            "class2,table9,obj2,atr2,np \n",
            "class2,table10,obj2,atr2,np \n",
            "class2,table11,obj2,atr2,np \n",
            "class2,table12,obj2,atr2,np \n",
            "class2,table13,obj2,atr2,np \n",
            "class2,table14,obj2,atr2,np \n",
            "class2,table15,obj2,atr2,np \n",
            "class2,table16,obj2,atr2,np \n",
            "class2,table17,obj2,atr2,np \n",
            "class2,table18,obj2,atr2,np \n",
            "class3,table1,obj1,atr1,p \n",
            "class3,table2,obj1,atr1,p \n",
            "class3,table3,obj1,atr1,p \n",
            "class3,table4,obj1,atr1,p \n",
            "class3,table5,obj1,atr1,p \n",
            "class3,table6,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class3,table8,obj1,atr1,p \n",
            "class3,table9,obj1,atr1,p \n",
            "class3,table10,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class2,table10,obj2,atr2,np \n",
            "class3,table13,obj1,atr1,p \n",
            "class3,table14,obj1,atr1,p \n",
            "class3,table15,obj1,atr1,p \n",
            "class3,table16,obj1,atr1,p \n",
            "class3,table17,obj1,atr1,p \n",
            "class3,table18,obj1,atr1,p \n",
            "class4,table1,obj2,atr2,np \n",
            "class4,table2,obj2,atr2,np \n",
            "class4,table3,obj2,atr2,np \n",
            "class4,table4,obj2,atr2,np \n",
            "class4,table5,obj2,atr2,np \n",
            "class4,table6,obj2,atr2,np \n",
            "class4,table7,obj2,atr2,np \n",
            "class4,table8,obj2,atr2,np \n",
            "class4,table9,obj2,atr2,np \n",
            "class4,table10,obj2,atr2,np \n",
            "class4,table11,obj2,atr2,np \n",
            "class4,table12,obj2,atr2,np \n",
            "class4,table13,obj2,atr2,np \n",
            "class4,table14,obj2,atr2,np \n",
            "class4,table15,obj2,atr2,np \n",
            "class4,table16,obj2,atr2,np \n",
            "class4,table17,obj2,atr2,np \n",
            "class4,table18,obj2,atr2,np \n",
            "class2,table10,obj2,atr2,np \n",
            "class5,table2,obj1,atr1,p \n",
            "class5,table3,obj1,atr1,p \n",
            "class5,table4,obj1,atr1,p \n",
            "class5,table5,obj1,atr1,p \n",
            "class5,table6,obj1,atr1,p \n",
            "class5,table7,obj1,atr1,p \n",
            "class5,table8,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class5,table10,obj1,atr1,p \n",
            "class5,table11,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class5,table13,obj1,atr1,p \n",
            "class5,table14,obj1,atr1,p \n",
            "class5,table15,obj1,atr1,p \n",
            "class5,table16,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class6,table1,obj1,atr1,p \n",
            "class6,table2,obj1,atr1,p \n",
            "class6,table3,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class6,table5,obj1,atr1,p \n",
            "class6,table6,obj1,atr1,p \n",
            "class6,table7,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class6,table9,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class6,table11,obj1,atr1,p \n",
            "class6,table12,obj1,atr1,p \n",
            "class6,table13,obj1,atr1,p \n",
            "class6,table14,obj1,atr1,p \n",
            "class6,table15,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class6,table17,obj1,atr1,p \n",
            "class6,table18,obj1,atr1,p \n",
            "\n",
            "CPU times: user 13.9 s, sys: 423 ms, total: 14.3 s\n",
            "Wall time: 13.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "for t in inputs:\n",
        "  mylist_res = model.translate([t])[0].numpy().decode()\n",
        "  print(model.translate([t])[0].numpy().decode())\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Report (Existing samples)"
      ],
      "metadata": {
        "id": "OCkpjZl3YOhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dg = pd.read_csv('dummy_data_test_prediction.csv')"
      ],
      "metadata": {
        "id": "scj5ksJ8f0nH"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_pred = dg['OM_Regular'].values\n",
        "y_test_pred = dg['OM_Prediction'].values"
      ],
      "metadata": {
        "id": "5z16vFuixofS"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (y_test_pred )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-Y-62gL5HYP",
        "outputId": "d08ce25d-f5dd-4fd2-f217-7b34f4814f75"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X3fBkX6FRkNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision = precision_score(y_test, y_test_pred) \n",
        "print(\"Testing: Precision = %f\" % precision)\n",
        "\n",
        "\n",
        "recall = recall_score(y_test, y_test_pred)\n",
        "print(\"Testing: Recall = %f\" % recall)\n",
        "\n",
        "\n",
        "f1 = f1_score(y_test, y_test_pred)\n",
        "print(\"Testing: F1 Score = %f\" % f1)\n",
        "\n",
        "print(\"\\nConfusion Matrix (Test Data):\\n\", confusion_matrix(y_test, y_test_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nflFqA_4-HB",
        "outputId": "da3c8498-4272-4234-e471-e2f27472cf0b"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: Precision = 1.000000\n",
            "Testing: Recall = 0.802817\n",
            "Testing: F1 Score = 0.890625\n",
            "\n",
            "Confusion Matrix (Test Data):\n",
            " [[36  0]\n",
            " [14 57]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,y_test_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSjhM_oPN7Tr",
        "outputId": "bf7ef965-8654-42ce-e069-1886e401efeb"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      1.00      0.84        36\n",
            "           1       1.00      0.80      0.89        71\n",
            "\n",
            "    accuracy                           0.87       107\n",
            "   macro avg       0.86      0.90      0.86       107\n",
            "weighted avg       0.91      0.87      0.87       107\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing unseen samples"
      ],
      "metadata": {
        "id": "Rc1aekzi9dLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dc = pd.read_csv('dummy_data_unseen_test.csv')"
      ],
      "metadata": {
        "id": "6OIFQKZI9bc5"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dc.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Nsx0IyYZ9k3v",
        "outputId": "ea4b2086-c7d6-4ec0-a9f9-99c670127193"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 OM_Regular  OM_Prediction\n",
              "0  class14,table1,obj1,atr1              1\n",
              "1  class14,table2,obj1,atr1              1\n",
              "2  class14,table3,obj1,atr1              1\n",
              "3  class14,table4,obj1,atr1              1\n",
              "4  class14,table5,obj1,atr1              1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9fafb27e-53d8-4bf1-9b26-7a336cb379d9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OM_Regular</th>\n",
              "      <th>OM_Prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>class14,table1,obj1,atr1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>class14,table2,obj1,atr1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>class14,table3,obj1,atr1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>class14,table4,obj1,atr1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>class14,table5,obj1,atr1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9fafb27e-53d8-4bf1-9b26-7a336cb379d9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9fafb27e-53d8-4bf1-9b26-7a336cb379d9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9fafb27e-53d8-4bf1-9b26-7a336cb379d9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test2 = df['OM_Regular'].values\n",
        "y_test2 = df['OM_Prediction'].values"
      ],
      "metadata": {
        "id": "naG54qF791Hs"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZFASLWP95TU",
        "outputId": "bbda94cf-3a7c-4622-a0ca-554dffea9702"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = X_test2"
      ],
      "metadata": {
        "id": "hgO5sa73-3f1"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for t in inputs:\n",
        "  mylist_res = model.translate([t])[0].numpy().decode()\n",
        "  print(model.translate([t])[0].numpy().decode())\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qjPTIDB-8UZ",
        "outputId": "53f4f9a8-3c8d-447e-f8c4-33f28ea937ae"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class1,table1,obj1,atr1,p \n",
            "class1,table2,obj1,atr1,p \n",
            "class1,table3,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class1,table5,obj1,atr1,p \n",
            "class1,table6,obj1,atr1,p \n",
            "class1,table7,obj1,atr1,p \n",
            "class1,table8,obj1,atr1,p \n",
            "class1,table9,obj1,atr1,p \n",
            "class1,table10,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class1,table12,obj1,atr1,p \n",
            "class1,table13,obj1,atr1,p \n",
            "class1,table14,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class2,table10,obj2,atr2,np \n",
            "class1,table17,obj1,atr1,p \n",
            "class1,table1,obj18,atr1,p \n",
            "class2,table1,obj2,atr2,np \n",
            "class2,table2,obj2,atr2,np \n",
            "class2,table3,obj2,atr2,np \n",
            "class2,table4,obj2,atr2,np \n",
            "class2,table10,obj2,atr2,np \n",
            "class2,table6,obj2,atr2,np \n",
            "class2,table7,obj2,atr2,np \n",
            "class2,table8,obj2,atr2,np \n",
            "class2,table9,obj2,atr2,np \n",
            "class2,table10,obj2,atr2,np \n",
            "class2,table11,obj2,atr2,np \n",
            "class2,table12,obj2,atr2,np \n",
            "class2,table13,obj2,atr2,np \n",
            "class2,table14,obj2,atr2,np \n",
            "class2,table15,obj2,atr2,np \n",
            "class2,table16,obj2,atr2,np \n",
            "class2,table17,obj2,atr2,np \n",
            "class2,table18,obj2,atr2,np \n",
            "class3,table1,obj1,atr1,p \n",
            "class3,table2,obj1,atr1,p \n",
            "class3,table3,obj1,atr1,p \n",
            "class3,table4,obj1,atr1,p \n",
            "class3,table5,obj1,atr1,p \n",
            "class3,table6,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class3,table8,obj1,atr1,p \n",
            "class3,table9,obj1,atr1,p \n",
            "class3,table10,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class2,table10,obj2,atr2,np \n",
            "class3,table13,obj1,atr1,p \n",
            "class3,table14,obj1,atr1,p \n",
            "class3,table15,obj1,atr1,p \n",
            "class3,table16,obj1,atr1,p \n",
            "class3,table17,obj1,atr1,p \n",
            "class3,table18,obj1,atr1,p \n",
            "class4,table1,obj2,atr2,np \n",
            "class4,table2,obj2,atr2,np \n",
            "class4,table3,obj2,atr2,np \n",
            "class4,table4,obj2,atr2,np \n",
            "class4,table5,obj2,atr2,np \n",
            "class4,table6,obj2,atr2,np \n",
            "class4,table7,obj2,atr2,np \n",
            "class4,table8,obj2,atr2,np \n",
            "class4,table9,obj2,atr2,np \n",
            "class4,table10,obj2,atr2,np \n",
            "class4,table11,obj2,atr2,np \n",
            "class4,table12,obj2,atr2,np \n",
            "class4,table13,obj2,atr2,np \n",
            "class4,table14,obj2,atr2,np \n",
            "class4,table15,obj2,atr2,np \n",
            "class4,table16,obj2,atr2,np \n",
            "class4,table17,obj2,atr2,np \n",
            "class4,table18,obj2,atr2,np \n",
            "class2,table10,obj2,atr2,np \n",
            "class5,table2,obj1,atr1,p \n",
            "class5,table3,obj1,atr1,p \n",
            "class5,table4,obj1,atr1,p \n",
            "class5,table5,obj1,atr1,p \n",
            "class5,table6,obj1,atr1,p \n",
            "class5,table7,obj1,atr1,p \n",
            "class5,table8,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class5,table10,obj1,atr1,p \n",
            "class5,table11,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class5,table13,obj1,atr1,p \n",
            "class5,table14,obj1,atr1,p \n",
            "class5,table15,obj1,atr1,p \n",
            "class5,table16,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class6,table1,obj1,atr1,p \n",
            "class6,table2,obj1,atr1,p \n",
            "class6,table3,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class6,table5,obj1,atr1,p \n",
            "class6,table6,obj1,atr1,p \n",
            "class6,table7,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class6,table9,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class6,table11,obj1,atr1,p \n",
            "class6,table12,obj1,atr1,p \n",
            "class6,table13,obj1,atr1,p \n",
            "class6,table14,obj1,atr1,p \n",
            "class6,table15,obj1,atr1,p \n",
            "class2,table10,obj2,atr2,np \n",
            "class6,table17,obj1,atr1,p \n",
            "class6,table18,obj1,atr1,p \n",
            "\n",
            "CPU times: user 14.4 s, sys: 439 ms, total: 14.8 s\n",
            "Wall time: 14.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Report (Unseen samples)\n"
      ],
      "metadata": {
        "id": "1t4_2FqbE9da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dd = pd.read_csv('dummy_data_unseen_test_prediction.csv')"
      ],
      "metadata": {
        "id": "jhKnUY4XFCSj"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dd.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "v9M2iW1MGjfM",
        "outputId": "6ecb11dd-2e0b-4ea4-8c9b-cab4c3f02b12"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     OM_Regular  OM_Prediction\n",
              "0    class1,table1,obj1,atr1,p               1\n",
              "1    class1,table2,obj1,atr1,p               1\n",
              "2    class1,table3,obj1,atr1,p               1\n",
              "3  class2,table10,obj2,atr2,np               0\n",
              "4    class1,table5,obj1,atr1,p               1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c53ffc6-c282-42db-8a91-9c4aa1fe7984\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OM_Regular</th>\n",
              "      <th>OM_Prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>class1,table1,obj1,atr1,p</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>class1,table2,obj1,atr1,p</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>class1,table3,obj1,atr1,p</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>class2,table10,obj2,atr2,np</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>class1,table5,obj1,atr1,p</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c53ffc6-c282-42db-8a91-9c4aa1fe7984')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3c53ffc6-c282-42db-8a91-9c4aa1fe7984 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3c53ffc6-c282-42db-8a91-9c4aa1fe7984');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_pred2 = df['OM_Regular'].values\n",
        "y_test_pred2 = df['OM_Prediction'].values"
      ],
      "metadata": {
        "id": "1tO_WHmVHQDR"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (y_test_pred2 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy2Fvt1fHYJO",
        "outputId": "1732d506-6622-4b1c-948a-af9192b4fad3"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision = precision_score(y_test, y_test_pred2) \n",
        "print(\"Testing: Precision = %f\" % precision)\n",
        "\n",
        "\n",
        "recall = recall_score(y_test, y_test_pred2)\n",
        "print(\"Testing: Recall = %f\" % recall)\n",
        "\n",
        "\n",
        "f1 = f1_score(y_test, y_test_pred2)\n",
        "print(\"Testing: F1 Score = %f\" % f1)\n",
        "\n",
        "print(\"\\nConfusion Matrix (Test Data):\\n\", confusion_matrix(y_test, y_test_pred2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7RY4modHkts",
        "outputId": "f4e1143b-c181-41f8-a73b-371b681053d5"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: Precision = 1.000000\n",
            "Testing: Recall = 1.000000\n",
            "Testing: F1 Score = 1.000000\n",
            "\n",
            "Confusion Matrix (Test Data):\n",
            " [[36  0]\n",
            " [ 0 71]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,y_test_pred2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd3P-TGIIN6b",
        "outputId": "e049269e-9d61-4213-f7c7-3b35efe7d47b"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        36\n",
            "           1       1.00      1.00      1.00        71\n",
            "\n",
            "    accuracy                           1.00       107\n",
            "   macro avg       1.00      1.00      1.00       107\n",
            "weighted avg       1.00      1.00      1.00       107\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}