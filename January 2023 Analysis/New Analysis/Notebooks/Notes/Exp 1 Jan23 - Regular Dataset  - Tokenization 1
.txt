Exp 1 Jan23 - Regular Dataset  - Tokenization 1 - 6 OM Dataset


Notes
Optimizers used: 'Adadelta', 'Adagrad', 'Adam', ‘SGD’, ‘RMSProp’


Tokenization structure:

- Comma separated tokenization, section based separation noticed.
- entity separation noticed.
- Labels considered as a token on target


Ran multiple times to observe translations



- Run 1 : Adam (Sample 1 - 20) val steps=30, epochs 100
- Run 2 : Adadelta (Sample 20 - 40) val steps=30, epochs 100
- Run 3 : Adagrad(Sample 41 - 60) val steps=30, epochs 100 (no prediction)


- Run 4 : SGD (Sample 41 - 60) (no prediction)
- Run 5 : RMSProp (Sample 41 - 60) (no prediction)

- Run 6 : Adam (Sample 41 - 60) val steps=30, epochs 100







Total Instance = 101

Pareto = 15
Not Pareto = 86


correctly predicted P =   (TP)
Incorrectly predicted P =   (FN)


correctly predicted NP =  (TN)
Incorrectly predicted NP =   (FP)

Incorrectly predicted =   predicted as NP but they were P |   predicted as P and they are P
  predicted as P but they were NP |   predicted as NP and they are NP





Precision - TP / (TP + FP) = 47 / (47 + 25) = 0.65
Recall - TP / (TP + FN) = 47 / (47 + 24) = 0.66
Accuracy = (TN + TP ) / (TN + FP + TP + FN ) = (6 + 47) / (6 + 25 + 47 + 24) = 0.519
F1 = 2 * (Precision * Recall / Precision + Recall) = 2 * (0.65 * 0.66 / 0.65 + 0.66) = 2 * (0.429 / 1.31) = 0.65




018 198 226 19
